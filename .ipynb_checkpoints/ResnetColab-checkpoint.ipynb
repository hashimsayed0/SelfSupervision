{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Zuq46nV7-LCl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexander\\.conda\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Alexander\\.conda\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Alexander\\.conda\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Alexander\\.conda\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Alexander\\.conda\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Alexander\\.conda\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import losses, layers, models, metrics, Model\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kHukaojD-ijS"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQlQREK1GQxe",
    "outputId": "9d88a6ed-0845-4d36-8059-6d2344baf2e9"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "T_zX0o9tG0WP"
   },
   "outputs": [],
   "source": [
    "x_train, y_train = train_data\n",
    "x_test, y_test = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "wU1ZbhQJ-g2Q",
    "outputId": "b886409e-a37c-433f-9656-713406fb37bb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[random.randint(0, 49999)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iRZ-kVCyLqyV"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "l26FOixkLtgY"
   },
   "outputs": [],
   "source": [
    "x_train = preprocess_input(x_train)\n",
    "x_test = preprocess_input(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dnJ6HxqUL1ON"
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UnBfIQFEEdFy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Alexander\\.conda\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Alexander\\.conda\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexander\\.conda\\envs\\keras-gpu\\lib\\site-packages\\keras_applications\\resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "base = ResNet50(include_top=False, weights=None)\n",
    "l = base.output\n",
    "l = layers.GlobalAveragePooling2D()(l)\n",
    "l = layers.Flatten()(l)\n",
    "l = layers.Dropout(0.5)(l)\n",
    "l = layers.Dense(256, kernel_regularizer=regularizers.l1_l2(l1 = 0.001, l2 = 0.01))(l)\n",
    "l = layers.BatchNormalization()(l)\n",
    "l = layers.Activation('sigmoid')(l)\n",
    "l = layers.Dropout(0.3)(l)\n",
    "l = layers.Dense(10, kernel_regularizer=regularizers.l1_l2(l1 = 0.001, l2 = 0.01), activation = 'softmax')(l)\n",
    "model = Model(inputs = base.input, outputs = l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, None, None, 3 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, None, None, 6 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalizationV1) (None, None, None, 6 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, None, 6 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, None, None, 6 0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, None, None, 6 0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, None, None, 6 4160        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, None, None, 6 256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, None, 6 0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, None, None, 6 36928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, None, None, 6 256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, None, 6 0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, None, None, 2 16640       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, None, None, 2 16640       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, None, None, 2 1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, None, None, 2 1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, None, None, 2 0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, None, 2 0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, None, None, 6 16448       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, None, None, 6 256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, None, None, 6 0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, None, None, 6 36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, None, None, 6 256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, None, None, 6 0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, None, None, 2 16640       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, None, None, 2 1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, None, None, 2 0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, None, None, 2 0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, None, None, 6 16448       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, None, None, 6 256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, None, None, 6 0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, None, None, 6 36928       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, None, None, 6 256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, None, 6 0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, None, None, 2 16640       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, None, None, 2 1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, None, None, 2 0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, None, 2 0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, None, None, 1 32896       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, None, None, 1 512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, None, None, 1 0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, None, None, 1 147584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, None, None, 1 512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, None, None, 1 0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, None, None, 5 66048       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, None, None, 5 131584      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, None, None, 5 2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, None, None, 5 2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, None, None, 5 0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, None, None, 5 0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, None, None, 1 65664       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, None, None, 1 512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, None, None, 1 0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, None, None, 1 147584      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, None, None, 1 512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, None, None, 1 0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, None, None, 5 66048       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, None, None, 5 2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, None, None, 5 0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, None, None, 5 0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, None, None, 1 65664       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, None, None, 1 512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, None, None, 1 0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, None, None, 1 147584      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, None, None, 1 512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, None, None, 1 0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, None, None, 5 66048       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, None, None, 5 2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, None, None, 5 0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, None, None, 5 0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, None, None, 1 65664       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, None, None, 1 512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, None, None, 1 0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, None, None, 1 147584      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, None, None, 1 512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, None, None, 1 0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, None, None, 5 66048       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, None, None, 5 2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, None, None, 5 0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, None, None, 5 0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, None, None, 2 131328      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, None, None, 2 1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, None, None, 2 0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, None, None, 2 590080      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, None, None, 2 1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, None, None, 2 0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, None, None, 1 263168      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, None, None, 1 525312      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, None, None, 1 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, None, None, 1 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, None, None, 1 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, None, None, 1 0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, None, None, 2 262400      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, None, None, 2 1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, None, None, 2 0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, None, None, 2 590080      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, None, None, 2 1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, None, None, 2 0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, None, None, 1 263168      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, None, None, 1 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, None, None, 1 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, None, None, 1 0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, None, None, 2 262400      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, None, None, 2 1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, None, None, 2 0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, None, None, 2 590080      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, None, None, 2 1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, None, None, 2 0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, None, None, 1 263168      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, None, None, 1 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, None, None, 1 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, None, None, 1 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, None, None, 2 262400      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, None, None, 2 1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, None, None, 2 0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, None, None, 2 590080      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, None, None, 2 1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, None, None, 2 0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, None, None, 1 263168      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, None, None, 1 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, None, None, 1 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, None, None, 1 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, None, None, 2 262400      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, None, None, 2 1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, None, None, 2 0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, None, None, 2 590080      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, None, None, 2 1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, None, None, 2 0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, None, None, 1 263168      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, None, None, 1 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, None, None, 1 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, None, None, 1 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, None, None, 2 262400      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, None, None, 2 1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, None, None, 2 0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, None, None, 2 590080      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, None, None, 2 1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, None, None, 2 0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, None, None, 1 263168      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, None, None, 1 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, None, None, 1 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, None, None, 1 0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, None, None, 5 524800      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, None, None, 5 2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, None, None, 5 0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, None, None, 5 2359808     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, None, None, 5 2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, None, None, 5 0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, None, None, 2 1050624     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, None, None, 2 2099200     activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, None, None, 2 8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, None, None, 2 8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, None, None, 2 0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, None, None, 2 0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, None, None, 5 1049088     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, None, None, 5 2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, None, None, 5 0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, None, None, 5 2359808     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, None, None, 5 2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, None, None, 5 0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, None, None, 2 1050624     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, None, None, 2 8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, None, None, 2 0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, None, None, 2 0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, None, None, 5 1049088     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, None, None, 5 2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, None, None, 5 0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, None, None, 5 2359808     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, None, None, 5 2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, None, None, 5 0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, None, None, 2 1050624     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, None, None, 2 8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, None, None, 2 0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, None, None, 2 0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2048)         0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 2048)         0           global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          524544      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 256)          1024        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 256)          0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           2570        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 24,115,850\n",
      "Trainable params: 24,062,218\n",
      "Non-trainable params: 53,632\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    if epoch > 75:\n",
    "        return 0.0003\n",
    "    if epoch > 50:\n",
    "        return 0.001\n",
    "    if epoch > 25:\n",
    "        return 0.003\n",
    "    return 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    )\n",
    "data_generator.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "a9R42lb4E6H7"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = optimizers.Adam(), loss = 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2-E3QnynPXO5",
    "outputId": "b91221ea-f0e7-4944-acd1-3bb87845ab14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/60\n",
      "50000/50000 [==============================] - 39s 773us/sample - loss: 3.4473 - acc: 0.2585 - val_loss: 3.4500 - val_acc: 0.2672\n",
      "Epoch 2/60\n",
      "50000/50000 [==============================] - 30s 601us/sample - loss: 2.7974 - acc: 0.4171 - val_loss: 3.0467 - val_acc: 0.4052\n",
      "Epoch 3/60\n",
      "50000/50000 [==============================] - 31s 620us/sample - loss: 2.5967 - acc: 0.5212 - val_loss: 2.6642 - val_acc: 0.5311\n",
      "Epoch 4/60\n",
      "50000/50000 [==============================] - 30s 604us/sample - loss: 2.4624 - acc: 0.6052 - val_loss: 2.6929 - val_acc: 0.5481\n",
      "Epoch 5/60\n",
      "50000/50000 [==============================] - 30s 598us/sample - loss: 2.4370 - acc: 0.6501 - val_loss: 2.8330 - val_acc: 0.5661\n",
      "Epoch 6/60\n",
      "50000/50000 [==============================] - 31s 617us/sample - loss: 2.3591 - acc: 0.6899 - val_loss: 2.5809 - val_acc: 0.6406\n",
      "Epoch 7/60\n",
      "50000/50000 [==============================] - 30s 606us/sample - loss: 2.3391 - acc: 0.7169 - val_loss: 2.4678 - val_acc: 0.6879\n",
      "Epoch 8/60\n",
      "50000/50000 [==============================] - 31s 610us/sample - loss: 2.2874 - acc: 0.7445 - val_loss: 2.6479 - val_acc: 0.6423\n",
      "Epoch 9/60\n",
      "50000/50000 [==============================] - 30s 597us/sample - loss: 2.2669 - acc: 0.7648 - val_loss: 2.6244 - val_acc: 0.6943\n",
      "Epoch 10/60\n",
      "50000/50000 [==============================] - 31s 620us/sample - loss: 2.2577 - acc: 0.7856 - val_loss: 2.5321 - val_acc: 0.6868\n",
      "Epoch 11/60\n",
      "50000/50000 [==============================] - 31s 617us/sample - loss: 2.2133 - acc: 0.7961 - val_loss: 2.5152 - val_acc: 0.7090\n",
      "Epoch 12/60\n",
      "50000/50000 [==============================] - 31s 625us/sample - loss: 2.1896 - acc: 0.8150 - val_loss: 2.4566 - val_acc: 0.7211\n",
      "Epoch 13/60\n",
      "50000/50000 [==============================] - 31s 613us/sample - loss: 2.1737 - acc: 0.8302 - val_loss: 2.6080 - val_acc: 0.7005\n",
      "Epoch 14/60\n",
      "50000/50000 [==============================] - 31s 614us/sample - loss: 2.1158 - acc: 0.8455 - val_loss: 2.5860 - val_acc: 0.7333\n",
      "Epoch 15/60\n",
      "50000/50000 [==============================] - 31s 613us/sample - loss: 2.1168 - acc: 0.8565 - val_loss: 2.5826 - val_acc: 0.7228\n",
      "Epoch 16/60\n",
      "50000/50000 [==============================] - 32s 632us/sample - loss: 2.1024 - acc: 0.8593 - val_loss: 2.5577 - val_acc: 0.7426\n",
      "Epoch 17/60\n",
      "50000/50000 [==============================] - 31s 630us/sample - loss: 2.0406 - acc: 0.8776 - val_loss: 2.5016 - val_acc: 0.7278\n",
      "Epoch 18/60\n",
      "50000/50000 [==============================] - 30s 594us/sample - loss: 2.0347 - acc: 0.8845 - val_loss: 2.4979 - val_acc: 0.7439\n",
      "Epoch 19/60\n",
      "50000/50000 [==============================] - 30s 600us/sample - loss: 2.0089 - acc: 0.8942 - val_loss: 2.5273 - val_acc: 0.7463\n",
      "Epoch 20/60\n",
      "50000/50000 [==============================] - 30s 603us/sample - loss: 2.0121 - acc: 0.8989 - val_loss: 2.7057 - val_acc: 0.7313\n",
      "Epoch 21/60\n",
      "50000/50000 [==============================] - 30s 596us/sample - loss: 1.9726 - acc: 0.9075 - val_loss: 2.4923 - val_acc: 0.7496\n",
      "Epoch 22/60\n",
      "50000/50000 [==============================] - 30s 604us/sample - loss: 0.9688 - acc: 0.9513 - val_loss: 1.5302 - val_acc: 0.7831\n",
      "Epoch 23/60\n",
      "50000/50000 [==============================] - 30s 599us/sample - loss: 0.8478 - acc: 0.9686 - val_loss: 1.5314 - val_acc: 0.7836\n",
      "Epoch 24/60\n",
      "50000/50000 [==============================] - 30s 598us/sample - loss: 0.8094 - acc: 0.9741 - val_loss: 1.5516 - val_acc: 0.7820\n",
      "Epoch 25/60\n",
      "50000/50000 [==============================] - 30s 599us/sample - loss: 0.7997 - acc: 0.9755 - val_loss: 1.6262 - val_acc: 0.7775\n",
      "Epoch 26/60\n",
      "50000/50000 [==============================] - 30s 600us/sample - loss: 0.8008 - acc: 0.9774 - val_loss: 1.6885 - val_acc: 0.7785\n",
      "Epoch 27/60\n",
      "50000/50000 [==============================] - 30s 600us/sample - loss: 0.7847 - acc: 0.9777 - val_loss: 1.5395 - val_acc: 0.7809\n",
      "Epoch 28/60\n",
      "50000/50000 [==============================] - 30s 599us/sample - loss: 0.7651 - acc: 0.9803 - val_loss: 1.6394 - val_acc: 0.7672\n",
      "Epoch 29/60\n",
      "50000/50000 [==============================] - 30s 600us/sample - loss: 0.7817 - acc: 0.9790 - val_loss: 1.5589 - val_acc: 0.7746\n",
      "Epoch 30/60\n",
      "50000/50000 [==============================] - 30s 605us/sample - loss: 0.7657 - acc: 0.9809 - val_loss: 1.6158 - val_acc: 0.7756\n",
      "Epoch 31/60\n",
      "50000/50000 [==============================] - 30s 603us/sample - loss: 0.7744 - acc: 0.9797 - val_loss: 1.6947 - val_acc: 0.7737\n",
      "Epoch 32/60\n",
      "50000/50000 [==============================] - 30s 602us/sample - loss: 0.7599 - acc: 0.9815 - val_loss: 1.6146 - val_acc: 0.7768\n",
      "Epoch 33/60\n",
      "50000/50000 [==============================] - 30s 600us/sample - loss: 0.7633 - acc: 0.9833 - val_loss: 1.6083 - val_acc: 0.7755\n",
      "Epoch 34/60\n",
      "50000/50000 [==============================] - 31s 615us/sample - loss: 0.7698 - acc: 0.9822 - val_loss: 1.6020 - val_acc: 0.7690\n",
      "Epoch 35/60\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.7482 - acc: 0.9838"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-4d0dbd5c94a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mLearningRateScheduler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr_schedule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\.conda\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[0;32m    362\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m           \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m           validation_in_fit=True)\n\u001b[0m\u001b[0;32m    365\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_results\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[0mval_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mval_results\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3076\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[1;32m~\\.conda\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logs = model.fit_generator(data_generator.flow(x_train, y_train, batch_size=128), epochs = 60, validation_data = (x_test, y_test), batch_size = 128, callbacks = [LearningRateScheduler(lr_schedule)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5F3yIlWHPzDu"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = optimizers.SGD(lr=0.1, momentum=0.9, decay=0.0001), loss = 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EjT0ycwAox_M",
    "outputId": "42dfe6de-c5d2-4d16-8f10-8c3e918cabe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  2/391 [..............................] - ETA: 1:06 - loss: 16.5988 - accuracy: 0.0820WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0758s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n",
      "391/391 [==============================] - 65s 165ms/step - loss: 3.5787 - accuracy: 0.2025 - val_loss: 2.4978 - val_accuracy: 0.2423\n",
      "Epoch 2/100\n",
      "391/391 [==============================] - 63s 162ms/step - loss: 2.3927 - accuracy: 0.3121 - val_loss: 2.2884 - val_accuracy: 0.3717\n",
      "Epoch 3/100\n",
      "391/391 [==============================] - 63s 162ms/step - loss: 2.2288 - accuracy: 0.4187 - val_loss: 2.1488 - val_accuracy: 0.4722\n",
      "Epoch 4/100\n",
      "391/391 [==============================] - 63s 162ms/step - loss: 2.0727 - accuracy: 0.4883 - val_loss: 2.1400 - val_accuracy: 0.4405\n",
      "Epoch 5/100\n",
      "391/391 [==============================] - 63s 162ms/step - loss: 1.9384 - accuracy: 0.5364 - val_loss: 1.9194 - val_accuracy: 0.5570\n",
      "Epoch 6/100\n",
      "391/391 [==============================] - 63s 161ms/step - loss: 1.8352 - accuracy: 0.5813 - val_loss: 1.7637 - val_accuracy: 0.5809\n",
      "Epoch 7/100\n",
      "391/391 [==============================] - 63s 161ms/step - loss: 1.7287 - accuracy: 0.6167 - val_loss: 1.9156 - val_accuracy: 0.5611\n",
      "Epoch 8/100\n",
      "391/391 [==============================] - 63s 161ms/step - loss: 1.6272 - accuracy: 0.6495 - val_loss: 1.7381 - val_accuracy: 0.6019\n",
      "Epoch 9/100\n",
      "391/391 [==============================] - 63s 162ms/step - loss: 1.5398 - accuracy: 0.6774 - val_loss: 1.7462 - val_accuracy: 0.5987\n",
      "Epoch 10/100\n",
      "391/391 [==============================] - 63s 161ms/step - loss: 1.4503 - accuracy: 0.7111 - val_loss: 1.8024 - val_accuracy: 0.6094\n",
      "Epoch 11/100\n",
      "391/391 [==============================] - 63s 161ms/step - loss: 1.3671 - accuracy: 0.7375 - val_loss: 1.7542 - val_accuracy: 0.6250\n",
      "Epoch 12/100\n",
      "391/391 [==============================] - 63s 161ms/step - loss: 1.2835 - accuracy: 0.7651 - val_loss: 1.7525 - val_accuracy: 0.6194\n",
      "Epoch 13/100\n",
      "391/391 [==============================] - 63s 161ms/step - loss: 1.2005 - accuracy: 0.7865 - val_loss: 1.6559 - val_accuracy: 0.6510\n",
      "Epoch 14/100\n",
      "391/391 [==============================] - 63s 161ms/step - loss: 1.1554 - accuracy: 0.8043 - val_loss: 1.6530 - val_accuracy: 0.6506\n",
      "Epoch 15/100\n",
      "391/391 [==============================] - 63s 161ms/step - loss: 1.0843 - accuracy: 0.8225 - val_loss: 1.6870 - val_accuracy: 0.6506\n",
      "Epoch 16/100\n",
      "391/391 [==============================] - 63s 161ms/step - loss: 1.0218 - accuracy: 0.8418 - val_loss: 1.7202 - val_accuracy: 0.6366\n",
      "Epoch 17/100\n",
      "391/391 [==============================] - 63s 161ms/step - loss: 0.9675 - accuracy: 0.8562 - val_loss: 1.7911 - val_accuracy: 0.6389\n",
      "Epoch 18/100\n",
      "391/391 [==============================] - 63s 161ms/step - loss: 0.9256 - accuracy: 0.8687 - val_loss: 1.7133 - val_accuracy: 0.6423\n",
      "Epoch 19/100\n",
      "391/391 [==============================] - 63s 161ms/step - loss: 0.8700 - accuracy: 0.8835 - val_loss: 1.7247 - val_accuracy: 0.6483\n",
      "Epoch 20/100\n",
      "391/391 [==============================] - 63s 161ms/step - loss: 0.8359 - accuracy: 0.8936 - val_loss: 1.6897 - val_accuracy: 0.6508\n",
      "Epoch 21/100\n",
      "391/391 [==============================] - 63s 162ms/step - loss: 0.7835 - accuracy: 0.9063 - val_loss: 1.6935 - val_accuracy: 0.6627\n",
      "Epoch 22/100\n",
      "391/391 [==============================] - 63s 162ms/step - loss: 0.7458 - accuracy: 0.9142 - val_loss: 1.7533 - val_accuracy: 0.6563\n",
      "Epoch 23/100\n",
      "391/391 [==============================] - 63s 162ms/step - loss: 0.7349 - accuracy: 0.9187 - val_loss: 1.7566 - val_accuracy: 0.6560\n",
      "Epoch 24/100\n",
      "389/391 [============================>.] - ETA: 0s - loss: 0.6871 - accuracy: 0.9276"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-57ee42019f4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logs = model.fit(data_generator.flow(x_train, y_train, batch_size=128), epochs = 60, validation_data = (x_test, y_test), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "zpUH_xIX_Gfo"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = optimizers.RMSprop(), loss = 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5qZ_em1Wo0ZU",
    "outputId": "4d3b1f89-9e5b-4c55-a6de-a3a0c2aa7f63",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 5s 549us/sample - loss: 5.1291 - acc: 0.1740\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 5.5238 - acc: 0.1621 - val_loss: 5.1313 - val_acc: 0.1740\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 2s 213us/sample - loss: 4.8721 - acc: 0.1946\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 4.9056 - acc: 0.2168 - val_loss: 4.8695 - val_acc: 0.1946\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 2s 212us/sample - loss: 4.7690 - acc: 0.2930\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 4.7846 - acc: 0.2732 - val_loss: 4.7683 - val_acc: 0.2930\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 2s 221us/sample - loss: 4.6668 - acc: 0.3548\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 4.6535 - acc: 0.3352 - val_loss: 4.6658 - val_acc: 0.3548\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 2s 212us/sample - loss: 4.5995 - acc: 0.3873\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 4.5954 - acc: 0.3647 - val_loss: 4.5997 - val_acc: 0.3873\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 2s 214us/sample - loss: 4.5442 - acc: 0.4290\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 4.5498 - acc: 0.4007 - val_loss: 4.5444 - val_acc: 0.4290\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 2s 218us/sample - loss: 4.3829 - acc: 0.4983\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 4.4902 - acc: 0.4554 - val_loss: 4.3836 - val_acc: 0.4983\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 2s 212us/sample - loss: 4.6217 - acc: 0.3676\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 4.4381 - acc: 0.4873 - val_loss: 4.6208 - val_acc: 0.3676\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 2s 218us/sample - loss: 4.3530 - acc: 0.5537\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 4.3995 - acc: 0.5164 - val_loss: 4.3512 - val_acc: 0.5537\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 2s 216us/sample - loss: 4.4040 - acc: 0.5567\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 4.3710 - acc: 0.5517 - val_loss: 4.3988 - val_acc: 0.5567\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 2s 214us/sample - loss: 4.2774 - acc: 0.6134\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 4.3358 - acc: 0.5818 - val_loss: 4.2745 - val_acc: 0.6134\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 2s 219us/sample - loss: 4.3453 - acc: 0.5831\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 4.3063 - acc: 0.5984 - val_loss: 4.3430 - val_acc: 0.5831\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 2s 220us/sample - loss: 4.2743 - acc: 0.6444\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 4.2937 - acc: 0.6125 - val_loss: 4.2729 - val_acc: 0.6444\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 2s 216us/sample - loss: 4.2517 - acc: 0.6482\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 4.2666 - acc: 0.6271 - val_loss: 4.2474 - val_acc: 0.6482\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 2s 213us/sample - loss: 4.2212 - acc: 0.6513\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 4.2542 - acc: 0.6387 - val_loss: 4.2219 - val_acc: 0.6513\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 2s 217us/sample - loss: 4.1887 - acc: 0.6804\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 4.2478 - acc: 0.6446 - val_loss: 4.1875 - val_acc: 0.6804\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 2s 221us/sample - loss: 4.2599 - acc: 0.6577\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 4.2287 - acc: 0.6552 - val_loss: 4.2579 - val_acc: 0.6577\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 2s 219us/sample - loss: 4.1623 - acc: 0.6813\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 4.2257 - acc: 0.6621 - val_loss: 4.1622 - val_acc: 0.6813\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 2s 221us/sample - loss: 4.4163 - acc: 0.6439\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 4.2071 - acc: 0.6702 - val_loss: 4.4154 - val_acc: 0.6439\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 2s 221us/sample - loss: 4.1522 - acc: 0.7000\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 4.1954 - acc: 0.6814 - val_loss: 4.1550 - val_acc: 0.7000\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 2s 217us/sample - loss: 4.1369 - acc: 0.7089\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 4.1881 - acc: 0.6893 - val_loss: 4.1373 - val_acc: 0.7089\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 2s 213us/sample - loss: 4.1022 - acc: 0.7220\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 4.1719 - acc: 0.6953 - val_loss: 4.1016 - val_acc: 0.7220\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 2s 215us/sample - loss: 4.1127 - acc: 0.7163\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 4.1675 - acc: 0.7043 - val_loss: 4.1103 - val_acc: 0.7163\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 2s 212us/sample - loss: 4.1816 - acc: 0.7078\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 4.1675 - acc: 0.7086 - val_loss: 4.1808 - val_acc: 0.7078\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 2s 222us/sample - loss: 4.1668 - acc: 0.7293\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 4.1613 - acc: 0.7142 - val_loss: 4.1622 - val_acc: 0.7293\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 2s 213us/sample - loss: 4.2242 - acc: 0.6880\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 4.1633 - acc: 0.7171 - val_loss: 4.2238 - val_acc: 0.6880\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 2s 214us/sample - loss: 1.8848 - acc: 0.7716\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 1.9390 - acc: 0.7618 - val_loss: 1.8836 - val_acc: 0.7716\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 2s 214us/sample - loss: 1.9060 - acc: 0.7616\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 1.8936 - acc: 0.7690 - val_loss: 1.9059 - val_acc: 0.7616\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 2s 226us/sample - loss: 1.8465 - acc: 0.7713\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 1.8762 - acc: 0.7755 - val_loss: 1.8465 - val_acc: 0.7713\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 2s 210us/sample - loss: 1.8442 - acc: 0.7772\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 1.8678 - acc: 0.7786 - val_loss: 1.8435 - val_acc: 0.7772\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 2s 217us/sample - loss: 1.8458 - acc: 0.7841\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 1.8590 - acc: 0.7845 - val_loss: 1.8450 - val_acc: 0.7841\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 2s 213us/sample - loss: 1.8512 - acc: 0.7874\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 1.8500 - acc: 0.7858 - val_loss: 1.8500 - val_acc: 0.7874\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 2s 210us/sample - loss: 1.8605 - acc: 0.7778\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 1.8468 - acc: 0.7883 - val_loss: 1.8610 - val_acc: 0.7778\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 2s 217us/sample - loss: 1.8216 - acc: 0.7940\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 1.8446 - acc: 0.7896 - val_loss: 1.8201 - val_acc: 0.7940\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 2s 227us/sample - loss: 1.8527 - acc: 0.7869\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 1.8330 - acc: 0.7947 - val_loss: 1.8507 - val_acc: 0.7869\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 2s 226us/sample - loss: 1.8145 - acc: 0.7994\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 1.8289 - acc: 0.7975 - val_loss: 1.8137 - val_acc: 0.7994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 2s 209us/sample - loss: 1.8767 - acc: 0.7780\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 1.8266 - acc: 0.7984 - val_loss: 1.8800 - val_acc: 0.7780\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 2s 230us/sample - loss: 1.8716 - acc: 0.7872s \n",
      "391/391 [==============================] - 33s 85ms/step - loss: 1.8207 - acc: 0.8016 - val_loss: 1.8715 - val_acc: 0.7872\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 2s 209us/sample - loss: 1.8528 - acc: 0.7880\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 1.8090 - acc: 0.8042 - val_loss: 1.8562 - val_acc: 0.7880\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 2s 212us/sample - loss: 1.8589 - acc: 0.7866\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 1.8073 - acc: 0.8055 - val_loss: 1.8591 - val_acc: 0.7866\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 2s 215us/sample - loss: 1.8082 - acc: 0.8047\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 1.8070 - acc: 0.8063 - val_loss: 1.8066 - val_acc: 0.8047\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 2s 222us/sample - loss: 1.8111 - acc: 0.8041\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 1.7956 - acc: 0.8106 - val_loss: 1.8117 - val_acc: 0.8041\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 2s 215us/sample - loss: 1.8436 - acc: 0.7870\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 1.7973 - acc: 0.8120 - val_loss: 1.8456 - val_acc: 0.7870\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 2s 222us/sample - loss: 1.8614 - acc: 0.7944\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 1.7905 - acc: 0.8141 - val_loss: 1.8619 - val_acc: 0.7944\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 2s 219us/sample - loss: 1.8256 - acc: 0.7991\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 1.7902 - acc: 0.8145 - val_loss: 1.8246 - val_acc: 0.7991\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 2s 222us/sample - loss: 1.8629 - acc: 0.7959\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 1.7847 - acc: 0.8152 - val_loss: 1.8649 - val_acc: 0.7959\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 2s 216us/sample - loss: 1.8429 - acc: 0.7977\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 1.7791 - acc: 0.8193 - val_loss: 1.8410 - val_acc: 0.7977\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 2s 225us/sample - loss: 1.8285 - acc: 0.7999\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 1.7765 - acc: 0.8193 - val_loss: 1.8262 - val_acc: 0.7999\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 2s 214us/sample - loss: 1.8331 - acc: 0.8035\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 1.7725 - acc: 0.8206 - val_loss: 1.8328 - val_acc: 0.8035\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 2s 214us/sample - loss: 1.8562 - acc: 0.7910\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 1.7717 - acc: 0.8223 - val_loss: 1.8560 - val_acc: 0.7910\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 2s 219us/sample - loss: 1.8615 - acc: 0.7958\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 1.7659 - acc: 0.8254 - val_loss: 1.8622 - val_acc: 0.7958\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 2s 216us/sample - loss: 1.1636 - acc: 0.8125\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 1.1122 - acc: 0.8356 - val_loss: 1.1614 - val_acc: 0.8125\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 2s 227us/sample - loss: 1.1470 - acc: 0.8113\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 1.0842 - acc: 0.8401 - val_loss: 1.1462 - val_acc: 0.8113\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 2s 213us/sample - loss: 1.1492 - acc: 0.8151\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 1.0800 - acc: 0.8416 - val_loss: 1.1485 - val_acc: 0.8151\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 2s 224us/sample - loss: 1.1488 - acc: 0.8169\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 1.0730 - acc: 0.8446 - val_loss: 1.1467 - val_acc: 0.8169\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 2s 215us/sample - loss: 1.1610 - acc: 0.8139\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 1.0681 - acc: 0.8462 - val_loss: 1.1607 - val_acc: 0.8139\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 2s 223us/sample - loss: 1.1569 - acc: 0.8170\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 1.0702 - acc: 0.8454 - val_loss: 1.1557 - val_acc: 0.8170\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 2s 214us/sample - loss: 1.1695 - acc: 0.8149\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 1.0623 - acc: 0.8474 - val_loss: 1.1683 - val_acc: 0.8149\n",
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 2s 219us/sample - loss: 1.1596 - acc: 0.8181\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 1.0664 - acc: 0.8463 - val_loss: 1.1593 - val_acc: 0.8181\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 2s 212us/sample - loss: 1.1490 - acc: 0.8204\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 1.0652 - acc: 0.8464 - val_loss: 1.1476 - val_acc: 0.8204\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 2s 214us/sample - loss: 1.1696 - acc: 0.8142\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 1.0643 - acc: 0.8465 - val_loss: 1.1682 - val_acc: 0.8142\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 2s 216us/sample - loss: 1.1583 - acc: 0.8165\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 1.0599 - acc: 0.8493 - val_loss: 1.1587 - val_acc: 0.8165\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 2s 213us/sample - loss: 1.1525 - acc: 0.8180\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 1.0578 - acc: 0.8486 - val_loss: 1.1517 - val_acc: 0.8180\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 2s 217us/sample - loss: 1.1567 - acc: 0.8174\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 1.0562 - acc: 0.8509 - val_loss: 1.1558 - val_acc: 0.8174\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 2s 212us/sample - loss: 1.1587 - acc: 0.8163\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 1.0507 - acc: 0.8514 - val_loss: 1.1576 - val_acc: 0.8163\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 2s 212us/sample - loss: 1.1554 - acc: 0.8169\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 1.0484 - acc: 0.8530 - val_loss: 1.1540 - val_acc: 0.8169\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 2s 223us/sample - loss: 1.1622 - acc: 0.8182\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 1.0512 - acc: 0.8522 - val_loss: 1.1620 - val_acc: 0.8182\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 2s 215us/sample - loss: 1.1645 - acc: 0.8147\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 1.0435 - acc: 0.8553 - val_loss: 1.1639 - val_acc: 0.8147\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 2s 213us/sample - loss: 1.1696 - acc: 0.8153\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 1.0452 - acc: 0.8545 - val_loss: 1.1681 - val_acc: 0.8153\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 2s 217us/sample - loss: 1.1491 - acc: 0.8210\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 1.0501 - acc: 0.8525 - val_loss: 1.1474 - val_acc: 0.8210\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 2s 217us/sample - loss: 1.1573 - acc: 0.8232\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 1.0447 - acc: 0.8546 - val_loss: 1.1555 - val_acc: 0.8232\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 2s 216us/sample - loss: 1.1342 - acc: 0.8279\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 1.0364 - acc: 0.8584 - val_loss: 1.1324 - val_acc: 0.8279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 2s 215us/sample - loss: 1.1580 - acc: 0.8186\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 1.0352 - acc: 0.8595 - val_loss: 1.1567 - val_acc: 0.8186\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 2s 216us/sample - loss: 1.1648 - acc: 0.8192\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 1.0375 - acc: 0.8581 - val_loss: 1.1637 - val_acc: 0.8192\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 2s 217us/sample - loss: 1.1641 - acc: 0.8137\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 1.0338 - acc: 0.8573 - val_loss: 1.1642 - val_acc: 0.8137\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 2s 216us/sample - loss: 1.1525 - acc: 0.8206\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 1.0306 - acc: 0.8601 - val_loss: 1.1525 - val_acc: 0.8206\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 2s 217us/sample - loss: 0.9051 - acc: 0.8205\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 0.7849 - acc: 0.8650 - val_loss: 0.9038 - val_acc: 0.8205\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 2s 212us/sample - loss: 0.8960 - acc: 0.8238\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.7710 - acc: 0.8627 - val_loss: 0.8949 - val_acc: 0.8238\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 2s 217us/sample - loss: 0.9039 - acc: 0.8213\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 0.7700 - acc: 0.8639 - val_loss: 0.9033 - val_acc: 0.8213\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 2s 212us/sample - loss: 0.9058 - acc: 0.8239\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 0.7628 - acc: 0.8668 - val_loss: 0.9045 - val_acc: 0.8239\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 2s 213us/sample - loss: 0.8987 - acc: 0.8228\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 0.7600 - acc: 0.8679 - val_loss: 0.8972 - val_acc: 0.8228\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 2s 215us/sample - loss: 0.8928 - acc: 0.8228\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.7710 - acc: 0.8644 - val_loss: 0.8916 - val_acc: 0.8228\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 2s 213us/sample - loss: 0.9060 - acc: 0.8223\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 0.7657 - acc: 0.8660 - val_loss: 0.9057 - val_acc: 0.8223\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 2s 216us/sample - loss: 0.8948 - acc: 0.8257\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 0.7608 - acc: 0.8672 - val_loss: 0.8936 - val_acc: 0.8257\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 2s 217us/sample - loss: 0.9082 - acc: 0.8214\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.7612 - acc: 0.8683 - val_loss: 0.9076 - val_acc: 0.8214\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 2s 215us/sample - loss: 0.9072 - acc: 0.8231\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.7568 - acc: 0.8679 - val_loss: 0.9059 - val_acc: 0.8231\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 2s 214us/sample - loss: 0.9014 - acc: 0.8238\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.7544 - acc: 0.8706 - val_loss: 0.9000 - val_acc: 0.8238\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 2s 213us/sample - loss: 0.9050 - acc: 0.8229\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.7607 - acc: 0.8673 - val_loss: 0.9047 - val_acc: 0.8229\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 2s 229us/sample - loss: 0.9094 - acc: 0.8230\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.7552 - acc: 0.8688 - val_loss: 0.9085 - val_acc: 0.8230\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 2s 219us/sample - loss: 0.8965 - acc: 0.8242\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.7603 - acc: 0.8670 - val_loss: 0.8951 - val_acc: 0.8242\n",
      "Epoch 91/100\n",
      "10000/10000 [==============================] - 2s 217us/sample - loss: 0.8965 - acc: 0.8257\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.7578 - acc: 0.8678 - val_loss: 0.8947 - val_acc: 0.8257\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 2s 216us/sample - loss: 0.8997 - acc: 0.8240\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.7565 - acc: 0.8691 - val_loss: 0.8984 - val_acc: 0.8240\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 2s 216us/sample - loss: 0.8933 - acc: 0.8274\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 0.7589 - acc: 0.8689 - val_loss: 0.8923 - val_acc: 0.8274\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 2s 217us/sample - loss: 0.9072 - acc: 0.8254\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 0.7510 - acc: 0.8709 - val_loss: 0.9062 - val_acc: 0.8254\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 2s 218us/sample - loss: 0.9004 - acc: 0.8240\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 0.7519 - acc: 0.8701 - val_loss: 0.8998 - val_acc: 0.8240\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 2s 214us/sample - loss: 0.9048 - acc: 0.8221\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 0.7506 - acc: 0.8707 - val_loss: 0.9038 - val_acc: 0.8221\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 2s 213us/sample - loss: 0.9045 - acc: 0.8239\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.7490 - acc: 0.8698 - val_loss: 0.9040 - val_acc: 0.8239\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 2s 216us/sample - loss: 0.8983 - acc: 0.8243\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 0.7446 - acc: 0.8743 - val_loss: 0.8972 - val_acc: 0.8243\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 2s 214us/sample - loss: 0.9129 - acc: 0.8205\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 0.7521 - acc: 0.8711 - val_loss: 0.9128 - val_acc: 0.8205\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 2s 213us/sample - loss: 0.8949 - acc: 0.8247\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.7489 - acc: 0.8713 - val_loss: 0.8942 - val_acc: 0.8247\n"
     ]
    }
   ],
   "source": [
    "logs = model.fit(data_generator.flow(x_train, y_train, batch_size=128), epochs = 100, validation_data = (x_test, y_test), batch_size=128, callbacks = [LearningRateScheduler(lr_schedule)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 6s 601us/sample - loss: 0.8949 - acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8948538196563721, 0.8247]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(**kwargs):\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    for k, v in kwargs.items():\n",
    "        if k != 'name' and k != 'filename':\n",
    "            plt.plot(v, label=k)\n",
    "    plt.grid(True)\n",
    "    if 'name' in kwargs:\n",
    "        plt.title(kwargs['name'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend(loc='lower right')\n",
    "    if 'filename' in kwargs:\n",
    "        plt.savefig(kwargs['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "IuCr92B__UGg"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_training(training = logs.history['acc'],\n",
    "              validation = logs.history['val_acc'],\n",
    "              name = 'Accuracy_rmsprop',\n",
    "              filename = 'acc_rmsprop_resnet_100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
