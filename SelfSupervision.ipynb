{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2960,
     "status": "ok",
     "timestamp": 1605689381252,
     "user": {
      "displayName": "Alexander Dimitrienko",
      "photoUrl": "",
      "userId": "07582037467622387890"
     },
     "user_tz": -180
    },
    "id": "i2WkWgBL4CUB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import losses, layers, models, metrics, Model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn import svm, metrics\n",
    "import PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2558,
     "status": "ok",
     "timestamp": 1605689381253,
     "user": {
      "displayName": "Alexander Dimitrienko",
      "photoUrl": "",
      "userId": "07582037467622387890"
     },
     "user_tz": -180
    },
    "id": "ncVLyOSa4CUF"
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5452,
     "status": "ok",
     "timestamp": 1605689384733,
     "user": {
      "displayName": "Alexander Dimitrienko",
      "photoUrl": "",
      "userId": "07582037467622387890"
     },
     "user_tz": -180
    },
    "id": "hUGBTkHz4CUJ"
   },
   "outputs": [],
   "source": [
    "files = []\n",
    "for i in range(6):\n",
    "    name =  ''\n",
    "    if i == 5:\n",
    "        name = 'cifar-10-batches-py/test_batch'\n",
    "    else:\n",
    "        name = 'cifar-10-batches-py/data_batch_' + str(i + 1)\n",
    "    files.append(unpickle(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5116,
     "status": "ok",
     "timestamp": 1605689384734,
     "user": {
      "displayName": "Alexander Dimitrienko",
      "photoUrl": "",
      "userId": "07582037467622387890"
     },
     "user_tz": -180
    },
    "id": "kEB_4w7t4CUM"
   },
   "outputs": [],
   "source": [
    "rotations_num = 4\n",
    "augment_num = 10\n",
    "\n",
    "saved_name = 'selfsupervised'\n",
    "cnn_name = 'mycnn'\n",
    "\n",
    "selfsupervised_epochs = 80\n",
    "selfsupervised_batch_size = 128\n",
    "supervised_epochs = 50\n",
    "supervised_batch_size = 128\n",
    "supervised_trainval_ratio = 1. / 6 #1/6 is implied in CIFAR-10\n",
    "\n",
    "feature_layer_trained = 'conv2_block3_out'\n",
    "feature_layer = 'conv2_block3_out'\n",
    "first_resnet_layer = 'conv2_block1_out'\n",
    "second_resnet_layer = 'conv2_block2_out'\n",
    "feature_layer_cnn = 'out_layer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5203,
     "status": "ok",
     "timestamp": 1605689385208,
     "user": {
      "displayName": "Alexander Dimitrienko",
      "photoUrl": "",
      "userId": "07582037467622387890"
     },
     "user_tz": -180
    },
    "id": "PpYC2nYp4CUP"
   },
   "outputs": [],
   "source": [
    "combined_data = np.zeros((60000 * rotations_num, 32, 32, 3), dtype = np.float32)\n",
    "combined_labels = np.zeros((60000 * rotations_num, rotations_num), dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4779,
     "status": "ok",
     "timestamp": 1605689385208,
     "user": {
      "displayName": "Alexander Dimitrienko",
      "photoUrl": "",
      "userId": "07582037467622387890"
     },
     "user_tz": -180
    },
    "id": "76MQkXH-YTkq"
   },
   "outputs": [],
   "source": [
    "true_labels = np.zeros((60000 * rotations_num, 10), dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 589508,
     "status": "ok",
     "timestamp": 1605689970454,
     "user": {
      "displayName": "Alexander Dimitrienko",
      "photoUrl": "",
      "userId": "07582037467622387890"
     },
     "user_tz": -180
    },
    "id": "qDJWyucW4CUT"
   },
   "outputs": [],
   "source": [
    "for i in range(len(files)):\n",
    "    data_len = files[i][b\"data\"].shape[0]\n",
    "    \n",
    "    for j in range(data_len):\n",
    "        row = files[i][b\"data\"][j]\n",
    "        true_labels[rotations_num * (data_len * i + j)][files[i][b\"labels\"][j]] = 1.\n",
    "        \n",
    "        for k in range(files[i][b\"data\"].shape[1]):\n",
    "            combined_data[rotations_num * (data_len * i + j)][(k & 1023) >> 5][k & 31][k >> 10] = row[k]\n",
    "        combined_labels[rotations_num * (data_len * i + j)][0] = 1.\n",
    "        \n",
    "        for t in range(1, rotations_num):\n",
    "            true_labels[rotations_num * (data_len * i + j) + t][files[i][b\"labels\"][j]] = 1.\n",
    "            combined_data[rotations_num * (data_len * i + j) + t] = np.rot90(combined_data[rotations_num * (data_len * i + j)], t)\n",
    "            combined_labels[rotations_num * (data_len * i + j) + t][t] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "executionInfo": {
     "elapsed": 589036,
     "status": "ok",
     "timestamp": 1605689970458,
     "user": {
      "displayName": "Alexander Dimitrienko",
      "photoUrl": "",
      "userId": "07582037467622387890"
     },
     "user_tz": -180
    },
    "id": "5alMJ4a_4CUV",
    "outputId": "58c46fee-16c9-4b80-98a9-04f0d66459c2",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "r = random.randint(0, 60000)\n",
    "\n",
    "for i in range(rotations_num):\n",
    "    plt.imshow(combined_data[r * rotations_num + i].astype(int))\n",
    "    plt.show()\n",
    "    print(combined_labels[r * rotations_num + i])\n",
    "    print(true_labels[r * rotations_num + i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_data = preprocess_input(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qHtoMw2t4CUb"
   },
   "outputs": [],
   "source": [
    "def plot_training(**kwargs):\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    \n",
    "    for k, v in kwargs.items():\n",
    "        if k != 'name' and k != 'filename':\n",
    "            plt.plot(v, label=k)\n",
    "            \n",
    "    plt.grid(True)\n",
    "    if 'name' in kwargs:\n",
    "        plt.title(kwargs['name'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend(loc='lower right')\n",
    "    if 'filename' in kwargs:\n",
    "        plt.savefig(kwargs['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kPX0I6P74CUd"
   },
   "outputs": [],
   "source": [
    "def get_feat_model():\n",
    "    inputs = tf.keras.Input((32, 32, 3))\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.4, name='out_layer')(x)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(200, kernel_regularizer=regularizers.l2(l = 0.0001), kernel_initializer='he_uniform')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(200, kernel_regularizer=regularizers.l2(l = 0.0001), kernel_initializer='he_uniform')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(rotations_num, kernel_regularizer=regularizers.l2(l = 0.0001), activation = 'softmax')(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs = inputs, outputs = x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_model():\n",
    "    base = ResNet50(include_top=False, weights=None, input_shape=(32, 32, 3))\n",
    "    l = base.get_layer(feature_layer_trained).output\n",
    "    l = layers.Flatten()(l)\n",
    "    l = layers.BatchNormalization()(l)\n",
    "    l = layers.Dropout(0.5)(l)\n",
    "    l = layers.Dense(200, kernel_regularizer=regularizers.l1_l2(l1 = 0.0001, l2 = 0.0005), kernel_initializer='he_uniform')(l)\n",
    "    l = layers.BatchNormalization()(l)\n",
    "    l = layers.Activation('relu')(l)\n",
    "    l = layers.Dropout(0.5)(l)\n",
    "    l = layers.Dense(200, kernel_regularizer=regularizers.l1_l2(l1 = 0.0001, l2 = 0.0005), kernel_initializer='he_uniform')(l)\n",
    "    l = layers.BatchNormalization()(l)\n",
    "    l = layers.Activation('relu')(l)\n",
    "    l = layers.Dropout(0.5)(l)\n",
    "    l = layers.Dense(rotations_num, kernel_regularizer=regularizers.l1_l2(l1 = 0.00005, l2 = 0.0001), activation = 'softmax')(l)\n",
    "    \n",
    "    return tf.keras.Model(inputs = base.input, outputs = l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch == 50:\n",
    "        return lr / 5.\n",
    "    if epoch == 40:\n",
    "        return lr / 5.\n",
    "    if epoch == 30:\n",
    "        return lr / 5.\n",
    "    \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule_conv(epoch, lr):\n",
    "    print(lr)\n",
    "    if epoch == 80:\n",
    "        return lr / 5.\n",
    "    if epoch == 60:\n",
    "        return lr / 5.\n",
    "    if epoch == 40:\n",
    "        return lr / 5.\n",
    "    if epoch == 20:\n",
    "        return lr / 5.\n",
    "    \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule_cls(epoch, lr):\n",
    "    print(lr)\n",
    "    if epoch == 40:\n",
    "        return lr / 5.\n",
    "    if epoch == 30:\n",
    "        return lr / 5.\n",
    "    if epoch == 20:\n",
    "        return lr / 5.\n",
    "    if epoch == 10:\n",
    "        return lr / 5.\n",
    "    \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule_linear(epoch, lr):\n",
    "    print(lr)\n",
    "    if epoch < supervised_epochs // 2:\n",
    "        return lr + (0.1 - 0.00001) / supervised_epochs * 2.\n",
    "    else:\n",
    "        return max(lr - (0.1 - 0.00001) / supervised_epochs * 2., 0.00001)\n",
    "    \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "rqvEEjX74CUg"
   },
   "outputs": [],
   "source": [
    "def train_feat():\n",
    "    model = get_conv_model()\n",
    "    model.summary()\n",
    "    model.compile(optimizer = optimizers.RMSprop(), loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    log = model.fit(combined_data, combined_labels, \n",
    "                    epochs = selfsupervised_epochs, batch_size = selfsupervised_batch_size, \n",
    "                    shuffle = True, callbacks = [LearningRateScheduler(lr_schedule)])\n",
    "    model.save(saved_name)\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "n5CGkQvR4CUj",
    "outputId": "120adc8d-7351-475e-aa82-25474b2b1295",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 16384)        0           conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16384)        65536       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 16384)        0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 200)          3277000     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 200)          800         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 200)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200)          0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 200)          40200       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 200)          800         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 200)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 200)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            804         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,614,900\n",
      "Trainable params: 3,578,388\n",
      "Non-trainable params: 36,512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240000 samples\n",
      "Epoch 1/80\n",
      "240000/240000 [==============================] - 40s 169us/sample - loss: 2.1408 - accuracy: 0.5871\n",
      "Epoch 2/80\n",
      "240000/240000 [==============================] - 37s 153us/sample - loss: 1.5825 - accuracy: 0.6723\n",
      "Epoch 3/80\n",
      "240000/240000 [==============================] - 37s 153us/sample - loss: 1.4982 - accuracy: 0.7153\n",
      "Epoch 4/80\n",
      "240000/240000 [==============================] - 37s 154us/sample - loss: 1.4170 - accuracy: 0.7436\n",
      "Epoch 5/80\n",
      "240000/240000 [==============================] - 37s 154us/sample - loss: 1.3465 - accuracy: 0.7623\n",
      "Epoch 6/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 1.2843 - accuracy: 0.7771\n",
      "Epoch 7/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 1.2277 - accuracy: 0.7907\n",
      "Epoch 8/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 1.1803 - accuracy: 0.8016\n",
      "Epoch 9/80\n",
      "240000/240000 [==============================] - 37s 152us/sample - loss: 1.1338 - accuracy: 0.8110\n",
      "Epoch 10/80\n",
      "240000/240000 [==============================] - 37s 154us/sample - loss: 1.0908 - accuracy: 0.8198\n",
      "Epoch 11/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 1.0541 - accuracy: 0.8272\n",
      "Epoch 12/80\n",
      "240000/240000 [==============================] - 37s 154us/sample - loss: 1.0196 - accuracy: 0.8341\n",
      "Epoch 13/80\n",
      "240000/240000 [==============================] - 37s 156us/sample - loss: 0.9900 - accuracy: 0.8395\n",
      "Epoch 14/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.9626 - accuracy: 0.8459\n",
      "Epoch 15/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.9369 - accuracy: 0.8506\n",
      "Epoch 16/80\n",
      "240000/240000 [==============================] - 37s 156us/sample - loss: 0.9136 - accuracy: 0.8556\n",
      "Epoch 17/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.8942 - accuracy: 0.8598\n",
      "Epoch 18/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.8738 - accuracy: 0.8641\n",
      "Epoch 19/80\n",
      "240000/240000 [==============================] - 37s 156us/sample - loss: 0.8588 - accuracy: 0.8677\n",
      "Epoch 20/80\n",
      "240000/240000 [==============================] - 37s 154us/sample - loss: 0.8408 - accuracy: 0.8722\n",
      "Epoch 21/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.8269 - accuracy: 0.8750\n",
      "Epoch 22/80\n",
      "240000/240000 [==============================] - 37s 153us/sample - loss: 0.8138 - accuracy: 0.8784\n",
      "Epoch 23/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.8000 - accuracy: 0.8815\n",
      "Epoch 24/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.7890 - accuracy: 0.8839\n",
      "Epoch 25/80\n",
      "240000/240000 [==============================] - 37s 156us/sample - loss: 0.7744 - accuracy: 0.8872\n",
      "Epoch 26/80\n",
      "240000/240000 [==============================] - 37s 156us/sample - loss: 0.7643 - accuracy: 0.8913\n",
      "Epoch 27/80\n",
      "240000/240000 [==============================] - 37s 154us/sample - loss: 0.7549 - accuracy: 0.8928\n",
      "Epoch 28/80\n",
      "240000/240000 [==============================] - 37s 153us/sample - loss: 0.7455 - accuracy: 0.8957\n",
      "Epoch 29/80\n",
      "240000/240000 [==============================] - 38s 156us/sample - loss: 0.7361 - accuracy: 0.8980\n",
      "Epoch 30/80\n",
      "240000/240000 [==============================] - 37s 156us/sample - loss: 0.7279 - accuracy: 0.9002\n",
      "Epoch 31/80\n",
      "240000/240000 [==============================] - 37s 153us/sample - loss: 0.3696 - accuracy: 0.9343\n",
      "Epoch 32/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.3348 - accuracy: 0.9424\n",
      "Epoch 33/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.3252 - accuracy: 0.9457\n",
      "Epoch 34/80\n",
      "240000/240000 [==============================] - 38s 157us/sample - loss: 0.3176 - accuracy: 0.9484\n",
      "Epoch 35/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.3103 - accuracy: 0.9511\n",
      "Epoch 36/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.3068 - accuracy: 0.9523\n",
      "Epoch 37/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.3011 - accuracy: 0.9542\n",
      "Epoch 38/80\n",
      "240000/240000 [==============================] - 37s 154us/sample - loss: 0.2974 - accuracy: 0.9556\n",
      "Epoch 39/80\n",
      "240000/240000 [==============================] - 37s 154us/sample - loss: 0.2950 - accuracy: 0.9562\n",
      "Epoch 40/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.2896 - accuracy: 0.9587\n",
      "Epoch 41/80\n",
      "240000/240000 [==============================] - 38s 156us/sample - loss: 0.1890 - accuracy: 0.9665\n",
      "Epoch 42/80\n",
      "240000/240000 [==============================] - 37s 156us/sample - loss: 0.1663 - accuracy: 0.9692\n",
      "Epoch 43/80\n",
      "240000/240000 [==============================] - 37s 153us/sample - loss: 0.1612 - accuracy: 0.9695\n",
      "Epoch 44/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.1573 - accuracy: 0.9708\n",
      "Epoch 45/80\n",
      "240000/240000 [==============================] - 37s 154us/sample - loss: 0.1581 - accuracy: 0.9703\n",
      "Epoch 46/80\n",
      "240000/240000 [==============================] - 37s 154us/sample - loss: 0.1584 - accuracy: 0.9704\n",
      "Epoch 47/80\n",
      "240000/240000 [==============================] - 37s 154us/sample - loss: 0.1558 - accuracy: 0.9713\n",
      "Epoch 48/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.1545 - accuracy: 0.9714\n",
      "Epoch 49/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.1526 - accuracy: 0.9722\n",
      "Epoch 50/80\n",
      "240000/240000 [==============================] - 37s 153us/sample - loss: 0.1522 - accuracy: 0.9727\n",
      "Epoch 51/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.1252 - accuracy: 0.9750\n",
      "Epoch 52/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.1171 - accuracy: 0.9753\n",
      "Epoch 53/80\n",
      "240000/240000 [==============================] - 37s 154us/sample - loss: 0.1132 - accuracy: 0.9761\n",
      "Epoch 54/80\n",
      "240000/240000 [==============================] - 37s 154us/sample - loss: 0.1114 - accuracy: 0.9760\n",
      "Epoch 55/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.1102 - accuracy: 0.9757\n",
      "Epoch 56/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.1090 - accuracy: 0.9761\n",
      "Epoch 57/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.1087 - accuracy: 0.9756\n",
      "Epoch 58/80\n",
      "240000/240000 [==============================] - 37s 156us/sample - loss: 0.1062 - accuracy: 0.9765\n",
      "Epoch 59/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.1067 - accuracy: 0.9762\n",
      "Epoch 60/80\n",
      "240000/240000 [==============================] - 37s 154us/sample - loss: 0.1063 - accuracy: 0.9763\n",
      "Epoch 61/80\n",
      "240000/240000 [==============================] - 37s 153us/sample - loss: 0.1077 - accuracy: 0.9761\n",
      "Epoch 62/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.1051 - accuracy: 0.9766\n",
      "Epoch 63/80\n",
      "240000/240000 [==============================] - 37s 154us/sample - loss: 0.1043 - accuracy: 0.9767\n",
      "Epoch 64/80\n",
      "240000/240000 [==============================] - 37s 154us/sample - loss: 0.1062 - accuracy: 0.9761\n",
      "Epoch 65/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.1045 - accuracy: 0.9766\n",
      "Epoch 66/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.1039 - accuracy: 0.9769\n",
      "Epoch 67/80\n",
      "240000/240000 [==============================] - 37s 154us/sample - loss: 0.1042 - accuracy: 0.9767\n",
      "Epoch 68/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.1036 - accuracy: 0.9771\n",
      "Epoch 69/80\n",
      "240000/240000 [==============================] - 37s 154us/sample - loss: 0.1029 - accuracy: 0.9770\n",
      "Epoch 70/80\n",
      "240000/240000 [==============================] - 37s 154us/sample - loss: 0.1046 - accuracy: 0.9764\n",
      "Epoch 71/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.1029 - accuracy: 0.9772\n",
      "Epoch 72/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.1027 - accuracy: 0.9774\n",
      "Epoch 73/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.1022 - accuracy: 0.9771\n",
      "Epoch 74/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240000/240000 [==============================] - 36s 152us/sample - loss: 0.1021 - accuracy: 0.9773\n",
      "Epoch 75/80\n",
      "240000/240000 [==============================] - 37s 154us/sample - loss: 0.1011 - accuracy: 0.9777\n",
      "Epoch 76/80\n",
      "240000/240000 [==============================] - 37s 153us/sample - loss: 0.1015 - accuracy: 0.9776\n",
      "Epoch 77/80\n",
      "240000/240000 [==============================] - 37s 153us/sample - loss: 0.1009 - accuracy: 0.9779\n",
      "Epoch 78/80\n",
      "240000/240000 [==============================] - 37s 154us/sample - loss: 0.1002 - accuracy: 0.9776\n",
      "Epoch 79/80\n",
      "240000/240000 [==============================] - 37s 153us/sample - loss: 0.1008 - accuracy: 0.9776\n",
      "Epoch 80/80\n",
      "240000/240000 [==============================] - 37s 155us/sample - loss: 0.1002 - accuracy: 0.9778\n",
      "WARNING:tensorflow:From /home/student/anaconda3/envs/sayali/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: selfsupervised/assets\n"
     ]
    }
   ],
   "source": [
    "log = train_feat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "3_4GXBHL4CUn"
   },
   "outputs": [],
   "source": [
    "def get_cls_model(name = saved_name, use_features = True, percent = 100):\n",
    "    if use_features:\n",
    "        model = tf.keras.models.load_model(name)\n",
    "    else:\n",
    "        model = tf.keras.models.load_model('emptyResNet')\n",
    "        \n",
    "    if percent >= 50:\n",
    "        l = model.get_layer(feature_layer).output\n",
    "    else:\n",
    "        l = model.get_layer(first_resnet_layer).output\n",
    "        \n",
    "    #model = tf.keras.models.load_model(cnn_name)\n",
    "    #l = model.get_layer(feature_layer_cnn).output\n",
    "    \n",
    "    l = layers.Flatten()(l)\n",
    "    l = layers.BatchNormalization()(l)\n",
    "    l = layers.Dropout(0.5)(l)\n",
    "    l = layers.Dense(200, kernel_regularizer=regularizers.l1_l2(l1 = 0.0001, l2 = 0.0005), kernel_initializer='he_uniform')(l)\n",
    "    l = layers.BatchNormalization()(l)\n",
    "    l = layers.Activation('relu')(l)\n",
    "    l = layers.Dropout(0.5)(l)\n",
    "    l = layers.Dense(200, kernel_regularizer=regularizers.l1_l2(l1 = 0.0001, l2 = 0.0005), kernel_initializer='he_uniform')(l)\n",
    "    l = layers.BatchNormalization()(l)\n",
    "    l = layers.Activation('relu')(l)\n",
    "    l = layers.Dropout(0.5)(l)\n",
    "    l = layers.Dense(10, kernel_regularizer=regularizers.l1_l2(l1 = 0.0001, l2 = 0.0005), activation = 'softmax')(l)\n",
    "    return tf.keras.Model(inputs = model.input, outputs = l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "JwBUGYKx4CUq"
   },
   "outputs": [],
   "source": [
    "def train_cls(train_features = True):\n",
    "    cls_model = get_cls_model()\n",
    "    cls_model.summary()\n",
    "    \n",
    "    if not train_features:\n",
    "        for l in cls_model.layers[:-5]:\n",
    "            l.trainable = False\n",
    "            \n",
    "    cls_model.compile(optimizer = optimizers.RMSprop(), loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    cls_log = cls_model.fit(combined_data[:int(60000 * (1.0 - supervised_trainval_ratio))], \n",
    "                            true_labels[:int(60000 * (1.0 - supervised_trainval_ratio))], \n",
    "                            epochs = supervised_epochs, batch_size = supervised_batch_size, shuffle = True,\n",
    "                            callbacks = [LearningRateScheduler(lr_schedule)],\n",
    "                            validation_data = (combined_data[int(60000 * (1.0 - supervised_trainval_ratio)):60000], \n",
    "                                               true_labels[int(60000 * (1.0 - supervised_trainval_ratio)):60000]))\n",
    "    return cls_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "EDLC9hEw4CUs",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 16384)        0           conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16384)        65536       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 16384)        0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 200)          3277000     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 200)          800         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 200)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 200)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 200)          40200       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 200)          800         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 200)          0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 200)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           2010        dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,616,106\n",
      "Trainable params: 3,579,594\n",
      "Non-trainable params: 36,512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 12s 240us/sample - loss: 4.3538 - accuracy: 0.4132 - val_loss: 3.4000 - val_accuracy: 0.4934\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 9s 179us/sample - loss: 3.0372 - accuracy: 0.5451 - val_loss: 3.0208 - val_accuracy: 0.5107\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 9s 178us/sample - loss: 2.7176 - accuracy: 0.5967 - val_loss: 2.9609 - val_accuracy: 0.5103\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 9s 180us/sample - loss: 2.5486 - accuracy: 0.6303 - val_loss: 2.6063 - val_accuracy: 0.6041\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 9s 178us/sample - loss: 2.4448 - accuracy: 0.6555 - val_loss: 2.6550 - val_accuracy: 0.6009\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 9s 177us/sample - loss: 2.3783 - accuracy: 0.6745 - val_loss: 2.8766 - val_accuracy: 0.5514\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 9s 174us/sample - loss: 2.3170 - accuracy: 0.6932 - val_loss: 2.5300 - val_accuracy: 0.6340\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 9s 182us/sample - loss: 2.2616 - accuracy: 0.7122 - val_loss: 2.6846 - val_accuracy: 0.5998\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 9s 183us/sample - loss: 2.2280 - accuracy: 0.7230 - val_loss: 2.6394 - val_accuracy: 0.6197\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 9s 178us/sample - loss: 2.1979 - accuracy: 0.7386 - val_loss: 2.6112 - val_accuracy: 0.6311\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 9s 178us/sample - loss: 2.1659 - accuracy: 0.7502 - val_loss: 2.5639 - val_accuracy: 0.6415\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 9s 176us/sample - loss: 2.1275 - accuracy: 0.7642 - val_loss: 2.8871 - val_accuracy: 0.5985\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 9s 177us/sample - loss: 2.1078 - accuracy: 0.7761 - val_loss: 2.7308 - val_accuracy: 0.6292\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 9s 177us/sample - loss: 2.0738 - accuracy: 0.7843 - val_loss: 2.6703 - val_accuracy: 0.6293\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 9s 175us/sample - loss: 2.0404 - accuracy: 0.7929 - val_loss: 2.9026 - val_accuracy: 0.6093\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 9s 171us/sample - loss: 2.0067 - accuracy: 0.8061 - val_loss: 2.7398 - val_accuracy: 0.6178\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 9s 180us/sample - loss: 1.9858 - accuracy: 0.8137 - val_loss: 2.7311 - val_accuracy: 0.6206\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 9s 181us/sample - loss: 1.9571 - accuracy: 0.8209 - val_loss: 2.7780 - val_accuracy: 0.6285\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 9s 180us/sample - loss: 1.9311 - accuracy: 0.8291 - val_loss: 2.8112 - val_accuracy: 0.6180\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 9s 180us/sample - loss: 1.8987 - accuracy: 0.8377 - val_loss: 2.8109 - val_accuracy: 0.6368\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 9s 179us/sample - loss: 1.8695 - accuracy: 0.8419 - val_loss: 2.7267 - val_accuracy: 0.6430\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 9s 182us/sample - loss: 1.8515 - accuracy: 0.8494 - val_loss: 2.9458 - val_accuracy: 0.5995\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 9s 178us/sample - loss: 1.8204 - accuracy: 0.8589 - val_loss: 2.9380 - val_accuracy: 0.6105\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 9s 178us/sample - loss: 1.7969 - accuracy: 0.8627 - val_loss: 3.1036 - val_accuracy: 0.5930\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 9s 179us/sample - loss: 1.7797 - accuracy: 0.8675 - val_loss: 2.8615 - val_accuracy: 0.6365\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 9s 185us/sample - loss: 1.7499 - accuracy: 0.8726 - val_loss: 3.2660 - val_accuracy: 0.5918\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 9s 176us/sample - loss: 1.7408 - accuracy: 0.8743 - val_loss: 2.9832 - val_accuracy: 0.6197\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 8s 169us/sample - loss: 1.7071 - accuracy: 0.8803 - val_loss: 2.8503 - val_accuracy: 0.6322\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 9s 177us/sample - loss: 1.6827 - accuracy: 0.8833 - val_loss: 3.0513 - val_accuracy: 0.5988\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 9s 182us/sample - loss: 1.6508 - accuracy: 0.8875 - val_loss: 2.9510 - val_accuracy: 0.6207\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 9s 179us/sample - loss: 0.9389 - accuracy: 0.9407 - val_loss: 1.9143 - val_accuracy: 0.6669\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 9s 173us/sample - loss: 0.6581 - accuracy: 0.9537 - val_loss: 2.0175 - val_accuracy: 0.6634\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 9s 179us/sample - loss: 0.6198 - accuracy: 0.9574 - val_loss: 2.0605 - val_accuracy: 0.6630\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 9s 176us/sample - loss: 0.6049 - accuracy: 0.9603 - val_loss: 2.1226 - val_accuracy: 0.6655\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 9s 178us/sample - loss: 0.5962 - accuracy: 0.9626 - val_loss: 2.1679 - val_accuracy: 0.6620\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 9s 181us/sample - loss: 0.5832 - accuracy: 0.9644 - val_loss: 2.1581 - val_accuracy: 0.6601\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 9s 178us/sample - loss: 0.5704 - accuracy: 0.9660 - val_loss: 2.2433 - val_accuracy: 0.6575\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 9s 182us/sample - loss: 0.5738 - accuracy: 0.9667 - val_loss: 2.2687 - val_accuracy: 0.6561\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 9s 182us/sample - loss: 0.5734 - accuracy: 0.9669 - val_loss: 2.2535 - val_accuracy: 0.6584\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 9s 183us/sample - loss: 0.5543 - accuracy: 0.9705 - val_loss: 2.2487 - val_accuracy: 0.6614\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 9s 179us/sample - loss: 0.4260 - accuracy: 0.9791 - val_loss: 2.0630 - val_accuracy: 0.6675\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 9s 173us/sample - loss: 0.3180 - accuracy: 0.9834 - val_loss: 2.0255 - val_accuracy: 0.6679\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 9s 175us/sample - loss: 0.2804 - accuracy: 0.9833 - val_loss: 2.0357 - val_accuracy: 0.6664\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 9s 179us/sample - loss: 0.2624 - accuracy: 0.9836 - val_loss: 2.0261 - val_accuracy: 0.6673\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 9s 179us/sample - loss: 0.2508 - accuracy: 0.9841 - val_loss: 2.0272 - val_accuracy: 0.6680\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 9s 179us/sample - loss: 0.2466 - accuracy: 0.9841 - val_loss: 2.0470 - val_accuracy: 0.6637\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 9s 179us/sample - loss: 0.2392 - accuracy: 0.9846 - val_loss: 2.0272 - val_accuracy: 0.6668\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 9s 180us/sample - loss: 0.2420 - accuracy: 0.9834 - val_loss: 2.0707 - val_accuracy: 0.6668\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 9s 177us/sample - loss: 0.2366 - accuracy: 0.9853 - val_loss: 2.0552 - val_accuracy: 0.6672\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 9s 181us/sample - loss: 0.2371 - accuracy: 0.9846 - val_loss: 2.0550 - val_accuracy: 0.6674\n"
     ]
    }
   ],
   "source": [
    "cls_train_features = train_cls(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "xwysIrWV4CUv",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 16384)        0           conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16384)        65536       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 16384)        0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 200)          3277000     dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 200)          800         dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 200)          0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 200)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 200)          40200       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 200)          800         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 200)          0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 200)          0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 10)           2010        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,616,106\n",
      "Trainable params: 3,579,594\n",
      "Non-trainable params: 36,512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 7s 144us/sample - loss: 6.3374 - accuracy: 0.1152 - val_loss: 5.9904 - val_accuracy: 0.1772\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 5.9537 - accuracy: 0.1403 - val_loss: 5.8420 - val_accuracy: 0.1540\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 5s 107us/sample - loss: 5.7994 - accuracy: 0.1496 - val_loss: 5.7556 - val_accuracy: 0.1209\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 5.7231 - accuracy: 0.1540 - val_loss: 5.7057 - val_accuracy: 0.1329\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 5.6862 - accuracy: 0.1601 - val_loss: 5.6813 - val_accuracy: 0.1250\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 5s 105us/sample - loss: 5.6723 - accuracy: 0.1630 - val_loss: 5.6795 - val_accuracy: 0.1235\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 5.6625 - accuracy: 0.1636 - val_loss: 5.6844 - val_accuracy: 0.1156\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 5.6637 - accuracy: 0.1642 - val_loss: 5.6703 - val_accuracy: 0.1351\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 5.6604 - accuracy: 0.1628 - val_loss: 5.6820 - val_accuracy: 0.1162\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 5.6621 - accuracy: 0.1668 - val_loss: 5.6810 - val_accuracy: 0.1254\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 5s 107us/sample - loss: 5.6658 - accuracy: 0.1662 - val_loss: 5.6739 - val_accuracy: 0.1288\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 5s 108us/sample - loss: 5.6657 - accuracy: 0.1680 - val_loss: 5.6879 - val_accuracy: 0.1219\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 5s 107us/sample - loss: 5.6707 - accuracy: 0.1671 - val_loss: 5.6950 - val_accuracy: 0.1136\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 5.6706 - accuracy: 0.1713 - val_loss: 5.6976 - val_accuracy: 0.1173\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 5.6754 - accuracy: 0.1694 - val_loss: 5.7104 - val_accuracy: 0.1125\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 5s 105us/sample - loss: 5.6751 - accuracy: 0.1721 - val_loss: 5.7005 - val_accuracy: 0.1219\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 5s 105us/sample - loss: 5.6788 - accuracy: 0.1703 - val_loss: 5.6999 - val_accuracy: 0.1341\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 5.6838 - accuracy: 0.1687 - val_loss: 5.7099 - val_accuracy: 0.1265\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 5s 107us/sample - loss: 5.6835 - accuracy: 0.1690 - val_loss: 5.7125 - val_accuracy: 0.1209\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 5s 105us/sample - loss: 5.6880 - accuracy: 0.1701 - val_loss: 5.7209 - val_accuracy: 0.1186\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 5.6907 - accuracy: 0.1679 - val_loss: 5.7307 - val_accuracy: 0.1155\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 5.6893 - accuracy: 0.1755 - val_loss: 5.7231 - val_accuracy: 0.1176\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 5.6943 - accuracy: 0.1696 - val_loss: 5.7284 - val_accuracy: 0.1140\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 5s 107us/sample - loss: 5.6942 - accuracy: 0.1705 - val_loss: 5.7219 - val_accuracy: 0.1304\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 5s 107us/sample - loss: 5.6998 - accuracy: 0.1699 - val_loss: 5.7245 - val_accuracy: 0.1246\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 5s 104us/sample - loss: 5.6994 - accuracy: 0.1718 - val_loss: 5.7220 - val_accuracy: 0.1327\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 5s 105us/sample - loss: 5.7005 - accuracy: 0.1723 - val_loss: 5.7326 - val_accuracy: 0.1253\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 5s 105us/sample - loss: 5.7027 - accuracy: 0.1714 - val_loss: 5.7311 - val_accuracy: 0.1228\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 5s 105us/sample - loss: 5.7059 - accuracy: 0.1720 - val_loss: 5.7278 - val_accuracy: 0.1371\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 5s 108us/sample - loss: 5.7057 - accuracy: 0.1734 - val_loss: 5.7379 - val_accuracy: 0.1234\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 5.6985 - accuracy: 0.1740 - val_loss: 5.7217 - val_accuracy: 0.1290\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 5s 108us/sample - loss: 5.6918 - accuracy: 0.1759 - val_loss: 5.7172 - val_accuracy: 0.1320\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 5.6847 - accuracy: 0.1790 - val_loss: 5.7101 - val_accuracy: 0.1302\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 5.6818 - accuracy: 0.1760 - val_loss: 5.7063 - val_accuracy: 0.1327\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 5s 107us/sample - loss: 5.6801 - accuracy: 0.1769 - val_loss: 5.7057 - val_accuracy: 0.1301\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 5.6765 - accuracy: 0.1769 - val_loss: 5.7036 - val_accuracy: 0.1299\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 5.6747 - accuracy: 0.1787 - val_loss: 5.7012 - val_accuracy: 0.1277\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 5.6750 - accuracy: 0.1790 - val_loss: 5.6933 - val_accuracy: 0.1320\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 5s 107us/sample - loss: 5.6709 - accuracy: 0.1800 - val_loss: 5.6913 - val_accuracy: 0.1341\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 6s 110us/sample - loss: 5.6688 - accuracy: 0.1796 - val_loss: 5.6917 - val_accuracy: 0.1334\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 5s 107us/sample - loss: 5.6668 - accuracy: 0.1821 - val_loss: 5.6875 - val_accuracy: 0.1337\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 5s 105us/sample - loss: 5.6658 - accuracy: 0.1804 - val_loss: 5.6882 - val_accuracy: 0.1321\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 5.6642 - accuracy: 0.1820 - val_loss: 5.6869 - val_accuracy: 0.1325\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 5s 105us/sample - loss: 5.6649 - accuracy: 0.1811 - val_loss: 5.6850 - val_accuracy: 0.1339\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 5.6631 - accuracy: 0.1805 - val_loss: 5.6826 - val_accuracy: 0.1343\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 5.6636 - accuracy: 0.1812 - val_loss: 5.6830 - val_accuracy: 0.1347\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 5s 105us/sample - loss: 5.6618 - accuracy: 0.1829 - val_loss: 5.6803 - val_accuracy: 0.1358\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 5s 105us/sample - loss: 5.6615 - accuracy: 0.1858 - val_loss: 5.6807 - val_accuracy: 0.1349\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 5.6617 - accuracy: 0.1820 - val_loss: 5.6812 - val_accuracy: 0.1355\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 5.6580 - accuracy: 0.1856 - val_loss: 5.6773 - val_accuracy: 0.1364\n"
     ]
    }
   ],
   "source": [
    "cls_no_train_features = train_cls(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "UdOvJsoy4CUx"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_training(name = 'Train accuracy',\n",
    "              fine_tuning_accuracy = cls_train_features.history['accuracy'],\n",
    "              feature_extracting_accuracy = cls_no_train_features.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "p3YdkFcH4CU0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_training(name = 'Validation accuracy',\n",
    "              fine_tuning_accuracy = cls_train_features.history['val_accuracy'],\n",
    "              feature_extracting_accuracy = cls_no_train_features.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Snwmob4x4CU5"
   },
   "outputs": [],
   "source": [
    "labelsPercent = [1, 2, 3, 5, 10, 25, 50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 572768,
     "status": "ok",
     "timestamp": 1605689970459,
     "user": {
      "displayName": "Alexander Dimitrienko",
      "photoUrl": "",
      "userId": "07582037467622387890"
     },
     "user_tz": -180
    },
    "id": "J5JOcjUp4CU7"
   },
   "outputs": [],
   "source": [
    "def shuffle_examples():\n",
    "    #     , ..   n \n",
    "    for i in range(len(examplesForClass)):\n",
    "        random.shuffle(examplesForClass[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 570785,
     "status": "ok",
     "timestamp": 1605689970459,
     "user": {
      "displayName": "Alexander Dimitrienko",
      "photoUrl": "",
      "userId": "07582037467622387890"
     },
     "user_tz": -180
    },
    "id": "sJIwuRIQ4CU9"
   },
   "outputs": [],
   "source": [
    "def generate_labeled_unlabeled(examples, percent, shuffle = True):\n",
    "    #           \n",
    "    if shuffle:\n",
    "        shuffle_examples()\n",
    "    labeled = []\n",
    "    tmp = []\n",
    "    unlabeled = []\n",
    "    \n",
    "    #labeled_nums = [rotations_num * i for i in range(5000 - percent * 50, 5000)]\n",
    "    labeled_nums = [i for i in range((5000 - percent * 50) * rotations_num, 5000 * rotations_num)]\n",
    "    unlabeled_nums = [i for i in range((5000 - percent * 50) * rotations_num)]\n",
    "\n",
    "    for i in range(len(examples)):\n",
    "        for j in labeled_nums:\n",
    "            labeled += [examples[i][j]] #50 == 5000 / 100\n",
    "        for j in unlabeled_nums:\n",
    "            unlabeled += [examples[i][j]]\n",
    "        \n",
    "    return labeled, unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_augmented_labeled_unlabeled(examples, percent, shuffle = True):\n",
    "    #           \n",
    "    if shuffle:\n",
    "        shuffle_examples()\n",
    "    labeled = []\n",
    "    tmp = []\n",
    "    unlabeled = []\n",
    "    \n",
    "    #labeled_nums = [rotations_num * i for i in range(5000 - percent * 50, 5000)]\n",
    "    labeled_nums = [i for i in range((5000 - percent * 50) * augment_num, 5000 * augment_num)]\n",
    "    unlabeled_nums = [i for i in range((5000 - percent * 50) * augment_num)]\n",
    "\n",
    "    for i in range(len(examples)):\n",
    "        for j in labeled_nums:\n",
    "            labeled += [examples[i][j]] #50 == 5000 / 100\n",
    "        for j in unlabeled_nums:\n",
    "            unlabeled += [examples[i][j]]\n",
    "        \n",
    "    return labeled, unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "zVKZmOo64CVE"
   },
   "outputs": [],
   "source": [
    "def self_supervised_train(unlabeled, percent, use_datagen = False, **kwargs):\n",
    "    if percent >= 100:\n",
    "        return None\n",
    "    \n",
    "    model = get_conv_model()\n",
    "    model.summary()\n",
    "    model.compile(optimizer = optimizers.Adam(), loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    if use_datagen:\n",
    "        log = model.fit(kwargs['datagen'].flow(np.asarray([combined_data[i] for i in unlabeled]), \n",
    "                    np.asarray([combined_labels[i] for i in unlabeled]),\n",
    "                    batch_size = selfsupervised_batch_size),\n",
    "                    epochs = selfsupervised_epochs, \n",
    "                    steps_per_epoch = len(unlabeled) // selfsupervised_batch_size,\n",
    "                    shuffle = True,\n",
    "                    #callbacks = [LearningRateScheduler(lr_schedule_conv)],\n",
    "                            validation_data = (np.asarray(combined_data[50000 * rotations_num:]), \n",
    "                                               np.asarray(combined_labels[50000 * rotations_num:])))\n",
    "    else:    \n",
    "        log = model.fit(np.asarray([combined_data[i] for i in unlabeled]) / 256., \n",
    "                    np.asarray([combined_labels[i] for i in unlabeled]), \n",
    "                    epochs = selfsupervised_epochs, \n",
    "                    batch_size = selfsupervised_batch_size, \n",
    "                    shuffle = True,\n",
    "                    #callbacks = [LearningRateScheduler(lr_schedule_conv)],\n",
    "                            validation_data = (np.asarray([combined_data[(50000 + i) * rotations_num] for i in range(10000)])  / 256., \n",
    "                                               np.asarray([combined_labels[(50000 + i) * rotations_num] for i in range(10000)])))\n",
    "    model.save(saved_name + \"_\" + str(percent))\n",
    "    \n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "KKgxJlIT4CVG"
   },
   "outputs": [],
   "source": [
    "def fine_tune(labeled, percent, train_features = True, use_features = True, use_datagen = False, **kwargs):\n",
    "    #cls_model = get_cls_model(saved_name + \"_\" + str(percent))\n",
    "    cls_model = get_cls_model(saved_name, use_features, percent)\n",
    "    \n",
    "    if not train_features:\n",
    "        flag = False\n",
    "        for l in cls_model.layers:\n",
    "            l.trainable = flag\n",
    "            if l.name == feature_layer or l.name == feature_layer_trained:\n",
    "                flag = True\n",
    "                \n",
    "    cls_model.summary()\n",
    "            \n",
    "    cls_model.compile(optimizer = optimizers.Adam(lr=0.001), loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    if use_datagen:\n",
    "        print('Datagen is used...')\n",
    "        cls_log = cls_model.fit(kwargs['datagen'].flow(np.asarray([combined_data[i] for i in labeled]), \n",
    "                            np.asarray([true_labels[i] for i in labeled]),\n",
    "                            batch_size = supervised_batch_size), \n",
    "                            epochs = supervised_epochs,\n",
    "                            steps_per_epoch = len(labeled) // supervised_batch_size, \n",
    "                            shuffle = True,\n",
    "                            validation_data = (np.asarray([combined_data[(50000 + i) * rotations_num] for i in range(10000)]) / 256., \n",
    "                                               np.asarray([true_labels[(50000 + i) * rotations_num] for i in range(10000)])))\n",
    "    else:\n",
    "        cls_log = cls_model.fit(np.asarray([augmented_data[i] for i in labeled]) / 256., \n",
    "                            np.asarray([augmented_labels[i] for i in labeled]), \n",
    "                            epochs = supervised_epochs, batch_size = supervised_batch_size, \n",
    "                            shuffle = True,\n",
    "                            validation_data = (np.asarray([combined_data[(50000 + i) * rotations_num] for i in range(10000)])  / 256., \n",
    "                                               np.asarray([true_labels[(50000 + i) * rotations_num] for i in range(10000)])))\n",
    "    \n",
    "    return cls_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "5S_EOaHJ4CVJ"
   },
   "outputs": [],
   "source": [
    "def get_empty_model():\n",
    "    #      \n",
    "    model = get_conv_model()\n",
    "    model.compile(optimizer = optimizers.RMSprop(), loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.save(saved_name + \"_100\")\n",
    "    \n",
    "    return get_cls_model(saved_name + \"_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_preprocess(data):\n",
    "    output = data / 256.0\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=4,\n",
    "    height_shift_range=4,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=my_preprocess)\n",
    "aug_datagen.fit(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ResizeMethod', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_sys', 'adjust_brightness', 'adjust_contrast', 'adjust_gamma', 'adjust_hue', 'adjust_jpeg_quality', 'adjust_saturation', 'central_crop', 'combined_non_max_suppression', 'convert_image_dtype', 'crop_and_resize', 'crop_to_bounding_box', 'decode_and_crop_jpeg', 'decode_bmp', 'decode_gif', 'decode_image', 'decode_jpeg', 'decode_png', 'draw_bounding_boxes', 'encode_jpeg', 'encode_png', 'extract_glimpse', 'extract_jpeg_shape', 'extract_patches', 'flip_left_right', 'flip_up_down', 'generate_bounding_box_proposals', 'grayscale_to_rgb', 'hsv_to_rgb', 'image_gradients', 'is_jpeg', 'non_max_suppression', 'non_max_suppression_overlaps', 'non_max_suppression_padded', 'non_max_suppression_with_scores', 'pad_to_bounding_box', 'per_image_standardization', 'psnr', 'random_brightness', 'random_contrast', 'random_crop', 'random_flip_left_right', 'random_flip_up_down', 'random_hue', 'random_jpeg_quality', 'random_saturation', 'resize', 'resize_with_crop_or_pad', 'resize_with_pad', 'rgb_to_grayscale', 'rgb_to_hsv', 'rgb_to_yiq', 'rgb_to_yuv', 'rot90', 'sample_distorted_bounding_box', 'sobel_edges', 'ssim', 'ssim_multiscale', 'total_variation', 'transpose', 'yiq_to_rgb', 'yuv_to_rgb']\n"
     ]
    }
   ],
   "source": [
    "print(dir(tf.image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data = np.zeros((50000 * augment_num, 32, 32, 3), dtype = np.float32)\n",
    "augmented_labels = np.zeros((50000 * augment_num, 10), dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n"
     ]
    }
   ],
   "source": [
    "for i in range(50000):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    augmented_data[i * augment_num] = combined_data[i * rotations_num]\n",
    "    augmented_data[i * augment_num + 1] = tf.image.flip_up_down(tf.image.random_contrast(combined_data[i * rotations_num], 0.2, 0.7))\n",
    "    augmented_data[i * augment_num + 2] = tf.image.flip_left_right(tf.image.random_contrast(combined_data[i * rotations_num], 0.2, 0.7))\n",
    "    augmented_data[i * augment_num + 3] = tf.image.random_contrast(combined_data[i * rotations_num], 0.2, 0.7)\n",
    "    augmented_data[i * augment_num + 4] = tf.image.flip_left_right(combined_data[i * rotations_num])\n",
    "    augmented_data[i * augment_num + 5] = tf.image.flip_up_down(combined_data[i * rotations_num])\n",
    "    augmented_data[i * augment_num + 6] = tf.image.random_hue(combined_data[i * rotations_num], 0.3)\n",
    "    augmented_data[i * augment_num + 7] = tf.image.random_saturation(combined_data[i * rotations_num], 5, 10)\n",
    "    augmented_data[i * augment_num + 8] = tf.image.flip_left_right(tf.image.random_hue(combined_data[i * rotations_num], 0.3))\n",
    "    augmented_data[i * augment_num + 9] = tf.image.flip_left_right(tf.image.random_saturation(combined_data[i * rotations_num], 5, 10))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50000):\n",
    "    for j in range(augment_num):\n",
    "            augmented_labels[i * augment_num + j] = true_labels[i * rotations_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "r = random.randint(0, 50000)\n",
    "for i in range(augment_num):\n",
    "    plt.imshow(augmented_data[r * augment_num + i].astype(int))\n",
    "    plt.show()\n",
    "    print(augmented_labels[r * augment_num + i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentedExamplesForClass = []\n",
    "for i in range(10):\n",
    "    augmentedExamplesForClass.append([j for j in range(50000 * augment_num) if np.where(augmented_labels[j] == 1)[0][0] == i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "wLjLXjvQ4CVM"
   },
   "outputs": [],
   "source": [
    "examplesForClass = []\n",
    "for i in range(10):\n",
    "    examplesForClass.append([j for j in range(50000 * rotations_num) if np.where(true_labels[j] == 1)[0][0] == i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 16384)        0           conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16384)        65536       flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 16384)        0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 200)          3277000     dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 200)          800         dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 200)          0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 200)          0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 200)          40200       dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 200)          800         dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 200)          0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 200)          0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 4)            804         dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,614,900\n",
      "Trainable params: 3,578,388\n",
      "Non-trainable params: 36,512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/80\n",
      "200000/200000 [==============================] - 31s 157us/sample - loss: 2.4446 - accuracy: 0.5756 - val_loss: 1.8339 - val_accuracy: 0.5717\n",
      "Epoch 2/80\n",
      "200000/200000 [==============================] - 29s 145us/sample - loss: 1.7002 - accuracy: 0.6559 - val_loss: 1.3682 - val_accuracy: 0.8283\n",
      "Epoch 3/80\n",
      "200000/200000 [==============================] - 30s 150us/sample - loss: 1.6393 - accuracy: 0.6969 - val_loss: 1.6253 - val_accuracy: 0.6940\n",
      "Epoch 4/80\n",
      "200000/200000 [==============================] - 29s 147us/sample - loss: 1.5753 - accuracy: 0.7271 - val_loss: 1.4542 - val_accuracy: 0.7775\n",
      "Epoch 5/80\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 1.5071 - accuracy: 0.7506 - val_loss: 1.8867 - val_accuracy: 0.6051\n",
      "Epoch 6/80\n",
      "200000/200000 [==============================] - 29s 143us/sample - loss: 1.4498 - accuracy: 0.7673 - val_loss: 1.6653 - val_accuracy: 0.6804\n",
      "Epoch 7/80\n",
      "200000/200000 [==============================] - 29s 143us/sample - loss: 1.3940 - accuracy: 0.7811 - val_loss: 1.6512 - val_accuracy: 0.6951\n",
      "Epoch 8/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 1.3440 - accuracy: 0.7941 - val_loss: 1.2619 - val_accuracy: 0.8015\n",
      "Epoch 9/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 1.3013 - accuracy: 0.8036 - val_loss: 1.0809 - val_accuracy: 0.8810\n",
      "Epoch 10/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 1.2555 - accuracy: 0.8146 - val_loss: 1.0739 - val_accuracy: 0.8701\n",
      "Epoch 11/80\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 1.2146 - accuracy: 0.8218 - val_loss: 1.9931 - val_accuracy: 0.5747\n",
      "Epoch 12/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 1.1805 - accuracy: 0.8290 - val_loss: 2.7453 - val_accuracy: 0.3731\n",
      "Epoch 13/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 1.1375 - accuracy: 0.8368 - val_loss: 1.1282 - val_accuracy: 0.8159\n",
      "Epoch 14/80\n",
      "200000/200000 [==============================] - 29s 144us/sample - loss: 1.1079 - accuracy: 0.8425 - val_loss: 1.3101 - val_accuracy: 0.7513\n",
      "Epoch 15/80\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 1.0742 - accuracy: 0.8501 - val_loss: 1.4263 - val_accuracy: 0.7007\n",
      "Epoch 16/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 1.0350 - accuracy: 0.8543 - val_loss: 1.5710 - val_accuracy: 0.6539\n",
      "Epoch 17/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 1.0079 - accuracy: 0.8597 - val_loss: 1.1810 - val_accuracy: 0.7892\n",
      "Epoch 18/80\n",
      "200000/200000 [==============================] - 30s 149us/sample - loss: 0.9847 - accuracy: 0.8639 - val_loss: 1.0432 - val_accuracy: 0.8308\n",
      "Epoch 19/80\n",
      "200000/200000 [==============================] - 29s 143us/sample - loss: 0.9610 - accuracy: 0.8684 - val_loss: 1.1648 - val_accuracy: 0.8053\n",
      "Epoch 20/80\n",
      "200000/200000 [==============================] - 29s 143us/sample - loss: 0.9338 - accuracy: 0.8735 - val_loss: 1.1986 - val_accuracy: 0.7903\n",
      "Epoch 21/80\n",
      "200000/200000 [==============================] - 29s 145us/sample - loss: 0.9129 - accuracy: 0.8772 - val_loss: 1.3851 - val_accuracy: 0.7212\n",
      "Epoch 22/80\n",
      "200000/200000 [==============================] - 29s 146us/sample - loss: 0.8887 - accuracy: 0.8820 - val_loss: 0.9098 - val_accuracy: 0.8690\n",
      "Epoch 23/80\n",
      "200000/200000 [==============================] - 29s 147us/sample - loss: 0.8726 - accuracy: 0.8849 - val_loss: 1.2519 - val_accuracy: 0.7511\n",
      "Epoch 24/80\n",
      "200000/200000 [==============================] - 29s 143us/sample - loss: 0.8556 - accuracy: 0.8893 - val_loss: 1.0806 - val_accuracy: 0.8154\n",
      "Epoch 25/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.8356 - accuracy: 0.8910 - val_loss: 1.1190 - val_accuracy: 0.7888\n",
      "Epoch 26/80\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 0.8204 - accuracy: 0.8950 - val_loss: 1.1318 - val_accuracy: 0.8075\n",
      "Epoch 27/80\n",
      "200000/200000 [==============================] - 29s 147us/sample - loss: 0.8060 - accuracy: 0.8975 - val_loss: 0.8809 - val_accuracy: 0.8594\n",
      "Epoch 28/80\n",
      "200000/200000 [==============================] - 28s 139us/sample - loss: 0.7912 - accuracy: 0.9002 - val_loss: 1.4301 - val_accuracy: 0.6909\n",
      "Epoch 29/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.7774 - accuracy: 0.9036 - val_loss: 0.9423 - val_accuracy: 0.8554\n",
      "Epoch 30/80\n",
      "200000/200000 [==============================] - 29s 144us/sample - loss: 0.7635 - accuracy: 0.9059 - val_loss: 0.8797 - val_accuracy: 0.8684\n",
      "Epoch 31/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.7551 - accuracy: 0.9087 - val_loss: 1.3661 - val_accuracy: 0.7377\n",
      "Epoch 32/80\n",
      "200000/200000 [==============================] - 29s 146us/sample - loss: 0.7441 - accuracy: 0.9105 - val_loss: 0.9751 - val_accuracy: 0.8415\n",
      "Epoch 33/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.7325 - accuracy: 0.9120 - val_loss: 1.2968 - val_accuracy: 0.7467\n",
      "Epoch 34/80\n",
      "200000/200000 [==============================] - 28s 139us/sample - loss: 0.7263 - accuracy: 0.9137 - val_loss: 1.4922 - val_accuracy: 0.6942\n",
      "Epoch 35/80\n",
      "200000/200000 [==============================] - 28s 138us/sample - loss: 0.7109 - accuracy: 0.9173 - val_loss: 1.0476 - val_accuracy: 0.8209\n",
      "Epoch 36/80\n",
      "200000/200000 [==============================] - 28s 138us/sample - loss: 0.7062 - accuracy: 0.9184 - val_loss: 1.1351 - val_accuracy: 0.8007\n",
      "Epoch 37/80\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 0.6948 - accuracy: 0.9212 - val_loss: 1.0103 - val_accuracy: 0.8373\n",
      "Epoch 38/80\n",
      "200000/200000 [==============================] - 27s 136us/sample - loss: 0.6898 - accuracy: 0.9214 - val_loss: 0.9832 - val_accuracy: 0.8364\n",
      "Epoch 39/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.6826 - accuracy: 0.9236 - val_loss: 1.1315 - val_accuracy: 0.7788\n",
      "Epoch 40/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.6756 - accuracy: 0.9251 - val_loss: 1.2793 - val_accuracy: 0.7523\n",
      "Epoch 41/80\n",
      "200000/200000 [==============================] - 29s 144us/sample - loss: 0.6628 - accuracy: 0.9283 - val_loss: 0.9001 - val_accuracy: 0.8621\n",
      "Epoch 42/80\n",
      "200000/200000 [==============================] - 29s 147us/sample - loss: 0.6604 - accuracy: 0.9283 - val_loss: 1.3069 - val_accuracy: 0.7655\n",
      "Epoch 43/80\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 0.6540 - accuracy: 0.9300 - val_loss: 1.1287 - val_accuracy: 0.7833\n",
      "Epoch 44/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.6472 - accuracy: 0.9306 - val_loss: 1.3592 - val_accuracy: 0.7330\n",
      "Epoch 45/80\n",
      "200000/200000 [==============================] - 26s 131us/sample - loss: 0.6420 - accuracy: 0.9322 - val_loss: 1.3023 - val_accuracy: 0.7531\n",
      "Epoch 46/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.6356 - accuracy: 0.9325 - val_loss: 1.5503 - val_accuracy: 0.6907\n",
      "Epoch 47/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.6325 - accuracy: 0.9346 - val_loss: 1.4733 - val_accuracy: 0.7160\n",
      "Epoch 48/80\n",
      "200000/200000 [==============================] - 29s 144us/sample - loss: 0.6265 - accuracy: 0.9356 - val_loss: 0.9215 - val_accuracy: 0.8522\n",
      "Epoch 49/80\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 0.6229 - accuracy: 0.9363 - val_loss: 1.3162 - val_accuracy: 0.7596\n",
      "Epoch 50/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.6181 - accuracy: 0.9383 - val_loss: 1.1208 - val_accuracy: 0.8207\n",
      "Epoch 51/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.6138 - accuracy: 0.9388 - val_loss: 1.2443 - val_accuracy: 0.7670\n",
      "Epoch 52/80\n",
      "200000/200000 [==============================] - 29s 143us/sample - loss: 0.6080 - accuracy: 0.9405 - val_loss: 0.9981 - val_accuracy: 0.8374\n",
      "Epoch 53/80\n",
      "200000/200000 [==============================] - 29s 143us/sample - loss: 0.6034 - accuracy: 0.9407 - val_loss: 1.3465 - val_accuracy: 0.7534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/80\n",
      "200000/200000 [==============================] - 29s 143us/sample - loss: 0.5996 - accuracy: 0.9422 - val_loss: 1.0986 - val_accuracy: 0.8178\n",
      "Epoch 55/80\n",
      "200000/200000 [==============================] - 29s 145us/sample - loss: 0.5900 - accuracy: 0.9432 - val_loss: 1.3480 - val_accuracy: 0.7525\n",
      "Epoch 56/80\n",
      "200000/200000 [==============================] - 30s 148us/sample - loss: 0.5881 - accuracy: 0.9442 - val_loss: 1.3888 - val_accuracy: 0.7440\n",
      "Epoch 57/80\n",
      "200000/200000 [==============================] - 29s 146us/sample - loss: 0.5850 - accuracy: 0.9448 - val_loss: 1.1901 - val_accuracy: 0.7957\n",
      "Epoch 58/80\n",
      "200000/200000 [==============================] - 29s 144us/sample - loss: 0.5805 - accuracy: 0.9453 - val_loss: 1.4212 - val_accuracy: 0.7393\n",
      "Epoch 59/80\n",
      "200000/200000 [==============================] - 29s 145us/sample - loss: 0.5771 - accuracy: 0.9465 - val_loss: 1.4947 - val_accuracy: 0.7267\n",
      "Epoch 60/80\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 0.5750 - accuracy: 0.9471 - val_loss: 1.1133 - val_accuracy: 0.8088\n",
      "Epoch 61/80\n",
      "200000/200000 [==============================] - 27s 137us/sample - loss: 0.5693 - accuracy: 0.9467 - val_loss: 1.5869 - val_accuracy: 0.7240\n",
      "Epoch 62/80\n",
      "200000/200000 [==============================] - 27s 136us/sample - loss: 0.5634 - accuracy: 0.9494 - val_loss: 1.2874 - val_accuracy: 0.7707\n",
      "Epoch 63/80\n",
      "200000/200000 [==============================] - 27s 137us/sample - loss: 0.5648 - accuracy: 0.9486 - val_loss: 1.2759 - val_accuracy: 0.7698\n",
      "Epoch 64/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.5588 - accuracy: 0.9491 - val_loss: 1.0739 - val_accuracy: 0.8200\n",
      "Epoch 65/80\n",
      "200000/200000 [==============================] - 28s 138us/sample - loss: 0.5555 - accuracy: 0.9499 - val_loss: 0.9878 - val_accuracy: 0.8397\n",
      "Epoch 66/80\n",
      "200000/200000 [==============================] - 27s 136us/sample - loss: 0.5505 - accuracy: 0.9516 - val_loss: 1.6429 - val_accuracy: 0.7034\n",
      "Epoch 67/80\n",
      "200000/200000 [==============================] - 29s 143us/sample - loss: 0.5502 - accuracy: 0.9510 - val_loss: 1.0792 - val_accuracy: 0.8273\n",
      "Epoch 68/80\n",
      "200000/200000 [==============================] - 27s 134us/sample - loss: 0.5479 - accuracy: 0.9522 - val_loss: 1.1737 - val_accuracy: 0.8001\n",
      "Epoch 69/80\n",
      "200000/200000 [==============================] - 28s 139us/sample - loss: 0.5442 - accuracy: 0.9528 - val_loss: 1.3806 - val_accuracy: 0.7744\n",
      "Epoch 70/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.5411 - accuracy: 0.9537 - val_loss: 1.2446 - val_accuracy: 0.7898\n",
      "Epoch 71/80\n",
      "200000/200000 [==============================] - 27s 136us/sample - loss: 0.5444 - accuracy: 0.9534 - val_loss: 1.2246 - val_accuracy: 0.8102\n",
      "Epoch 72/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.5412 - accuracy: 0.9541 - val_loss: 1.3058 - val_accuracy: 0.7566\n",
      "Epoch 73/80\n",
      "200000/200000 [==============================] - 27s 137us/sample - loss: 0.5392 - accuracy: 0.9542 - val_loss: 1.4818 - val_accuracy: 0.7347\n",
      "Epoch 74/80\n",
      "200000/200000 [==============================] - 28s 138us/sample - loss: 0.5365 - accuracy: 0.9550 - val_loss: 1.4286 - val_accuracy: 0.7539\n",
      "Epoch 75/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.5278 - accuracy: 0.9564 - val_loss: 1.3101 - val_accuracy: 0.7817\n",
      "Epoch 76/80\n",
      "200000/200000 [==============================] - 28s 138us/sample - loss: 0.5321 - accuracy: 0.9563 - val_loss: 1.2229 - val_accuracy: 0.7960\n",
      "Epoch 77/80\n",
      "200000/200000 [==============================] - 27s 137us/sample - loss: 0.5292 - accuracy: 0.9567 - val_loss: 1.0243 - val_accuracy: 0.8523\n",
      "Epoch 78/80\n",
      "200000/200000 [==============================] - 27s 136us/sample - loss: 0.5253 - accuracy: 0.9575 - val_loss: 1.3208 - val_accuracy: 0.7870\n",
      "Epoch 79/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.5215 - accuracy: 0.9582 - val_loss: 1.4630 - val_accuracy: 0.7558\n",
      "Epoch 80/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.5223 - accuracy: 0.9582 - val_loss: 1.3235 - val_accuracy: 0.7855\n",
      "INFO:tensorflow:Assets written to: selfsupervised_0/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdb2c500f98>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled, unlabeled = generate_labeled_unlabeled(examplesForClass, 0, False)\n",
    "self_supervised_train(unlabeled, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = []\n",
    "cls_logs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 989
    },
    "executionInfo": {
     "elapsed": 29958,
     "status": "error",
     "timestamp": 1605100230880,
     "user": {
      "displayName": "Alexander Dimitrienko",
      "photoUrl": "",
      "userId": "07582037467622387890"
     },
     "user_tz": -180
    },
    "id": "TRtVD-9I4CVP",
    "outputId": "3a012661-c3f8-48ff-a1d3-db504c9aef8b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 16384)        0           conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16384)        65536       flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 16384)        0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 200)          3277000     dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 200)          800         dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 200)          0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 200)          0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 200)          40200       dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 200)          800         dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 200)          0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 200)          0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 4)            804         dropout_14[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,614,900\n",
      "Trainable params: 3,578,388\n",
      "Non-trainable params: 36,512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 198000 samples, validate on 10000 samples\n",
      "Epoch 1/80\n",
      "198000/198000 [==============================] - 30s 151us/sample - loss: 2.4493 - accuracy: 0.5758 - val_loss: 1.9762 - val_accuracy: 0.5287\n",
      "Epoch 2/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 1.7048 - accuracy: 0.6606 - val_loss: 2.2252 - val_accuracy: 0.3625\n",
      "Epoch 3/80\n",
      "198000/198000 [==============================] - 28s 139us/sample - loss: 1.6404 - accuracy: 0.7024 - val_loss: 1.4969 - val_accuracy: 0.7956\n",
      "Epoch 4/80\n",
      "198000/198000 [==============================] - 28s 141us/sample - loss: 1.5709 - accuracy: 0.7340 - val_loss: 1.6849 - val_accuracy: 0.7421\n",
      "Epoch 5/80\n",
      "198000/198000 [==============================] - 28s 139us/sample - loss: 1.5068 - accuracy: 0.7549 - val_loss: 2.0196 - val_accuracy: 0.5283\n",
      "Epoch 6/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 1.4436 - accuracy: 0.7727 - val_loss: 1.8819 - val_accuracy: 0.6272\n",
      "Epoch 7/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 1.3987 - accuracy: 0.7858 - val_loss: 1.4290 - val_accuracy: 0.7762\n",
      "Epoch 8/80\n",
      "198000/198000 [==============================] - 28s 141us/sample - loss: 1.3486 - accuracy: 0.7976 - val_loss: 1.4078 - val_accuracy: 0.7577\n",
      "Epoch 9/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 1.2932 - accuracy: 0.8087 - val_loss: 1.4062 - val_accuracy: 0.7564\n",
      "Epoch 10/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 1.2577 - accuracy: 0.8185 - val_loss: 1.2283 - val_accuracy: 0.8307\n",
      "Epoch 11/80\n",
      "198000/198000 [==============================] - 28s 141us/sample - loss: 1.2181 - accuracy: 0.8251 - val_loss: 1.2964 - val_accuracy: 0.7807\n",
      "Epoch 12/80\n",
      "198000/198000 [==============================] - 28s 141us/sample - loss: 1.1794 - accuracy: 0.8337 - val_loss: 1.3252 - val_accuracy: 0.7702\n",
      "Epoch 13/80\n",
      "198000/198000 [==============================] - 27s 139us/sample - loss: 1.1389 - accuracy: 0.8388 - val_loss: 1.3013 - val_accuracy: 0.7594\n",
      "Epoch 14/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 1.1081 - accuracy: 0.8458 - val_loss: 1.2653 - val_accuracy: 0.7747\n",
      "Epoch 15/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 1.0758 - accuracy: 0.8517 - val_loss: 1.3071 - val_accuracy: 0.7462\n",
      "Epoch 16/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 1.0475 - accuracy: 0.8569 - val_loss: 1.2538 - val_accuracy: 0.7694\n",
      "Epoch 17/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 1.0119 - accuracy: 0.8617 - val_loss: 1.0419 - val_accuracy: 0.8317\n",
      "Epoch 18/80\n",
      "198000/198000 [==============================] - 28s 139us/sample - loss: 0.9863 - accuracy: 0.8674 - val_loss: 1.8827 - val_accuracy: 0.5978\n",
      "Epoch 19/80\n",
      "198000/198000 [==============================] - 28s 141us/sample - loss: 0.9592 - accuracy: 0.8712 - val_loss: 1.4053 - val_accuracy: 0.7140\n",
      "Epoch 20/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 0.9349 - accuracy: 0.8757 - val_loss: 0.9661 - val_accuracy: 0.8592\n",
      "Epoch 21/80\n",
      "198000/198000 [==============================] - 28s 141us/sample - loss: 0.9165 - accuracy: 0.8790 - val_loss: 1.8571 - val_accuracy: 0.5894\n",
      "Epoch 22/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 0.8933 - accuracy: 0.8828 - val_loss: 1.0806 - val_accuracy: 0.8144\n",
      "Epoch 23/80\n",
      "198000/198000 [==============================] - 28s 141us/sample - loss: 0.8751 - accuracy: 0.8854 - val_loss: 1.0780 - val_accuracy: 0.8186\n",
      "Epoch 24/80\n",
      "198000/198000 [==============================] - 28s 142us/sample - loss: 0.8593 - accuracy: 0.8894 - val_loss: 0.9383 - val_accuracy: 0.8536\n",
      "Epoch 25/80\n",
      "198000/198000 [==============================] - 28s 139us/sample - loss: 0.8398 - accuracy: 0.8934 - val_loss: 1.1262 - val_accuracy: 0.7966\n",
      "Epoch 26/80\n",
      "198000/198000 [==============================] - 28s 141us/sample - loss: 0.8233 - accuracy: 0.8962 - val_loss: 1.0882 - val_accuracy: 0.8169\n",
      "Epoch 27/80\n",
      "198000/198000 [==============================] - 28s 139us/sample - loss: 0.8044 - accuracy: 0.8995 - val_loss: 1.1912 - val_accuracy: 0.7772\n",
      "Epoch 28/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 0.7922 - accuracy: 0.9015 - val_loss: 0.9704 - val_accuracy: 0.8476\n",
      "Epoch 29/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 0.7835 - accuracy: 0.9036 - val_loss: 1.1495 - val_accuracy: 0.7907\n",
      "Epoch 30/80\n",
      "198000/198000 [==============================] - 28s 142us/sample - loss: 0.7694 - accuracy: 0.9062 - val_loss: 1.0998 - val_accuracy: 0.8018\n",
      "Epoch 31/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 0.7570 - accuracy: 0.9085 - val_loss: 1.0744 - val_accuracy: 0.8093\n",
      "Epoch 32/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 0.7474 - accuracy: 0.9124 - val_loss: 1.0969 - val_accuracy: 0.7998\n",
      "Epoch 33/80\n",
      "198000/198000 [==============================] - 28s 142us/sample - loss: 0.7388 - accuracy: 0.9129 - val_loss: 1.2913 - val_accuracy: 0.7508\n",
      "Epoch 34/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 0.7305 - accuracy: 0.9157 - val_loss: 0.8708 - val_accuracy: 0.8674\n",
      "Epoch 35/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 0.7191 - accuracy: 0.9172 - val_loss: 0.9934 - val_accuracy: 0.8346\n",
      "Epoch 36/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 0.7072 - accuracy: 0.9192 - val_loss: 1.1958 - val_accuracy: 0.7893\n",
      "Epoch 37/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 0.7016 - accuracy: 0.9205 - val_loss: 1.0311 - val_accuracy: 0.8161\n",
      "Epoch 38/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 0.6917 - accuracy: 0.9232 - val_loss: 1.0267 - val_accuracy: 0.8163\n",
      "Epoch 39/80\n",
      "198000/198000 [==============================] - 28s 143us/sample - loss: 0.6856 - accuracy: 0.9242 - val_loss: 1.1515 - val_accuracy: 0.7945\n",
      "Epoch 40/80\n",
      "198000/198000 [==============================] - 28s 142us/sample - loss: 0.6769 - accuracy: 0.9268 - val_loss: 1.4219 - val_accuracy: 0.7153\n",
      "Epoch 41/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 0.6720 - accuracy: 0.9274 - val_loss: 1.1850 - val_accuracy: 0.7754\n",
      "Epoch 42/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 0.6663 - accuracy: 0.9287 - val_loss: 1.0803 - val_accuracy: 0.8015\n",
      "Epoch 43/80\n",
      "198000/198000 [==============================] - 28s 142us/sample - loss: 0.6554 - accuracy: 0.9305 - val_loss: 1.4505 - val_accuracy: 0.7023\n",
      "Epoch 44/80\n",
      "198000/198000 [==============================] - 28s 139us/sample - loss: 0.6528 - accuracy: 0.9309 - val_loss: 1.2030 - val_accuracy: 0.7840\n",
      "Epoch 45/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 0.6464 - accuracy: 0.9331 - val_loss: 1.2036 - val_accuracy: 0.7703\n",
      "Epoch 46/80\n",
      "198000/198000 [==============================] - 28s 141us/sample - loss: 0.6384 - accuracy: 0.9342 - val_loss: 1.2320 - val_accuracy: 0.7804\n",
      "Epoch 47/80\n",
      "198000/198000 [==============================] - 28s 142us/sample - loss: 0.6323 - accuracy: 0.9360 - val_loss: 1.2009 - val_accuracy: 0.7783\n",
      "Epoch 48/80\n",
      "198000/198000 [==============================] - 28s 139us/sample - loss: 0.6316 - accuracy: 0.9358 - val_loss: 1.1003 - val_accuracy: 0.8004\n",
      "Epoch 49/80\n",
      "198000/198000 [==============================] - 28s 139us/sample - loss: 0.6249 - accuracy: 0.9371 - val_loss: 0.9961 - val_accuracy: 0.8314\n",
      "Epoch 50/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 0.6182 - accuracy: 0.9395 - val_loss: 1.2438 - val_accuracy: 0.7673\n",
      "Epoch 51/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 0.6149 - accuracy: 0.9400 - val_loss: 1.0482 - val_accuracy: 0.8203\n",
      "Epoch 52/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 0.6085 - accuracy: 0.9411 - val_loss: 1.4207 - val_accuracy: 0.7465\n",
      "Epoch 53/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 0.6068 - accuracy: 0.9413 - val_loss: 1.5581 - val_accuracy: 0.7368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/80\n",
      "198000/198000 [==============================] - 27s 139us/sample - loss: 0.5993 - accuracy: 0.9425 - val_loss: 0.9656 - val_accuracy: 0.8492\n",
      "Epoch 55/80\n",
      "198000/198000 [==============================] - 28s 141us/sample - loss: 0.5968 - accuracy: 0.9428 - val_loss: 1.3710 - val_accuracy: 0.7411\n",
      "Epoch 56/80\n",
      "198000/198000 [==============================] - 27s 139us/sample - loss: 0.5889 - accuracy: 0.9453 - val_loss: 1.4346 - val_accuracy: 0.7426\n",
      "Epoch 57/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 0.5844 - accuracy: 0.9457 - val_loss: 1.3983 - val_accuracy: 0.7443\n",
      "Epoch 58/80\n",
      "198000/198000 [==============================] - 28s 139us/sample - loss: 0.5845 - accuracy: 0.9452 - val_loss: 1.3852 - val_accuracy: 0.7581\n",
      "Epoch 59/80\n",
      "198000/198000 [==============================] - 28s 141us/sample - loss: 0.5782 - accuracy: 0.9471 - val_loss: 1.0295 - val_accuracy: 0.8479\n",
      "Epoch 60/80\n",
      "198000/198000 [==============================] - 28s 141us/sample - loss: 0.5787 - accuracy: 0.9473 - val_loss: 1.5114 - val_accuracy: 0.7296\n",
      "Epoch 61/80\n",
      "198000/198000 [==============================] - 28s 139us/sample - loss: 0.5754 - accuracy: 0.9486 - val_loss: 0.9036 - val_accuracy: 0.8706\n",
      "Epoch 62/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 0.5699 - accuracy: 0.9490 - val_loss: 1.1250 - val_accuracy: 0.8255\n",
      "Epoch 63/80\n",
      "198000/198000 [==============================] - 28s 142us/sample - loss: 0.5695 - accuracy: 0.9489 - val_loss: 1.5804 - val_accuracy: 0.7286\n",
      "Epoch 64/80\n",
      "198000/198000 [==============================] - 28s 141us/sample - loss: 0.5666 - accuracy: 0.9496 - val_loss: 1.1706 - val_accuracy: 0.7976\n",
      "Epoch 65/80\n",
      "198000/198000 [==============================] - 27s 138us/sample - loss: 0.5575 - accuracy: 0.9516 - val_loss: 1.2402 - val_accuracy: 0.7888\n",
      "Epoch 66/80\n",
      "198000/198000 [==============================] - 28s 141us/sample - loss: 0.5592 - accuracy: 0.9505 - val_loss: 1.3360 - val_accuracy: 0.7665\n",
      "Epoch 67/80\n",
      "198000/198000 [==============================] - 28s 143us/sample - loss: 0.5524 - accuracy: 0.9520 - val_loss: 1.1123 - val_accuracy: 0.8132\n",
      "Epoch 68/80\n",
      "198000/198000 [==============================] - 27s 139us/sample - loss: 0.5557 - accuracy: 0.9525 - val_loss: 1.3586 - val_accuracy: 0.7664\n",
      "Epoch 69/80\n",
      "198000/198000 [==============================] - 28s 141us/sample - loss: 0.5498 - accuracy: 0.9532 - val_loss: 1.1150 - val_accuracy: 0.8203\n",
      "Epoch 70/80\n",
      "198000/198000 [==============================] - 27s 137us/sample - loss: 0.5443 - accuracy: 0.9543 - val_loss: 1.0129 - val_accuracy: 0.8316\n",
      "Epoch 71/80\n",
      "198000/198000 [==============================] - 28s 141us/sample - loss: 0.5439 - accuracy: 0.9540 - val_loss: 1.3012 - val_accuracy: 0.7960\n",
      "Epoch 72/80\n",
      "198000/198000 [==============================] - 28s 141us/sample - loss: 0.5400 - accuracy: 0.9551 - val_loss: 1.4752 - val_accuracy: 0.7576\n",
      "Epoch 73/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 0.5368 - accuracy: 0.9557 - val_loss: 1.3385 - val_accuracy: 0.7778\n",
      "Epoch 74/80\n",
      "198000/198000 [==============================] - 28s 141us/sample - loss: 0.5357 - accuracy: 0.9556 - val_loss: 1.2480 - val_accuracy: 0.8070\n",
      "Epoch 75/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 0.5296 - accuracy: 0.9568 - val_loss: 1.5285 - val_accuracy: 0.7434\n",
      "Epoch 76/80\n",
      "198000/198000 [==============================] - 28s 143us/sample - loss: 0.5354 - accuracy: 0.9558 - val_loss: 1.0351 - val_accuracy: 0.8429\n",
      "Epoch 77/80\n",
      "198000/198000 [==============================] - 28s 141us/sample - loss: 0.5241 - accuracy: 0.9575 - val_loss: 2.0599 - val_accuracy: 0.6396\n",
      "Epoch 78/80\n",
      "198000/198000 [==============================] - 28s 141us/sample - loss: 0.5220 - accuracy: 0.9584 - val_loss: 1.4525 - val_accuracy: 0.7618\n",
      "Epoch 79/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 0.5193 - accuracy: 0.9583 - val_loss: 0.9782 - val_accuracy: 0.8516\n",
      "Epoch 80/80\n",
      "198000/198000 [==============================] - 28s 140us/sample - loss: 0.5178 - accuracy: 0.9591 - val_loss: 1.1583 - val_accuracy: 0.8215\n",
      "INFO:tensorflow:Assets written to: selfsupervised_1/assets\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 16384)        0           conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16384)        65536       flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 16384)        0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 200)          3277000     dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 200)          800         dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 200)          0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 200)          0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 200)          40200       dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 200)          800         dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 200)          0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 200)          0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 10)           2010        dropout_17[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,473,002\n",
      "Trainable params: 3,438,026\n",
      "Non-trainable params: 34,976\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 2s 1ms/sample - loss: 6.1983 - accuracy: 0.2470 - val_loss: 8.0786 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 1s 366us/sample - loss: 5.4656 - accuracy: 0.4970 - val_loss: 7.5415 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 1s 359us/sample - loss: 5.0422 - accuracy: 0.6380 - val_loss: 7.0538 - val_accuracy: 0.1244\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 1s 368us/sample - loss: 4.6034 - accuracy: 0.7655 - val_loss: 6.6608 - val_accuracy: 0.1000\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 1s 365us/sample - loss: 4.2143 - accuracy: 0.8480 - val_loss: 6.4291 - val_accuracy: 0.1000\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 1s 368us/sample - loss: 3.8799 - accuracy: 0.8940 - val_loss: 6.7509 - val_accuracy: 0.1000\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 1s 366us/sample - loss: 3.5870 - accuracy: 0.9245 - val_loss: 6.5266 - val_accuracy: 0.1000\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 1s 367us/sample - loss: 3.3246 - accuracy: 0.9445 - val_loss: 6.8108 - val_accuracy: 0.1000\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 1s 361us/sample - loss: 3.0924 - accuracy: 0.9595 - val_loss: 5.9604 - val_accuracy: 0.1000\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 1s 357us/sample - loss: 2.8729 - accuracy: 0.9665 - val_loss: 5.5790 - val_accuracy: 0.1000\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 1s 364us/sample - loss: 2.6541 - accuracy: 0.9740 - val_loss: 6.0945 - val_accuracy: 0.1000\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 1s 369us/sample - loss: 2.4611 - accuracy: 0.9765 - val_loss: 5.9334 - val_accuracy: 0.1000\n",
      "Epoch 13/50\n",
      "2000/2000 [==============================] - 1s 360us/sample - loss: 2.2994 - accuracy: 0.9815 - val_loss: 5.4926 - val_accuracy: 0.1000\n",
      "Epoch 14/50\n",
      "2000/2000 [==============================] - 1s 367us/sample - loss: 2.1756 - accuracy: 0.9765 - val_loss: 6.2851 - val_accuracy: 0.1000\n",
      "Epoch 15/50\n",
      "2000/2000 [==============================] - 1s 368us/sample - loss: 2.0917 - accuracy: 0.9750 - val_loss: 4.7433 - val_accuracy: 0.1000\n",
      "Epoch 16/50\n",
      "2000/2000 [==============================] - 1s 362us/sample - loss: 2.0782 - accuracy: 0.9690 - val_loss: 5.8729 - val_accuracy: 0.1000\n",
      "Epoch 17/50\n",
      "2000/2000 [==============================] - 1s 357us/sample - loss: 2.0805 - accuracy: 0.9700 - val_loss: 5.6214 - val_accuracy: 0.1000\n",
      "Epoch 18/50\n",
      "2000/2000 [==============================] - 1s 364us/sample - loss: 2.0822 - accuracy: 0.9675 - val_loss: 6.4047 - val_accuracy: 0.1000\n",
      "Epoch 19/50\n",
      "2000/2000 [==============================] - 1s 368us/sample - loss: 2.0842 - accuracy: 0.9685 - val_loss: 5.5347 - val_accuracy: 0.1000\n",
      "Epoch 20/50\n",
      "2000/2000 [==============================] - 1s 360us/sample - loss: 2.0739 - accuracy: 0.9715 - val_loss: 7.1228 - val_accuracy: 0.1000\n",
      "Epoch 21/50\n",
      "2000/2000 [==============================] - 1s 362us/sample - loss: 2.0592 - accuracy: 0.9675 - val_loss: 7.1050 - val_accuracy: 0.1000\n",
      "Epoch 22/50\n",
      "2000/2000 [==============================] - 1s 362us/sample - loss: 2.0678 - accuracy: 0.9730 - val_loss: 4.9018 - val_accuracy: 0.1000\n",
      "Epoch 23/50\n",
      "2000/2000 [==============================] - 1s 364us/sample - loss: 2.0712 - accuracy: 0.9755 - val_loss: 4.9231 - val_accuracy: 0.1224\n",
      "Epoch 24/50\n",
      "2000/2000 [==============================] - 1s 358us/sample - loss: 2.0666 - accuracy: 0.9700 - val_loss: 5.2086 - val_accuracy: 0.1001\n",
      "Epoch 25/50\n",
      "2000/2000 [==============================] - 1s 359us/sample - loss: 2.0928 - accuracy: 0.9650 - val_loss: 5.4887 - val_accuracy: 0.1000\n",
      "Epoch 26/50\n",
      "2000/2000 [==============================] - 1s 365us/sample - loss: 2.0845 - accuracy: 0.9720 - val_loss: 5.8951 - val_accuracy: 0.1000\n",
      "Epoch 27/50\n",
      "2000/2000 [==============================] - 1s 364us/sample - loss: 2.0837 - accuracy: 0.9710 - val_loss: 7.4281 - val_accuracy: 0.1000\n",
      "Epoch 28/50\n",
      "2000/2000 [==============================] - 1s 367us/sample - loss: 2.0810 - accuracy: 0.9760 - val_loss: 6.1308 - val_accuracy: 0.1241\n",
      "Epoch 29/50\n",
      "2000/2000 [==============================] - 1s 355us/sample - loss: 2.1039 - accuracy: 0.9710 - val_loss: 5.8999 - val_accuracy: 0.1089\n",
      "Epoch 30/50\n",
      "2000/2000 [==============================] - 1s 366us/sample - loss: 2.0964 - accuracy: 0.9720 - val_loss: 5.5673 - val_accuracy: 0.1294\n",
      "Epoch 31/50\n",
      "2000/2000 [==============================] - 1s 392us/sample - loss: 2.0915 - accuracy: 0.9725 - val_loss: 6.4327 - val_accuracy: 0.1000\n",
      "Epoch 32/50\n",
      "2000/2000 [==============================] - 1s 360us/sample - loss: 2.0871 - accuracy: 0.9740 - val_loss: 6.0460 - val_accuracy: 0.1198\n",
      "Epoch 33/50\n",
      "2000/2000 [==============================] - 1s 368us/sample - loss: 2.0882 - accuracy: 0.9730 - val_loss: 5.1671 - val_accuracy: 0.1470\n",
      "Epoch 34/50\n",
      "2000/2000 [==============================] - 1s 370us/sample - loss: 2.0494 - accuracy: 0.9765 - val_loss: 4.9337 - val_accuracy: 0.1840\n",
      "Epoch 35/50\n",
      "2000/2000 [==============================] - 1s 362us/sample - loss: 2.0114 - accuracy: 0.9735 - val_loss: 5.7270 - val_accuracy: 0.1059\n",
      "Epoch 36/50\n",
      "2000/2000 [==============================] - 1s 375us/sample - loss: 2.0079 - accuracy: 0.9765 - val_loss: 5.8667 - val_accuracy: 0.1120\n",
      "Epoch 37/50\n",
      "2000/2000 [==============================] - 1s 374us/sample - loss: 2.0066 - accuracy: 0.9780 - val_loss: 5.3090 - val_accuracy: 0.1373\n",
      "Epoch 38/50\n",
      "2000/2000 [==============================] - 1s 375us/sample - loss: 2.0091 - accuracy: 0.9785 - val_loss: 5.1703 - val_accuracy: 0.1614\n",
      "Epoch 39/50\n",
      "2000/2000 [==============================] - 1s 373us/sample - loss: 2.0019 - accuracy: 0.9805 - val_loss: 6.4118 - val_accuracy: 0.1091\n",
      "Epoch 40/50\n",
      "2000/2000 [==============================] - 1s 366us/sample - loss: 2.0091 - accuracy: 0.9780 - val_loss: 5.4842 - val_accuracy: 0.1292\n",
      "Epoch 41/50\n",
      "2000/2000 [==============================] - 1s 368us/sample - loss: 2.0139 - accuracy: 0.9750 - val_loss: 4.4842 - val_accuracy: 0.1923\n",
      "Epoch 42/50\n",
      "2000/2000 [==============================] - 1s 365us/sample - loss: 2.0273 - accuracy: 0.9740 - val_loss: 6.4205 - val_accuracy: 0.1407\n",
      "Epoch 43/50\n",
      "2000/2000 [==============================] - 1s 366us/sample - loss: 2.0539 - accuracy: 0.9745 - val_loss: 5.0125 - val_accuracy: 0.2014\n",
      "Epoch 44/50\n",
      "2000/2000 [==============================] - 1s 361us/sample - loss: 2.0631 - accuracy: 0.9710 - val_loss: 4.7861 - val_accuracy: 0.2008\n",
      "Epoch 45/50\n",
      "2000/2000 [==============================] - 1s 362us/sample - loss: 2.0565 - accuracy: 0.9750 - val_loss: 5.3179 - val_accuracy: 0.1893\n",
      "Epoch 46/50\n",
      "2000/2000 [==============================] - 1s 358us/sample - loss: 2.0425 - accuracy: 0.9760 - val_loss: 4.9392 - val_accuracy: 0.2169\n",
      "Epoch 47/50\n",
      "2000/2000 [==============================] - 1s 359us/sample - loss: 1.9891 - accuracy: 0.9790 - val_loss: 4.8903 - val_accuracy: 0.2413\n",
      "Epoch 48/50\n",
      "2000/2000 [==============================] - 1s 373us/sample - loss: 1.9007 - accuracy: 0.9860 - val_loss: 5.1570 - val_accuracy: 0.2326\n",
      "Epoch 49/50\n",
      "2000/2000 [==============================] - 1s 369us/sample - loss: 1.7944 - accuracy: 0.9855 - val_loss: 4.7055 - val_accuracy: 0.2460\n",
      "Epoch 50/50\n",
      "2000/2000 [==============================] - 1s 361us/sample - loss: 1.7325 - accuracy: 0.9825 - val_loss: 4.3315 - val_accuracy: 0.2846\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 16384)        0           conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16384)        65536       flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 16384)        0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 200)          3277000     dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 200)          800         dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 200)          0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 200)          0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 200)          40200       dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 200)          800         dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 200)          0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 200)          0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 4)            804         dropout_20[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,614,900\n",
      "Trainable params: 3,578,388\n",
      "Non-trainable params: 36,512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 196000 samples, validate on 10000 samples\n",
      "Epoch 1/80\n",
      "196000/196000 [==============================] - 30s 151us/sample - loss: 2.4631 - accuracy: 0.5740 - val_loss: 1.7482 - val_accuracy: 0.6160\n",
      "Epoch 2/80\n",
      "196000/196000 [==============================] - 28s 140us/sample - loss: 1.7387 - accuracy: 0.6583 - val_loss: 1.2615 - val_accuracy: 0.8746\n",
      "Epoch 3/80\n",
      "196000/196000 [==============================] - 28s 142us/sample - loss: 1.6740 - accuracy: 0.7010 - val_loss: 1.4488 - val_accuracy: 0.7565\n",
      "Epoch 4/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 1.5979 - accuracy: 0.7301 - val_loss: 1.7249 - val_accuracy: 0.6989\n",
      "Epoch 5/80\n",
      "196000/196000 [==============================] - 27s 139us/sample - loss: 1.5344 - accuracy: 0.7541 - val_loss: 1.4197 - val_accuracy: 0.7990\n",
      "Epoch 6/80\n",
      "196000/196000 [==============================] - 28s 141us/sample - loss: 1.4741 - accuracy: 0.7710 - val_loss: 1.3302 - val_accuracy: 0.8139\n",
      "Epoch 7/80\n",
      "196000/196000 [==============================] - 27s 139us/sample - loss: 1.4186 - accuracy: 0.7841 - val_loss: 1.6869 - val_accuracy: 0.6604\n",
      "Epoch 8/80\n",
      "196000/196000 [==============================] - 27s 139us/sample - loss: 1.3612 - accuracy: 0.7980 - val_loss: 1.9142 - val_accuracy: 0.5526\n",
      "Epoch 9/80\n",
      "196000/196000 [==============================] - 27s 138us/sample - loss: 1.3205 - accuracy: 0.8077 - val_loss: 2.0056 - val_accuracy: 0.5847\n",
      "Epoch 10/80\n",
      "196000/196000 [==============================] - 27s 139us/sample - loss: 1.2739 - accuracy: 0.8166 - val_loss: 1.6104 - val_accuracy: 0.6843\n",
      "Epoch 11/80\n",
      "196000/196000 [==============================] - 27s 139us/sample - loss: 1.2377 - accuracy: 0.8263 - val_loss: 1.3335 - val_accuracy: 0.7774\n",
      "Epoch 12/80\n",
      "196000/196000 [==============================] - 28s 141us/sample - loss: 1.1961 - accuracy: 0.8328 - val_loss: 1.1070 - val_accuracy: 0.8662\n",
      "Epoch 13/80\n",
      "196000/196000 [==============================] - 27s 139us/sample - loss: 1.1679 - accuracy: 0.8391 - val_loss: 1.5836 - val_accuracy: 0.6695\n",
      "Epoch 14/80\n",
      "196000/196000 [==============================] - 27s 139us/sample - loss: 1.1275 - accuracy: 0.8451 - val_loss: 1.2768 - val_accuracy: 0.7740\n",
      "Epoch 15/80\n",
      "196000/196000 [==============================] - 27s 139us/sample - loss: 1.1005 - accuracy: 0.8515 - val_loss: 1.1944 - val_accuracy: 0.8084\n",
      "Epoch 16/80\n",
      "196000/196000 [==============================] - 27s 139us/sample - loss: 1.0668 - accuracy: 0.8567 - val_loss: 1.7253 - val_accuracy: 0.6400\n",
      "Epoch 17/80\n",
      "196000/196000 [==============================] - 28s 141us/sample - loss: 1.0379 - accuracy: 0.8623 - val_loss: 1.0528 - val_accuracy: 0.8491\n",
      "Epoch 18/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 1.0073 - accuracy: 0.8676 - val_loss: 1.1514 - val_accuracy: 0.8152\n",
      "Epoch 19/80\n",
      "196000/196000 [==============================] - 28s 140us/sample - loss: 0.9809 - accuracy: 0.8715 - val_loss: 1.5859 - val_accuracy: 0.6528\n",
      "Epoch 20/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.9560 - accuracy: 0.8758 - val_loss: 1.2131 - val_accuracy: 0.7726\n",
      "Epoch 21/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.9383 - accuracy: 0.8796 - val_loss: 1.2124 - val_accuracy: 0.7667\n",
      "Epoch 22/80\n",
      "196000/196000 [==============================] - 28s 140us/sample - loss: 0.9166 - accuracy: 0.8833 - val_loss: 0.9489 - val_accuracy: 0.8616\n",
      "Epoch 23/80\n",
      "196000/196000 [==============================] - 28s 141us/sample - loss: 0.8940 - accuracy: 0.8867 - val_loss: 1.8024 - val_accuracy: 0.6275\n",
      "Epoch 24/80\n",
      "196000/196000 [==============================] - 28s 140us/sample - loss: 0.8744 - accuracy: 0.8912 - val_loss: 1.0671 - val_accuracy: 0.8188\n",
      "Epoch 25/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.8582 - accuracy: 0.8930 - val_loss: 0.9211 - val_accuracy: 0.8737\n",
      "Epoch 26/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.8394 - accuracy: 0.8962 - val_loss: 1.5382 - val_accuracy: 0.7058\n",
      "Epoch 27/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.8266 - accuracy: 0.8991 - val_loss: 1.1704 - val_accuracy: 0.7856\n",
      "Epoch 28/80\n",
      "196000/196000 [==============================] - 28s 141us/sample - loss: 0.8088 - accuracy: 0.9021 - val_loss: 1.3773 - val_accuracy: 0.7281\n",
      "Epoch 29/80\n",
      "196000/196000 [==============================] - 28s 140us/sample - loss: 0.7970 - accuracy: 0.9050 - val_loss: 1.1742 - val_accuracy: 0.7917\n",
      "Epoch 30/80\n",
      "196000/196000 [==============================] - 28s 141us/sample - loss: 0.7828 - accuracy: 0.9073 - val_loss: 1.2724 - val_accuracy: 0.7502\n",
      "Epoch 31/80\n",
      "196000/196000 [==============================] - 27s 139us/sample - loss: 0.7710 - accuracy: 0.9095 - val_loss: 0.8994 - val_accuracy: 0.8748\n",
      "Epoch 32/80\n",
      "196000/196000 [==============================] - 27s 139us/sample - loss: 0.7626 - accuracy: 0.9110 - val_loss: 1.4684 - val_accuracy: 0.7093\n",
      "Epoch 33/80\n",
      "196000/196000 [==============================] - 27s 138us/sample - loss: 0.7458 - accuracy: 0.9142 - val_loss: 1.2166 - val_accuracy: 0.7750\n",
      "Epoch 34/80\n",
      "196000/196000 [==============================] - 27s 139us/sample - loss: 0.7382 - accuracy: 0.9171 - val_loss: 0.9847 - val_accuracy: 0.8313\n",
      "Epoch 35/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.7316 - accuracy: 0.9169 - val_loss: 1.0999 - val_accuracy: 0.7966\n",
      "Epoch 36/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.7171 - accuracy: 0.9201 - val_loss: 1.4231 - val_accuracy: 0.7167\n",
      "Epoch 37/80\n",
      "196000/196000 [==============================] - 28s 140us/sample - loss: 0.7138 - accuracy: 0.9214 - val_loss: 1.5443 - val_accuracy: 0.6985\n",
      "Epoch 38/80\n",
      "196000/196000 [==============================] - 28s 140us/sample - loss: 0.7066 - accuracy: 0.9225 - val_loss: 1.2145 - val_accuracy: 0.7892\n",
      "Epoch 39/80\n",
      "196000/196000 [==============================] - 28s 140us/sample - loss: 0.6946 - accuracy: 0.9242 - val_loss: 0.9710 - val_accuracy: 0.8308\n",
      "Epoch 40/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.6874 - accuracy: 0.9264 - val_loss: 1.3404 - val_accuracy: 0.7575\n",
      "Epoch 41/80\n",
      "196000/196000 [==============================] - 27s 139us/sample - loss: 0.6792 - accuracy: 0.9277 - val_loss: 0.9318 - val_accuracy: 0.8445\n",
      "Epoch 42/80\n",
      "196000/196000 [==============================] - 28s 143us/sample - loss: 0.6702 - accuracy: 0.9290 - val_loss: 0.9864 - val_accuracy: 0.8398\n",
      "Epoch 43/80\n",
      "196000/196000 [==============================] - 27s 139us/sample - loss: 0.6641 - accuracy: 0.9303 - val_loss: 2.2049 - val_accuracy: 0.5867\n",
      "Epoch 44/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.6557 - accuracy: 0.9318 - val_loss: 1.1672 - val_accuracy: 0.7897\n",
      "Epoch 45/80\n",
      "196000/196000 [==============================] - 28s 141us/sample - loss: 0.6534 - accuracy: 0.9321 - val_loss: 1.0533 - val_accuracy: 0.8221\n",
      "Epoch 46/80\n",
      "196000/196000 [==============================] - 27s 139us/sample - loss: 0.6500 - accuracy: 0.9339 - val_loss: 1.0087 - val_accuracy: 0.8361\n",
      "Epoch 47/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.6451 - accuracy: 0.9343 - val_loss: 1.0720 - val_accuracy: 0.8199\n",
      "Epoch 48/80\n",
      "196000/196000 [==============================] - 28s 141us/sample - loss: 0.6375 - accuracy: 0.9368 - val_loss: 1.0735 - val_accuracy: 0.8288\n",
      "Epoch 49/80\n",
      "196000/196000 [==============================] - 28s 141us/sample - loss: 0.6315 - accuracy: 0.9377 - val_loss: 1.0548 - val_accuracy: 0.8178\n",
      "Epoch 50/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.6280 - accuracy: 0.9392 - val_loss: 1.5687 - val_accuracy: 0.7086\n",
      "Epoch 51/80\n",
      "196000/196000 [==============================] - 28s 140us/sample - loss: 0.6226 - accuracy: 0.9401 - val_loss: 1.2873 - val_accuracy: 0.7706\n",
      "Epoch 52/80\n",
      "196000/196000 [==============================] - 27s 139us/sample - loss: 0.6187 - accuracy: 0.9411 - val_loss: 1.3840 - val_accuracy: 0.7708\n",
      "Epoch 53/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.6112 - accuracy: 0.9423 - val_loss: 1.3062 - val_accuracy: 0.7725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.6085 - accuracy: 0.9419 - val_loss: 1.2648 - val_accuracy: 0.7730\n",
      "Epoch 55/80\n",
      "196000/196000 [==============================] - 28s 141us/sample - loss: 0.6051 - accuracy: 0.9429 - val_loss: 1.2304 - val_accuracy: 0.7701\n",
      "Epoch 56/80\n",
      "196000/196000 [==============================] - 28s 141us/sample - loss: 0.5990 - accuracy: 0.9445 - val_loss: 1.0543 - val_accuracy: 0.8272\n",
      "Epoch 57/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.5988 - accuracy: 0.9457 - val_loss: 1.2310 - val_accuracy: 0.7798\n",
      "Epoch 58/80\n",
      "196000/196000 [==============================] - 27s 139us/sample - loss: 0.5952 - accuracy: 0.9458 - val_loss: 1.0577 - val_accuracy: 0.8025\n",
      "Epoch 59/80\n",
      "196000/196000 [==============================] - 27s 139us/sample - loss: 0.5877 - accuracy: 0.9470 - val_loss: 1.1528 - val_accuracy: 0.8117\n",
      "Epoch 60/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.5863 - accuracy: 0.9476 - val_loss: 1.1439 - val_accuracy: 0.8103\n",
      "Epoch 61/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.5867 - accuracy: 0.9474 - val_loss: 0.9434 - val_accuracy: 0.8677\n",
      "Epoch 62/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.5742 - accuracy: 0.9488 - val_loss: 1.1426 - val_accuracy: 0.8058\n",
      "Epoch 63/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.5758 - accuracy: 0.9491 - val_loss: 0.9771 - val_accuracy: 0.8462\n",
      "Epoch 64/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.5694 - accuracy: 0.9501 - val_loss: 1.3717 - val_accuracy: 0.7497\n",
      "Epoch 65/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.5668 - accuracy: 0.9507 - val_loss: 1.4644 - val_accuracy: 0.7459\n",
      "Epoch 66/80\n",
      "196000/196000 [==============================] - 28s 140us/sample - loss: 0.5601 - accuracy: 0.9524 - val_loss: 1.3379 - val_accuracy: 0.7742\n",
      "Epoch 67/80\n",
      "196000/196000 [==============================] - 28s 141us/sample - loss: 0.5618 - accuracy: 0.9516 - val_loss: 1.7717 - val_accuracy: 0.6816\n",
      "Epoch 68/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.5580 - accuracy: 0.9518 - val_loss: 1.1580 - val_accuracy: 0.8063\n",
      "Epoch 69/80\n",
      "196000/196000 [==============================] - 28s 142us/sample - loss: 0.5513 - accuracy: 0.9537 - val_loss: 1.4314 - val_accuracy: 0.7544\n",
      "Epoch 70/80\n",
      "196000/196000 [==============================] - 27s 139us/sample - loss: 0.5490 - accuracy: 0.9543 - val_loss: 1.3014 - val_accuracy: 0.7775\n",
      "Epoch 71/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.5455 - accuracy: 0.9549 - val_loss: 1.2643 - val_accuracy: 0.7778\n",
      "Epoch 72/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.5452 - accuracy: 0.9549 - val_loss: 1.0580 - val_accuracy: 0.8401\n",
      "Epoch 73/80\n",
      "196000/196000 [==============================] - 27s 139us/sample - loss: 0.5428 - accuracy: 0.9550 - val_loss: 1.2646 - val_accuracy: 0.7779\n",
      "Epoch 74/80\n",
      "196000/196000 [==============================] - 27s 139us/sample - loss: 0.5331 - accuracy: 0.9563 - val_loss: 1.5004 - val_accuracy: 0.7333\n",
      "Epoch 75/80\n",
      "196000/196000 [==============================] - 27s 139us/sample - loss: 0.5372 - accuracy: 0.9561 - val_loss: 1.0824 - val_accuracy: 0.8246\n",
      "Epoch 76/80\n",
      "196000/196000 [==============================] - 27s 139us/sample - loss: 0.5345 - accuracy: 0.9567 - val_loss: 1.5271 - val_accuracy: 0.7403\n",
      "Epoch 77/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.5296 - accuracy: 0.9574 - val_loss: 1.7735 - val_accuracy: 0.7210\n",
      "Epoch 78/80\n",
      "196000/196000 [==============================] - 28s 141us/sample - loss: 0.5278 - accuracy: 0.9579 - val_loss: 1.0876 - val_accuracy: 0.8287\n",
      "Epoch 79/80\n",
      "196000/196000 [==============================] - 27s 140us/sample - loss: 0.5247 - accuracy: 0.9588 - val_loss: 1.3972 - val_accuracy: 0.7561\n",
      "Epoch 80/80\n",
      "196000/196000 [==============================] - 27s 139us/sample - loss: 0.5260 - accuracy: 0.9579 - val_loss: 1.6284 - val_accuracy: 0.7295\n",
      "INFO:tensorflow:Assets written to: selfsupervised_2/assets\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 16384)        0           conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16384)        65536       flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 16384)        0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 200)          3277000     dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 200)          800         dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 200)          0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 200)          0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 200)          40200       dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 200)          800         dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 200)          0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 200)          0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 10)           2010        dropout_23[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,473,002\n",
      "Trainable params: 3,438,026\n",
      "Non-trainable params: 34,976\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "4000/4000 [==============================] - 2s 601us/sample - loss: 6.1349 - accuracy: 0.2885 - val_loss: 9.2522 - val_accuracy: 0.1064\n",
      "Epoch 2/50\n",
      "4000/4000 [==============================] - 1s 232us/sample - loss: 5.3271 - accuracy: 0.5145 - val_loss: 8.4348 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "4000/4000 [==============================] - 1s 235us/sample - loss: 4.6799 - accuracy: 0.6678 - val_loss: 7.4296 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "4000/4000 [==============================] - 1s 231us/sample - loss: 4.1574 - accuracy: 0.7682 - val_loss: 6.3004 - val_accuracy: 0.1000\n",
      "Epoch 5/50\n",
      "4000/4000 [==============================] - 1s 235us/sample - loss: 3.7209 - accuracy: 0.8415 - val_loss: 6.2669 - val_accuracy: 0.1000\n",
      "Epoch 6/50\n",
      "4000/4000 [==============================] - 1s 237us/sample - loss: 3.3794 - accuracy: 0.8817 - val_loss: 5.7653 - val_accuracy: 0.1000\n",
      "Epoch 7/50\n",
      "4000/4000 [==============================] - 1s 234us/sample - loss: 3.1278 - accuracy: 0.9010 - val_loss: 5.8851 - val_accuracy: 0.1000\n",
      "Epoch 8/50\n",
      "4000/4000 [==============================] - 1s 232us/sample - loss: 2.9977 - accuracy: 0.9130 - val_loss: 6.0061 - val_accuracy: 0.1000\n",
      "Epoch 9/50\n",
      "4000/4000 [==============================] - 1s 235us/sample - loss: 2.8758 - accuracy: 0.9260 - val_loss: 5.7829 - val_accuracy: 0.1001\n",
      "Epoch 10/50\n",
      "4000/4000 [==============================] - 1s 231us/sample - loss: 2.7788 - accuracy: 0.9273 - val_loss: 6.0196 - val_accuracy: 0.1000\n",
      "Epoch 11/50\n",
      "4000/4000 [==============================] - 1s 229us/sample - loss: 2.7402 - accuracy: 0.9308 - val_loss: 6.3667 - val_accuracy: 0.1000\n",
      "Epoch 12/50\n",
      "4000/4000 [==============================] - 1s 238us/sample - loss: 2.7055 - accuracy: 0.9333 - val_loss: 6.3624 - val_accuracy: 0.1144\n",
      "Epoch 13/50\n",
      "4000/4000 [==============================] - 1s 237us/sample - loss: 2.7045 - accuracy: 0.9305 - val_loss: 6.3513 - val_accuracy: 0.1012\n",
      "Epoch 14/50\n",
      "4000/4000 [==============================] - 1s 237us/sample - loss: 2.7021 - accuracy: 0.9388 - val_loss: 6.0337 - val_accuracy: 0.1011\n",
      "Epoch 15/50\n",
      "4000/4000 [==============================] - 1s 238us/sample - loss: 2.7293 - accuracy: 0.9290 - val_loss: 6.0217 - val_accuracy: 0.1178\n",
      "Epoch 16/50\n",
      "4000/4000 [==============================] - 1s 237us/sample - loss: 2.8052 - accuracy: 0.9250 - val_loss: 6.2955 - val_accuracy: 0.1045\n",
      "Epoch 17/50\n",
      "4000/4000 [==============================] - 1s 242us/sample - loss: 2.8444 - accuracy: 0.9340 - val_loss: 6.2669 - val_accuracy: 0.1529\n",
      "Epoch 18/50\n",
      "4000/4000 [==============================] - 1s 236us/sample - loss: 2.8118 - accuracy: 0.9392 - val_loss: 6.8637 - val_accuracy: 0.1125\n",
      "Epoch 19/50\n",
      "4000/4000 [==============================] - 1s 232us/sample - loss: 2.6916 - accuracy: 0.9473 - val_loss: 5.8473 - val_accuracy: 0.1702\n",
      "Epoch 20/50\n",
      "4000/4000 [==============================] - 1s 238us/sample - loss: 2.6019 - accuracy: 0.9515 - val_loss: 5.5442 - val_accuracy: 0.1837\n",
      "Epoch 21/50\n",
      "4000/4000 [==============================] - 1s 236us/sample - loss: 2.5684 - accuracy: 0.9513 - val_loss: 5.4966 - val_accuracy: 0.1468\n",
      "Epoch 22/50\n",
      "4000/4000 [==============================] - 1s 234us/sample - loss: 2.6163 - accuracy: 0.9440 - val_loss: 5.3341 - val_accuracy: 0.1540\n",
      "Epoch 23/50\n",
      "4000/4000 [==============================] - 1s 236us/sample - loss: 2.6617 - accuracy: 0.9442 - val_loss: 5.6332 - val_accuracy: 0.2021\n",
      "Epoch 24/50\n",
      "4000/4000 [==============================] - 1s 236us/sample - loss: 2.7263 - accuracy: 0.9402 - val_loss: 4.8976 - val_accuracy: 0.3082\n",
      "Epoch 25/50\n",
      "4000/4000 [==============================] - 1s 229us/sample - loss: 2.7292 - accuracy: 0.9450 - val_loss: 4.8234 - val_accuracy: 0.3235\n",
      "Epoch 26/50\n",
      "4000/4000 [==============================] - 1s 231us/sample - loss: 2.6644 - accuracy: 0.9545 - val_loss: 4.5716 - val_accuracy: 0.3605\n",
      "Epoch 27/50\n",
      "4000/4000 [==============================] - 1s 233us/sample - loss: 2.5554 - accuracy: 0.9620 - val_loss: 4.4546 - val_accuracy: 0.3980\n",
      "Epoch 28/50\n",
      "4000/4000 [==============================] - 1s 233us/sample - loss: 2.5324 - accuracy: 0.9528 - val_loss: 4.5140 - val_accuracy: 0.4012\n",
      "Epoch 29/50\n",
      "4000/4000 [==============================] - 1s 235us/sample - loss: 2.5710 - accuracy: 0.9553 - val_loss: 4.7807 - val_accuracy: 0.4038\n",
      "Epoch 30/50\n",
      "4000/4000 [==============================] - 1s 235us/sample - loss: 2.6183 - accuracy: 0.9488 - val_loss: 4.7203 - val_accuracy: 0.4267\n",
      "Epoch 31/50\n",
      "4000/4000 [==============================] - 1s 232us/sample - loss: 2.6107 - accuracy: 0.9545 - val_loss: 4.5207 - val_accuracy: 0.4672\n",
      "Epoch 32/50\n",
      "4000/4000 [==============================] - 1s 231us/sample - loss: 2.5802 - accuracy: 0.9567 - val_loss: 4.6650 - val_accuracy: 0.4492\n",
      "Epoch 33/50\n",
      "4000/4000 [==============================] - 1s 236us/sample - loss: 2.5802 - accuracy: 0.9535 - val_loss: 4.3758 - val_accuracy: 0.4847\n",
      "Epoch 34/50\n",
      "4000/4000 [==============================] - 1s 236us/sample - loss: 2.6408 - accuracy: 0.9490 - val_loss: 4.7436 - val_accuracy: 0.4507\n",
      "Epoch 35/50\n",
      "4000/4000 [==============================] - 1s 237us/sample - loss: 2.6326 - accuracy: 0.9532 - val_loss: 4.4876 - val_accuracy: 0.4944\n",
      "Epoch 36/50\n",
      "4000/4000 [==============================] - 1s 233us/sample - loss: 2.5828 - accuracy: 0.9580 - val_loss: 4.2645 - val_accuracy: 0.5069\n",
      "Epoch 37/50\n",
      "4000/4000 [==============================] - 1s 234us/sample - loss: 2.5242 - accuracy: 0.9600 - val_loss: 4.4698 - val_accuracy: 0.4886\n",
      "Epoch 38/50\n",
      "4000/4000 [==============================] - 1s 238us/sample - loss: 2.6150 - accuracy: 0.9470 - val_loss: 4.4758 - val_accuracy: 0.5023\n",
      "Epoch 39/50\n",
      "4000/4000 [==============================] - 1s 235us/sample - loss: 2.5943 - accuracy: 0.9630 - val_loss: 4.3697 - val_accuracy: 0.5086\n",
      "Epoch 40/50\n",
      "4000/4000 [==============================] - 1s 237us/sample - loss: 2.4497 - accuracy: 0.9710 - val_loss: 4.1194 - val_accuracy: 0.5035\n",
      "Epoch 41/50\n",
      "4000/4000 [==============================] - 1s 237us/sample - loss: 2.3650 - accuracy: 0.9645 - val_loss: 4.1831 - val_accuracy: 0.5032\n",
      "Epoch 42/50\n",
      "4000/4000 [==============================] - 1s 230us/sample - loss: 2.3887 - accuracy: 0.9600 - val_loss: 4.3514 - val_accuracy: 0.4928\n",
      "Epoch 43/50\n",
      "4000/4000 [==============================] - 1s 233us/sample - loss: 2.5545 - accuracy: 0.9475 - val_loss: 4.5702 - val_accuracy: 0.4908\n",
      "Epoch 44/50\n",
      "4000/4000 [==============================] - 1s 235us/sample - loss: 2.6966 - accuracy: 0.9488 - val_loss: 4.6712 - val_accuracy: 0.4792\n",
      "Epoch 45/50\n",
      "4000/4000 [==============================] - 1s 238us/sample - loss: 2.6769 - accuracy: 0.9580 - val_loss: 4.6212 - val_accuracy: 0.4982\n",
      "Epoch 46/50\n",
      "4000/4000 [==============================] - 1s 234us/sample - loss: 2.5774 - accuracy: 0.9665 - val_loss: 4.5351 - val_accuracy: 0.4746\n",
      "Epoch 47/50\n",
      "4000/4000 [==============================] - 1s 233us/sample - loss: 2.5019 - accuracy: 0.9630 - val_loss: 4.4644 - val_accuracy: 0.4862\n",
      "Epoch 48/50\n",
      "4000/4000 [==============================] - 1s 229us/sample - loss: 2.4448 - accuracy: 0.9655 - val_loss: 4.3964 - val_accuracy: 0.5069\n",
      "Epoch 49/50\n",
      "4000/4000 [==============================] - 1s 239us/sample - loss: 2.3941 - accuracy: 0.9670 - val_loss: 4.2160 - val_accuracy: 0.5143\n",
      "Epoch 50/50\n",
      "4000/4000 [==============================] - 1s 238us/sample - loss: 2.3376 - accuracy: 0.9705 - val_loss: 4.2786 - val_accuracy: 0.4960\n",
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 16384)        0           conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16384)        65536       flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 16384)        0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 200)          3277000     dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 200)          800         dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 200)          0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 200)          0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 200)          40200       dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 200)          800         dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 200)          0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 200)          0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 4)            804         dropout_26[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,614,900\n",
      "Trainable params: 3,578,388\n",
      "Non-trainable params: 36,512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 194000 samples, validate on 10000 samples\n",
      "Epoch 1/80\n",
      "194000/194000 [==============================] - 30s 154us/sample - loss: 2.4468 - accuracy: 0.5786 - val_loss: 1.7751 - val_accuracy: 0.5844\n",
      "Epoch 2/80\n",
      "194000/194000 [==============================] - 28s 144us/sample - loss: 1.7122 - accuracy: 0.6612 - val_loss: 1.8818 - val_accuracy: 0.6066\n",
      "Epoch 3/80\n",
      "194000/194000 [==============================] - 28s 142us/sample - loss: 1.6413 - accuracy: 0.7026 - val_loss: 1.3714 - val_accuracy: 0.8022\n",
      "Epoch 4/80\n",
      "194000/194000 [==============================] - 28s 144us/sample - loss: 1.5673 - accuracy: 0.7330 - val_loss: 1.5646 - val_accuracy: 0.7111\n",
      "Epoch 5/80\n",
      "194000/194000 [==============================] - 28s 144us/sample - loss: 1.5032 - accuracy: 0.7538 - val_loss: 1.6296 - val_accuracy: 0.6920\n",
      "Epoch 6/80\n",
      "194000/194000 [==============================] - 28s 142us/sample - loss: 1.4431 - accuracy: 0.7724 - val_loss: 1.6602 - val_accuracy: 0.6910\n",
      "Epoch 7/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 1.3966 - accuracy: 0.7844 - val_loss: 1.4324 - val_accuracy: 0.7569\n",
      "Epoch 8/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 1.3445 - accuracy: 0.7974 - val_loss: 1.4232 - val_accuracy: 0.7468\n",
      "Epoch 9/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 1.2990 - accuracy: 0.8074 - val_loss: 1.3050 - val_accuracy: 0.8150\n",
      "Epoch 10/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 1.2628 - accuracy: 0.8160 - val_loss: 1.5370 - val_accuracy: 0.7129\n",
      "Epoch 11/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 1.2203 - accuracy: 0.8254 - val_loss: 1.3410 - val_accuracy: 0.7537\n",
      "Epoch 12/80\n",
      "194000/194000 [==============================] - 28s 144us/sample - loss: 1.1815 - accuracy: 0.8337 - val_loss: 1.7396 - val_accuracy: 0.6365\n",
      "Epoch 13/80\n",
      "194000/194000 [==============================] - 27s 142us/sample - loss: 1.1420 - accuracy: 0.8401 - val_loss: 1.3588 - val_accuracy: 0.7573\n",
      "Epoch 14/80\n",
      "194000/194000 [==============================] - 27s 141us/sample - loss: 1.1089 - accuracy: 0.8471 - val_loss: 1.2347 - val_accuracy: 0.7966\n",
      "Epoch 15/80\n",
      "194000/194000 [==============================] - 27s 141us/sample - loss: 1.0841 - accuracy: 0.8513 - val_loss: 1.2414 - val_accuracy: 0.7671\n",
      "Epoch 16/80\n",
      "194000/194000 [==============================] - 28s 144us/sample - loss: 1.0525 - accuracy: 0.8583 - val_loss: 1.2160 - val_accuracy: 0.7910\n",
      "Epoch 17/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 1.0250 - accuracy: 0.8623 - val_loss: 1.1077 - val_accuracy: 0.8470\n",
      "Epoch 18/80\n",
      "194000/194000 [==============================] - 28s 144us/sample - loss: 0.9967 - accuracy: 0.8680 - val_loss: 1.1696 - val_accuracy: 0.7974\n",
      "Epoch 19/80\n",
      "194000/194000 [==============================] - 28s 142us/sample - loss: 0.9741 - accuracy: 0.8720 - val_loss: 0.9597 - val_accuracy: 0.8529\n",
      "Epoch 20/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.9505 - accuracy: 0.8761 - val_loss: 1.3473 - val_accuracy: 0.7628\n",
      "Epoch 21/80\n",
      "194000/194000 [==============================] - 28s 145us/sample - loss: 0.9296 - accuracy: 0.8800 - val_loss: 1.2273 - val_accuracy: 0.7817\n",
      "Epoch 22/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.9046 - accuracy: 0.8837 - val_loss: 1.1127 - val_accuracy: 0.8205\n",
      "Epoch 23/80\n",
      "194000/194000 [==============================] - 28s 144us/sample - loss: 0.8828 - accuracy: 0.8877 - val_loss: 1.0821 - val_accuracy: 0.8045\n",
      "Epoch 24/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.8709 - accuracy: 0.8913 - val_loss: 1.2046 - val_accuracy: 0.7825\n",
      "Epoch 25/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.8471 - accuracy: 0.8945 - val_loss: 1.2567 - val_accuracy: 0.7433\n",
      "Epoch 26/80\n",
      "194000/194000 [==============================] - 28s 142us/sample - loss: 0.8339 - accuracy: 0.8972 - val_loss: 1.1806 - val_accuracy: 0.8104\n",
      "Epoch 27/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.8173 - accuracy: 0.8997 - val_loss: 0.8795 - val_accuracy: 0.8842\n",
      "Epoch 28/80\n",
      "194000/194000 [==============================] - 27s 140us/sample - loss: 0.8030 - accuracy: 0.9034 - val_loss: 1.0803 - val_accuracy: 0.8023\n",
      "Epoch 29/80\n",
      "194000/194000 [==============================] - 28s 144us/sample - loss: 0.7917 - accuracy: 0.9061 - val_loss: 1.0813 - val_accuracy: 0.8188\n",
      "Epoch 30/80\n",
      "194000/194000 [==============================] - 28s 144us/sample - loss: 0.7772 - accuracy: 0.9080 - val_loss: 1.2097 - val_accuracy: 0.7696\n",
      "Epoch 31/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.7619 - accuracy: 0.9108 - val_loss: 1.4483 - val_accuracy: 0.7020\n",
      "Epoch 32/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.7548 - accuracy: 0.9123 - val_loss: 0.8642 - val_accuracy: 0.8721\n",
      "Epoch 33/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.7412 - accuracy: 0.9141 - val_loss: 1.0937 - val_accuracy: 0.8059\n",
      "Epoch 34/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.7327 - accuracy: 0.9169 - val_loss: 1.3086 - val_accuracy: 0.7342\n",
      "Epoch 35/80\n",
      "194000/194000 [==============================] - 27s 141us/sample - loss: 0.7255 - accuracy: 0.9190 - val_loss: 1.1094 - val_accuracy: 0.8156\n",
      "Epoch 36/80\n",
      "194000/194000 [==============================] - 28s 142us/sample - loss: 0.7135 - accuracy: 0.9206 - val_loss: 0.9627 - val_accuracy: 0.8525\n",
      "Epoch 37/80\n",
      "194000/194000 [==============================] - 28s 144us/sample - loss: 0.7087 - accuracy: 0.9220 - val_loss: 1.0764 - val_accuracy: 0.8298\n",
      "Epoch 38/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.6964 - accuracy: 0.9237 - val_loss: 1.6378 - val_accuracy: 0.6646\n",
      "Epoch 39/80\n",
      "194000/194000 [==============================] - 28s 142us/sample - loss: 0.6917 - accuracy: 0.9252 - val_loss: 1.3223 - val_accuracy: 0.7489\n",
      "Epoch 40/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.6803 - accuracy: 0.9271 - val_loss: 1.0824 - val_accuracy: 0.8085\n",
      "Epoch 41/80\n",
      "194000/194000 [==============================] - 28s 144us/sample - loss: 0.6778 - accuracy: 0.9291 - val_loss: 1.1008 - val_accuracy: 0.8101\n",
      "Epoch 42/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.6681 - accuracy: 0.9299 - val_loss: 1.0097 - val_accuracy: 0.8380\n",
      "Epoch 43/80\n",
      "194000/194000 [==============================] - 28s 142us/sample - loss: 0.6630 - accuracy: 0.9320 - val_loss: 1.1400 - val_accuracy: 0.7927\n",
      "Epoch 44/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.6533 - accuracy: 0.9335 - val_loss: 1.1582 - val_accuracy: 0.7919\n",
      "Epoch 45/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.6487 - accuracy: 0.9344 - val_loss: 0.9271 - val_accuracy: 0.8636\n",
      "Epoch 46/80\n",
      "194000/194000 [==============================] - 28s 144us/sample - loss: 0.6443 - accuracy: 0.9354 - val_loss: 0.9370 - val_accuracy: 0.8503\n",
      "Epoch 47/80\n",
      "194000/194000 [==============================] - 28s 144us/sample - loss: 0.6342 - accuracy: 0.9374 - val_loss: 1.1921 - val_accuracy: 0.7816\n",
      "Epoch 48/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.6320 - accuracy: 0.9373 - val_loss: 1.2479 - val_accuracy: 0.7651\n",
      "Epoch 49/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.6260 - accuracy: 0.9386 - val_loss: 1.3071 - val_accuracy: 0.7597\n",
      "Epoch 50/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.6206 - accuracy: 0.9399 - val_loss: 1.3510 - val_accuracy: 0.7715\n",
      "Epoch 51/80\n",
      "194000/194000 [==============================] - 28s 144us/sample - loss: 0.6144 - accuracy: 0.9414 - val_loss: 1.0884 - val_accuracy: 0.8202\n",
      "Epoch 52/80\n",
      "194000/194000 [==============================] - 28s 142us/sample - loss: 0.6094 - accuracy: 0.9417 - val_loss: 1.2454 - val_accuracy: 0.7884\n",
      "Epoch 53/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.6048 - accuracy: 0.9425 - val_loss: 1.3569 - val_accuracy: 0.7586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.6017 - accuracy: 0.9435 - val_loss: 1.2332 - val_accuracy: 0.7747\n",
      "Epoch 55/80\n",
      "194000/194000 [==============================] - 28s 144us/sample - loss: 0.5966 - accuracy: 0.9441 - val_loss: 1.1893 - val_accuracy: 0.7965\n",
      "Epoch 56/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.5902 - accuracy: 0.9456 - val_loss: 0.9918 - val_accuracy: 0.8401\n",
      "Epoch 57/80\n",
      "194000/194000 [==============================] - 27s 140us/sample - loss: 0.5839 - accuracy: 0.9465 - val_loss: 1.2699 - val_accuracy: 0.7691\n",
      "Epoch 58/80\n",
      "194000/194000 [==============================] - 28s 144us/sample - loss: 0.5842 - accuracy: 0.9464 - val_loss: 1.4603 - val_accuracy: 0.7304\n",
      "Epoch 59/80\n",
      "194000/194000 [==============================] - 28s 144us/sample - loss: 0.5780 - accuracy: 0.9482 - val_loss: 1.5564 - val_accuracy: 0.7056\n",
      "Epoch 60/80\n",
      "194000/194000 [==============================] - 28s 144us/sample - loss: 0.5747 - accuracy: 0.9488 - val_loss: 1.2820 - val_accuracy: 0.7620\n",
      "Epoch 61/80\n",
      "194000/194000 [==============================] - 28s 142us/sample - loss: 0.5711 - accuracy: 0.9502 - val_loss: 1.1166 - val_accuracy: 0.8075\n",
      "Epoch 62/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.5661 - accuracy: 0.9503 - val_loss: 1.1916 - val_accuracy: 0.7887\n",
      "Epoch 63/80\n",
      "194000/194000 [==============================] - 28s 145us/sample - loss: 0.5628 - accuracy: 0.9505 - val_loss: 1.3621 - val_accuracy: 0.7793\n",
      "Epoch 64/80\n",
      "194000/194000 [==============================] - 28s 142us/sample - loss: 0.5608 - accuracy: 0.9517 - val_loss: 1.4440 - val_accuracy: 0.7772\n",
      "Epoch 65/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.5566 - accuracy: 0.9517 - val_loss: 0.9295 - val_accuracy: 0.8638\n",
      "Epoch 66/80\n",
      "194000/194000 [==============================] - 27s 139us/sample - loss: 0.5481 - accuracy: 0.9533 - val_loss: 1.4120 - val_accuracy: 0.7436\n",
      "Epoch 67/80\n",
      "194000/194000 [==============================] - 28s 144us/sample - loss: 0.5464 - accuracy: 0.9535 - val_loss: 1.2404 - val_accuracy: 0.7997\n",
      "Epoch 68/80\n",
      "194000/194000 [==============================] - 28s 142us/sample - loss: 0.5452 - accuracy: 0.9539 - val_loss: 1.1745 - val_accuracy: 0.8043\n",
      "Epoch 69/80\n",
      "194000/194000 [==============================] - 28s 144us/sample - loss: 0.5417 - accuracy: 0.9545 - val_loss: 1.0229 - val_accuracy: 0.8389\n",
      "Epoch 70/80\n",
      "194000/194000 [==============================] - 28s 142us/sample - loss: 0.5396 - accuracy: 0.9554 - val_loss: 0.9937 - val_accuracy: 0.8417\n",
      "Epoch 71/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.5379 - accuracy: 0.9549 - val_loss: 1.5442 - val_accuracy: 0.7468\n",
      "Epoch 72/80\n",
      "194000/194000 [==============================] - 28s 142us/sample - loss: 0.5337 - accuracy: 0.9561 - val_loss: 1.1515 - val_accuracy: 0.8202\n",
      "Epoch 73/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.5319 - accuracy: 0.9564 - val_loss: 1.0957 - val_accuracy: 0.8271\n",
      "Epoch 74/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.5322 - accuracy: 0.9565 - val_loss: 1.6530 - val_accuracy: 0.7376\n",
      "Epoch 75/80\n",
      "194000/194000 [==============================] - 28s 144us/sample - loss: 0.5269 - accuracy: 0.9572 - val_loss: 1.0887 - val_accuracy: 0.8179\n",
      "Epoch 76/80\n",
      "194000/194000 [==============================] - 28s 144us/sample - loss: 0.5241 - accuracy: 0.9579 - val_loss: 1.0951 - val_accuracy: 0.8273\n",
      "Epoch 77/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.5218 - accuracy: 0.9582 - val_loss: 1.2760 - val_accuracy: 0.7769\n",
      "Epoch 78/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.5191 - accuracy: 0.9586 - val_loss: 1.3024 - val_accuracy: 0.7957\n",
      "Epoch 79/80\n",
      "194000/194000 [==============================] - 28s 143us/sample - loss: 0.5170 - accuracy: 0.9592 - val_loss: 1.5135 - val_accuracy: 0.7248\n",
      "Epoch 80/80\n",
      "194000/194000 [==============================] - 28s 144us/sample - loss: 0.5158 - accuracy: 0.9599 - val_loss: 1.2707 - val_accuracy: 0.7913\n",
      "INFO:tensorflow:Assets written to: selfsupervised_3/assets\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 16384)        0           conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 16384)        65536       flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 16384)        0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 200)          3277000     dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 200)          800         dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 200)          0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 200)          0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 200)          40200       dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 200)          800         dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 200)          0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 200)          0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 10)           2010        dropout_29[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,473,002\n",
      "Trainable params: 3,438,026\n",
      "Non-trainable params: 34,976\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "6000/6000 [==============================] - 3s 451us/sample - loss: 6.0010 - accuracy: 0.3162 - val_loss: 7.8474 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "6000/6000 [==============================] - 1s 206us/sample - loss: 5.0281 - accuracy: 0.5620 - val_loss: 6.8129 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "6000/6000 [==============================] - 1s 207us/sample - loss: 4.3240 - accuracy: 0.6942 - val_loss: 6.3710 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "6000/6000 [==============================] - 1s 207us/sample - loss: 3.7866 - accuracy: 0.7858 - val_loss: 5.9952 - val_accuracy: 0.1000\n",
      "Epoch 5/50\n",
      "6000/6000 [==============================] - 1s 210us/sample - loss: 3.4336 - accuracy: 0.8385 - val_loss: 5.6514 - val_accuracy: 0.1000\n",
      "Epoch 6/50\n",
      "6000/6000 [==============================] - 1s 198us/sample - loss: 3.1909 - accuracy: 0.8695 - val_loss: 5.6041 - val_accuracy: 0.1363\n",
      "Epoch 7/50\n",
      "6000/6000 [==============================] - 1s 190us/sample - loss: 3.0472 - accuracy: 0.8910 - val_loss: 6.7367 - val_accuracy: 0.1000\n",
      "Epoch 8/50\n",
      "6000/6000 [==============================] - 1s 193us/sample - loss: 2.9719 - accuracy: 0.8948 - val_loss: 6.3402 - val_accuracy: 0.1180\n",
      "Epoch 9/50\n",
      "6000/6000 [==============================] - 1s 188us/sample - loss: 2.9721 - accuracy: 0.8972 - val_loss: 6.5304 - val_accuracy: 0.1023\n",
      "Epoch 10/50\n",
      "6000/6000 [==============================] - 1s 191us/sample - loss: 2.9729 - accuracy: 0.9033 - val_loss: 6.4551 - val_accuracy: 0.1002\n",
      "Epoch 11/50\n",
      "6000/6000 [==============================] - 1s 190us/sample - loss: 2.9447 - accuracy: 0.9082 - val_loss: 7.0751 - val_accuracy: 0.1686\n",
      "Epoch 12/50\n",
      "6000/6000 [==============================] - 1s 191us/sample - loss: 2.9491 - accuracy: 0.9113 - val_loss: 6.1544 - val_accuracy: 0.2316\n",
      "Epoch 13/50\n",
      "6000/6000 [==============================] - 1s 192us/sample - loss: 2.8743 - accuracy: 0.9200 - val_loss: 6.8559 - val_accuracy: 0.1319\n",
      "Epoch 14/50\n",
      "6000/6000 [==============================] - 1s 195us/sample - loss: 2.8531 - accuracy: 0.9245 - val_loss: 5.4069 - val_accuracy: 0.1648\n",
      "Epoch 15/50\n",
      "6000/6000 [==============================] - 1s 192us/sample - loss: 2.8796 - accuracy: 0.9147 - val_loss: 5.7617 - val_accuracy: 0.2027\n",
      "Epoch 16/50\n",
      "6000/6000 [==============================] - 1s 195us/sample - loss: 2.8909 - accuracy: 0.9253 - val_loss: 5.9388 - val_accuracy: 0.2071\n",
      "Epoch 17/50\n",
      "6000/6000 [==============================] - 1s 190us/sample - loss: 2.8461 - accuracy: 0.9297 - val_loss: 5.1970 - val_accuracy: 0.3396\n",
      "Epoch 18/50\n",
      "6000/6000 [==============================] - 1s 188us/sample - loss: 2.8707 - accuracy: 0.9298 - val_loss: 5.2506 - val_accuracy: 0.3536\n",
      "Epoch 19/50\n",
      "6000/6000 [==============================] - 1s 190us/sample - loss: 2.8270 - accuracy: 0.9310 - val_loss: 4.9451 - val_accuracy: 0.3950\n",
      "Epoch 20/50\n",
      "6000/6000 [==============================] - 1s 191us/sample - loss: 2.8445 - accuracy: 0.9298 - val_loss: 4.8852 - val_accuracy: 0.4252\n",
      "Epoch 21/50\n",
      "6000/6000 [==============================] - 1s 189us/sample - loss: 2.7987 - accuracy: 0.9417 - val_loss: 4.4098 - val_accuracy: 0.5029\n",
      "Epoch 22/50\n",
      "6000/6000 [==============================] - 1s 194us/sample - loss: 2.7669 - accuracy: 0.9368 - val_loss: 4.4179 - val_accuracy: 0.4957\n",
      "Epoch 23/50\n",
      "6000/6000 [==============================] - 1s 188us/sample - loss: 2.8145 - accuracy: 0.9372 - val_loss: 4.4106 - val_accuracy: 0.5264\n",
      "Epoch 24/50\n",
      "6000/6000 [==============================] - 1s 192us/sample - loss: 2.7574 - accuracy: 0.9453 - val_loss: 4.2996 - val_accuracy: 0.5357\n",
      "Epoch 25/50\n",
      "6000/6000 [==============================] - 1s 190us/sample - loss: 2.6879 - accuracy: 0.9492 - val_loss: 4.4328 - val_accuracy: 0.4893\n",
      "Epoch 26/50\n",
      "6000/6000 [==============================] - 1s 191us/sample - loss: 2.6669 - accuracy: 0.9457 - val_loss: 4.1465 - val_accuracy: 0.5419\n",
      "Epoch 27/50\n",
      "6000/6000 [==============================] - 1s 192us/sample - loss: 2.7475 - accuracy: 0.9342 - val_loss: 4.3773 - val_accuracy: 0.5367\n",
      "Epoch 28/50\n",
      "6000/6000 [==============================] - 1s 187us/sample - loss: 2.7792 - accuracy: 0.9415 - val_loss: 4.3992 - val_accuracy: 0.5309\n",
      "Epoch 29/50\n",
      "6000/6000 [==============================] - 1s 188us/sample - loss: 2.7589 - accuracy: 0.9485 - val_loss: 4.2414 - val_accuracy: 0.5464\n",
      "Epoch 30/50\n",
      "6000/6000 [==============================] - 1s 185us/sample - loss: 2.6766 - accuracy: 0.9450 - val_loss: 4.2964 - val_accuracy: 0.5355\n",
      "Epoch 31/50\n",
      "6000/6000 [==============================] - 1s 187us/sample - loss: 2.6601 - accuracy: 0.9512 - val_loss: 4.3150 - val_accuracy: 0.5296\n",
      "Epoch 32/50\n",
      "6000/6000 [==============================] - 1s 190us/sample - loss: 2.6979 - accuracy: 0.9438 - val_loss: 4.4718 - val_accuracy: 0.5185\n",
      "Epoch 33/50\n",
      "6000/6000 [==============================] - 1s 192us/sample - loss: 2.6600 - accuracy: 0.9493 - val_loss: 4.1983 - val_accuracy: 0.5398\n",
      "Epoch 34/50\n",
      "6000/6000 [==============================] - 1s 190us/sample - loss: 2.6663 - accuracy: 0.9423 - val_loss: 4.3375 - val_accuracy: 0.5401\n",
      "Epoch 35/50\n",
      "6000/6000 [==============================] - 1s 195us/sample - loss: 2.7327 - accuracy: 0.9430 - val_loss: 4.6309 - val_accuracy: 0.5076\n",
      "Epoch 36/50\n",
      "6000/6000 [==============================] - 1s 192us/sample - loss: 2.7052 - accuracy: 0.9443 - val_loss: 4.3953 - val_accuracy: 0.5299\n",
      "Epoch 37/50\n",
      "6000/6000 [==============================] - 1s 188us/sample - loss: 2.6010 - accuracy: 0.9557 - val_loss: 4.2027 - val_accuracy: 0.5357\n",
      "Epoch 38/50\n",
      "6000/6000 [==============================] - 1s 196us/sample - loss: 2.5710 - accuracy: 0.9572 - val_loss: 4.1920 - val_accuracy: 0.5318\n",
      "Epoch 39/50\n",
      "6000/6000 [==============================] - 1s 190us/sample - loss: 2.5280 - accuracy: 0.9573 - val_loss: 4.1409 - val_accuracy: 0.5399\n",
      "Epoch 40/50\n",
      "6000/6000 [==============================] - 1s 193us/sample - loss: 2.5171 - accuracy: 0.9513 - val_loss: 4.1963 - val_accuracy: 0.5297\n",
      "Epoch 41/50\n",
      "6000/6000 [==============================] - 1s 187us/sample - loss: 2.6448 - accuracy: 0.9458 - val_loss: 4.4425 - val_accuracy: 0.5166\n",
      "Epoch 42/50\n",
      "6000/6000 [==============================] - 1s 190us/sample - loss: 2.6511 - accuracy: 0.9530 - val_loss: 4.3547 - val_accuracy: 0.5359\n",
      "Epoch 43/50\n",
      "6000/6000 [==============================] - 1s 193us/sample - loss: 2.6532 - accuracy: 0.9475 - val_loss: 4.4066 - val_accuracy: 0.5286\n",
      "Epoch 44/50\n",
      "6000/6000 [==============================] - 1s 193us/sample - loss: 2.6617 - accuracy: 0.9503 - val_loss: 4.4321 - val_accuracy: 0.5374\n",
      "Epoch 45/50\n",
      "6000/6000 [==============================] - 1s 193us/sample - loss: 2.6008 - accuracy: 0.9602 - val_loss: 4.2067 - val_accuracy: 0.5408\n",
      "Epoch 46/50\n",
      "6000/6000 [==============================] - 1s 193us/sample - loss: 2.5509 - accuracy: 0.9503 - val_loss: 4.5996 - val_accuracy: 0.5067\n",
      "Epoch 47/50\n",
      "6000/6000 [==============================] - 1s 191us/sample - loss: 2.6445 - accuracy: 0.9470 - val_loss: 4.4051 - val_accuracy: 0.5366\n",
      "Epoch 48/50\n",
      "6000/6000 [==============================] - 1s 191us/sample - loss: 2.6350 - accuracy: 0.9570 - val_loss: 4.4728 - val_accuracy: 0.5276\n",
      "Epoch 49/50\n",
      "6000/6000 [==============================] - 1s 190us/sample - loss: 2.5244 - accuracy: 0.9648 - val_loss: 4.2721 - val_accuracy: 0.5310\n",
      "Epoch 50/50\n",
      "6000/6000 [==============================] - 1s 193us/sample - loss: 2.4669 - accuracy: 0.9585 - val_loss: 4.3636 - val_accuracy: 0.5131\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 16384)        0           conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 16384)        65536       flatten_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 16384)        0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 200)          3277000     dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 200)          800         dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 200)          0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 200)          0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 200)          40200       dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 200)          800         dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 200)          0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 200)          0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 4)            804         dropout_32[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,614,900\n",
      "Trainable params: 3,578,388\n",
      "Non-trainable params: 36,512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 190000 samples, validate on 10000 samples\n",
      "Epoch 1/80\n",
      "190000/190000 [==============================] - 30s 155us/sample - loss: 2.4501 - accuracy: 0.5750 - val_loss: 1.4018 - val_accuracy: 0.7969\n",
      "Epoch 2/80\n",
      "190000/190000 [==============================] - 27s 144us/sample - loss: 1.7060 - accuracy: 0.6553 - val_loss: 2.0926 - val_accuracy: 0.4306\n",
      "Epoch 3/80\n",
      "190000/190000 [==============================] - 28s 145us/sample - loss: 1.6506 - accuracy: 0.6943 - val_loss: 1.4251 - val_accuracy: 0.7775\n",
      "Epoch 4/80\n",
      "190000/190000 [==============================] - 27s 143us/sample - loss: 1.5791 - accuracy: 0.7235 - val_loss: 1.7270 - val_accuracy: 0.6524\n",
      "Epoch 5/80\n",
      "190000/190000 [==============================] - 27s 144us/sample - loss: 1.5164 - accuracy: 0.7465 - val_loss: 1.5224 - val_accuracy: 0.7409\n",
      "Epoch 6/80\n",
      "190000/190000 [==============================] - 27s 144us/sample - loss: 1.4634 - accuracy: 0.7633 - val_loss: 1.3442 - val_accuracy: 0.8028\n",
      "Epoch 7/80\n",
      "190000/190000 [==============================] - 27s 143us/sample - loss: 1.4043 - accuracy: 0.7790 - val_loss: 1.6987 - val_accuracy: 0.6614\n",
      "Epoch 8/80\n",
      "190000/190000 [==============================] - 28s 145us/sample - loss: 1.3616 - accuracy: 0.7921 - val_loss: 1.2476 - val_accuracy: 0.8376\n",
      "Epoch 9/80\n",
      "190000/190000 [==============================] - 27s 144us/sample - loss: 1.3136 - accuracy: 0.8022 - val_loss: 1.4893 - val_accuracy: 0.7328\n",
      "Epoch 10/80\n",
      "190000/190000 [==============================] - 27s 144us/sample - loss: 1.2731 - accuracy: 0.8115 - val_loss: 1.1775 - val_accuracy: 0.8236\n",
      "Epoch 11/80\n",
      "190000/190000 [==============================] - 28s 145us/sample - loss: 1.2367 - accuracy: 0.8192 - val_loss: 1.0778 - val_accuracy: 0.8585\n",
      "Epoch 12/80\n",
      "190000/190000 [==============================] - 27s 144us/sample - loss: 1.1966 - accuracy: 0.8272 - val_loss: 1.4126 - val_accuracy: 0.7405\n",
      "Epoch 13/80\n",
      "190000/190000 [==============================] - 27s 143us/sample - loss: 1.1679 - accuracy: 0.8338 - val_loss: 1.2330 - val_accuracy: 0.7717\n",
      "Epoch 14/80\n",
      "190000/190000 [==============================] - 28s 145us/sample - loss: 1.1276 - accuracy: 0.8409 - val_loss: 1.1402 - val_accuracy: 0.8155\n",
      "Epoch 15/80\n",
      "190000/190000 [==============================] - 28s 145us/sample - loss: 1.0946 - accuracy: 0.8468 - val_loss: 1.6179 - val_accuracy: 0.6611\n",
      "Epoch 16/80\n",
      "190000/190000 [==============================] - 27s 144us/sample - loss: 1.0645 - accuracy: 0.8518 - val_loss: 1.3268 - val_accuracy: 0.7586\n",
      "Epoch 17/80\n",
      "190000/190000 [==============================] - 27s 144us/sample - loss: 1.0327 - accuracy: 0.8583 - val_loss: 1.2548 - val_accuracy: 0.7510\n",
      "Epoch 18/80\n",
      "190000/190000 [==============================] - 28s 145us/sample - loss: 1.0067 - accuracy: 0.8633 - val_loss: 1.1919 - val_accuracy: 0.7852\n",
      "Epoch 19/80\n",
      "190000/190000 [==============================] - 28s 145us/sample - loss: 0.9814 - accuracy: 0.8676 - val_loss: 1.3013 - val_accuracy: 0.7531\n",
      "Epoch 20/80\n",
      "190000/190000 [==============================] - 28s 146us/sample - loss: 0.9546 - accuracy: 0.8723 - val_loss: 1.1011 - val_accuracy: 0.8152\n",
      "Epoch 21/80\n",
      "190000/190000 [==============================] - 27s 143us/sample - loss: 0.9325 - accuracy: 0.8761 - val_loss: 1.1305 - val_accuracy: 0.8001\n",
      "Epoch 22/80\n",
      "190000/190000 [==============================] - 28s 146us/sample - loss: 0.9115 - accuracy: 0.8804 - val_loss: 1.4528 - val_accuracy: 0.6918\n",
      "Epoch 23/80\n",
      "190000/190000 [==============================] - 27s 145us/sample - loss: 0.8936 - accuracy: 0.8845 - val_loss: 1.3266 - val_accuracy: 0.7447\n",
      "Epoch 24/80\n",
      "190000/190000 [==============================] - 27s 143us/sample - loss: 0.8713 - accuracy: 0.8870 - val_loss: 1.2763 - val_accuracy: 0.7545\n",
      "Epoch 25/80\n",
      "190000/190000 [==============================] - 27s 144us/sample - loss: 0.8580 - accuracy: 0.8896 - val_loss: 1.0651 - val_accuracy: 0.8262\n",
      "Epoch 26/80\n",
      "190000/190000 [==============================] - 27s 143us/sample - loss: 0.8379 - accuracy: 0.8944 - val_loss: 1.1595 - val_accuracy: 0.7759\n",
      "Epoch 27/80\n",
      "190000/190000 [==============================] - 28s 145us/sample - loss: 0.8222 - accuracy: 0.8971 - val_loss: 1.3033 - val_accuracy: 0.7579\n",
      "Epoch 28/80\n",
      "190000/190000 [==============================] - 27s 143us/sample - loss: 0.8074 - accuracy: 0.8991 - val_loss: 1.1814 - val_accuracy: 0.7800\n",
      "Epoch 29/80\n",
      "190000/190000 [==============================] - 27s 144us/sample - loss: 0.7990 - accuracy: 0.9028 - val_loss: 1.1037 - val_accuracy: 0.8046\n",
      "Epoch 30/80\n",
      "190000/190000 [==============================] - 27s 142us/sample - loss: 0.7843 - accuracy: 0.9062 - val_loss: 1.0363 - val_accuracy: 0.8281\n",
      "Epoch 31/80\n",
      "190000/190000 [==============================] - 27s 143us/sample - loss: 0.7664 - accuracy: 0.9086 - val_loss: 1.1477 - val_accuracy: 0.7931\n",
      "Epoch 32/80\n",
      "190000/190000 [==============================] - 27s 143us/sample - loss: 0.7567 - accuracy: 0.9102 - val_loss: 1.2000 - val_accuracy: 0.7900\n",
      "Epoch 33/80\n",
      "190000/190000 [==============================] - 27s 144us/sample - loss: 0.7470 - accuracy: 0.9115 - val_loss: 1.1328 - val_accuracy: 0.8017\n",
      "Epoch 34/80\n",
      "190000/190000 [==============================] - 27s 144us/sample - loss: 0.7354 - accuracy: 0.9146 - val_loss: 1.2319 - val_accuracy: 0.7609\n",
      "Epoch 35/80\n",
      "190000/190000 [==============================] - 28s 145us/sample - loss: 0.7275 - accuracy: 0.9163 - val_loss: 0.8693 - val_accuracy: 0.8833\n",
      "Epoch 36/80\n",
      "190000/190000 [==============================] - 27s 143us/sample - loss: 0.7153 - accuracy: 0.9193 - val_loss: 1.3225 - val_accuracy: 0.7589\n",
      "Epoch 37/80\n",
      "190000/190000 [==============================] - 27s 143us/sample - loss: 0.7088 - accuracy: 0.9203 - val_loss: 1.3487 - val_accuracy: 0.7371\n",
      "Epoch 38/80\n",
      "190000/190000 [==============================] - 27s 142us/sample - loss: 0.6980 - accuracy: 0.9226 - val_loss: 1.2565 - val_accuracy: 0.7777\n",
      "Epoch 39/80\n",
      "190000/190000 [==============================] - 28s 145us/sample - loss: 0.6893 - accuracy: 0.9233 - val_loss: 1.2543 - val_accuracy: 0.7651\n",
      "Epoch 40/80\n",
      "190000/190000 [==============================] - 28s 145us/sample - loss: 0.6824 - accuracy: 0.9258 - val_loss: 1.1454 - val_accuracy: 0.7872\n",
      "Epoch 41/80\n",
      "190000/190000 [==============================] - 27s 143us/sample - loss: 0.6741 - accuracy: 0.9274 - val_loss: 1.0150 - val_accuracy: 0.8299\n",
      "Epoch 42/80\n",
      "190000/190000 [==============================] - 27s 144us/sample - loss: 0.6662 - accuracy: 0.9287 - val_loss: 1.3801 - val_accuracy: 0.7385\n",
      "Epoch 43/80\n",
      "190000/190000 [==============================] - 28s 146us/sample - loss: 0.6592 - accuracy: 0.9305 - val_loss: 1.1067 - val_accuracy: 0.8178\n",
      "Epoch 44/80\n",
      "190000/190000 [==============================] - 27s 145us/sample - loss: 0.6520 - accuracy: 0.9310 - val_loss: 1.2406 - val_accuracy: 0.7792\n",
      "Epoch 45/80\n",
      "190000/190000 [==============================] - 27s 144us/sample - loss: 0.6485 - accuracy: 0.9316 - val_loss: 1.1775 - val_accuracy: 0.7877\n",
      "Epoch 46/80\n",
      "190000/190000 [==============================] - 27s 142us/sample - loss: 0.6401 - accuracy: 0.9339 - val_loss: 1.1846 - val_accuracy: 0.7858\n",
      "Epoch 47/80\n",
      "190000/190000 [==============================] - 27s 144us/sample - loss: 0.6328 - accuracy: 0.9357 - val_loss: 1.7883 - val_accuracy: 0.6803\n",
      "Epoch 48/80\n",
      "190000/190000 [==============================] - 27s 142us/sample - loss: 0.6281 - accuracy: 0.9363 - val_loss: 1.3713 - val_accuracy: 0.7313\n",
      "Epoch 49/80\n",
      "190000/190000 [==============================] - 27s 143us/sample - loss: 0.6272 - accuracy: 0.9370 - val_loss: 1.4001 - val_accuracy: 0.7439\n",
      "Epoch 50/80\n",
      "190000/190000 [==============================] - 27s 142us/sample - loss: 0.6179 - accuracy: 0.9387 - val_loss: 1.5796 - val_accuracy: 0.7101\n",
      "Epoch 51/80\n",
      "190000/190000 [==============================] - 27s 144us/sample - loss: 0.6130 - accuracy: 0.9399 - val_loss: 1.5330 - val_accuracy: 0.7229\n",
      "Epoch 52/80\n",
      "190000/190000 [==============================] - 27s 143us/sample - loss: 0.6089 - accuracy: 0.9409 - val_loss: 1.2911 - val_accuracy: 0.7807\n",
      "Epoch 53/80\n",
      "190000/190000 [==============================] - 27s 144us/sample - loss: 0.6032 - accuracy: 0.9416 - val_loss: 1.1431 - val_accuracy: 0.8049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/80\n",
      "190000/190000 [==============================] - 27s 143us/sample - loss: 0.5985 - accuracy: 0.9420 - val_loss: 1.2172 - val_accuracy: 0.7937\n",
      "Epoch 55/80\n",
      "190000/190000 [==============================] - 27s 143us/sample - loss: 0.5973 - accuracy: 0.9432 - val_loss: 1.1646 - val_accuracy: 0.8155\n",
      "Epoch 56/80\n",
      "190000/190000 [==============================] - 27s 144us/sample - loss: 0.5920 - accuracy: 0.9435 - val_loss: 1.3695 - val_accuracy: 0.7614\n",
      "Epoch 57/80\n",
      "190000/190000 [==============================] - 27s 144us/sample - loss: 0.5870 - accuracy: 0.9455 - val_loss: 1.1929 - val_accuracy: 0.7959\n",
      "Epoch 58/80\n",
      "190000/190000 [==============================] - 27s 144us/sample - loss: 0.5835 - accuracy: 0.9460 - val_loss: 1.2904 - val_accuracy: 0.7721\n",
      "Epoch 59/80\n",
      "190000/190000 [==============================] - 27s 144us/sample - loss: 0.5815 - accuracy: 0.9461 - val_loss: 1.4613 - val_accuracy: 0.7584\n",
      "Epoch 60/80\n",
      "190000/190000 [==============================] - 27s 143us/sample - loss: 0.5722 - accuracy: 0.9472 - val_loss: 1.4157 - val_accuracy: 0.7506\n",
      "Epoch 61/80\n",
      "190000/190000 [==============================] - 27s 144us/sample - loss: 0.5727 - accuracy: 0.9475 - val_loss: 1.2737 - val_accuracy: 0.7804\n",
      "Epoch 62/80\n",
      "190000/190000 [==============================] - 27s 142us/sample - loss: 0.5674 - accuracy: 0.9489 - val_loss: 1.1040 - val_accuracy: 0.8294\n",
      "Epoch 63/80\n",
      "190000/190000 [==============================] - 27s 145us/sample - loss: 0.5625 - accuracy: 0.9492 - val_loss: 1.2778 - val_accuracy: 0.7775\n",
      "Epoch 64/80\n",
      "190000/190000 [==============================] - 27s 144us/sample - loss: 0.5636 - accuracy: 0.9492 - val_loss: 1.4964 - val_accuracy: 0.7239\n",
      "Epoch 65/80\n",
      "190000/190000 [==============================] - 27s 143us/sample - loss: 0.5609 - accuracy: 0.9511 - val_loss: 0.9151 - val_accuracy: 0.8598\n",
      "Epoch 66/80\n",
      "190000/190000 [==============================] - 27s 144us/sample - loss: 0.5506 - accuracy: 0.9522 - val_loss: 1.1554 - val_accuracy: 0.8122\n",
      "Epoch 67/80\n",
      "190000/190000 [==============================] - 28s 145us/sample - loss: 0.5539 - accuracy: 0.9516 - val_loss: 1.1279 - val_accuracy: 0.8116\n",
      "Epoch 68/80\n",
      "190000/190000 [==============================] - 28s 145us/sample - loss: 0.5490 - accuracy: 0.9533 - val_loss: 1.0510 - val_accuracy: 0.8293\n",
      "Epoch 69/80\n",
      "190000/190000 [==============================] - 28s 145us/sample - loss: 0.5468 - accuracy: 0.9526 - val_loss: 1.0107 - val_accuracy: 0.8496\n",
      "Epoch 70/80\n",
      "190000/190000 [==============================] - 27s 145us/sample - loss: 0.5443 - accuracy: 0.9540 - val_loss: 1.6856 - val_accuracy: 0.6919\n",
      "Epoch 71/80\n",
      "190000/190000 [==============================] - 27s 145us/sample - loss: 0.5430 - accuracy: 0.9546 - val_loss: 1.2919 - val_accuracy: 0.7891\n",
      "Epoch 72/80\n",
      "190000/190000 [==============================] - 27s 142us/sample - loss: 0.5396 - accuracy: 0.9548 - val_loss: 1.4707 - val_accuracy: 0.7389\n",
      "Epoch 73/80\n",
      "190000/190000 [==============================] - 27s 142us/sample - loss: 0.5392 - accuracy: 0.9555 - val_loss: 1.3081 - val_accuracy: 0.7871\n",
      "Epoch 74/80\n",
      "190000/190000 [==============================] - 27s 142us/sample - loss: 0.5305 - accuracy: 0.9562 - val_loss: 1.7317 - val_accuracy: 0.6836\n",
      "Epoch 75/80\n",
      "190000/190000 [==============================] - 27s 142us/sample - loss: 0.5287 - accuracy: 0.9564 - val_loss: 1.1732 - val_accuracy: 0.8139\n",
      "Epoch 76/80\n",
      "190000/190000 [==============================] - 27s 143us/sample - loss: 0.5268 - accuracy: 0.9576 - val_loss: 1.3777 - val_accuracy: 0.7598\n",
      "Epoch 77/80\n",
      "190000/190000 [==============================] - 27s 142us/sample - loss: 0.5283 - accuracy: 0.9570 - val_loss: 1.6426 - val_accuracy: 0.7213\n",
      "Epoch 78/80\n",
      "190000/190000 [==============================] - 27s 143us/sample - loss: 0.5196 - accuracy: 0.9584 - val_loss: 1.5229 - val_accuracy: 0.7472\n",
      "Epoch 79/80\n",
      "190000/190000 [==============================] - 27s 143us/sample - loss: 0.5181 - accuracy: 0.9589 - val_loss: 1.4502 - val_accuracy: 0.7599\n",
      "Epoch 80/80\n",
      "190000/190000 [==============================] - 27s 143us/sample - loss: 0.5172 - accuracy: 0.9589 - val_loss: 1.3272 - val_accuracy: 0.7752\n",
      "INFO:tensorflow:Assets written to: selfsupervised_5/assets\n",
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 16384)        0           conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 16384)        65536       flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 16384)        0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 200)          3277000     dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 200)          800         dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 200)          0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 200)          0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 200)          40200       dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 200)          800         dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 200)          0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 200)          0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 10)           2010        dropout_35[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,473,002\n",
      "Trainable params: 3,438,026\n",
      "Non-trainable params: 34,976\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 3s 308us/sample - loss: 5.8603 - accuracy: 0.3465 - val_loss: 7.8085 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 4.6878 - accuracy: 0.5810 - val_loss: 6.7138 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 2s 158us/sample - loss: 3.9752 - accuracy: 0.6951 - val_loss: 6.1530 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 3.7057 - accuracy: 0.7538 - val_loss: 5.7441 - val_accuracy: 0.0999\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 3.5567 - accuracy: 0.7966 - val_loss: 5.8700 - val_accuracy: 0.1013\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 3.4920 - accuracy: 0.8165 - val_loss: 5.9660 - val_accuracy: 0.1061\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 3.4611 - accuracy: 0.8319 - val_loss: 6.0034 - val_accuracy: 0.1515\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 3.5017 - accuracy: 0.8364 - val_loss: 6.7664 - val_accuracy: 0.1184\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 3.5655 - accuracy: 0.8451 - val_loss: 5.7893 - val_accuracy: 0.1795\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 3.4268 - accuracy: 0.8638 - val_loss: 5.0847 - val_accuracy: 0.3317\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 3.3098 - accuracy: 0.8764 - val_loss: 4.6848 - val_accuracy: 0.4479\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 3.2349 - accuracy: 0.8857 - val_loss: 4.5013 - val_accuracy: 0.5035\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 3.1908 - accuracy: 0.8864 - val_loss: 4.6172 - val_accuracy: 0.5022\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 3.1954 - accuracy: 0.8896 - val_loss: 4.5316 - val_accuracy: 0.5196\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 2s 158us/sample - loss: 3.1954 - accuracy: 0.8925 - val_loss: 4.2258 - val_accuracy: 0.5806\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 3.2712 - accuracy: 0.8849 - val_loss: 4.4286 - val_accuracy: 0.5763\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 3.3402 - accuracy: 0.8928 - val_loss: 4.4125 - val_accuracy: 0.5732\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 3.4516 - accuracy: 0.8849 - val_loss: 4.7267 - val_accuracy: 0.5440\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 2s 158us/sample - loss: 3.1513 - accuracy: 0.9179 - val_loss: 4.2861 - val_accuracy: 0.5623\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 3.2776 - accuracy: 0.8991 - val_loss: 4.4958 - val_accuracy: 0.5611\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 3.2196 - accuracy: 0.9101 - val_loss: 4.3789 - val_accuracy: 0.5800\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 3.1957 - accuracy: 0.9082 - val_loss: 4.4113 - val_accuracy: 0.5754\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 2s 158us/sample - loss: 3.1537 - accuracy: 0.9139 - val_loss: 4.2997 - val_accuracy: 0.5924\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 3.2820 - accuracy: 0.9008 - val_loss: 4.4894 - val_accuracy: 0.5785\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 3.2612 - accuracy: 0.9100 - val_loss: 4.4233 - val_accuracy: 0.5771\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 2s 159us/sample - loss: 3.0247 - accuracy: 0.9263 - val_loss: 4.2445 - val_accuracy: 0.5739\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 2.9739 - accuracy: 0.9244 - val_loss: 4.4120 - val_accuracy: 0.5563\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 3.0588 - accuracy: 0.9166 - val_loss: 4.3228 - val_accuracy: 0.5779\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 3.1166 - accuracy: 0.9146 - val_loss: 4.4257 - val_accuracy: 0.5729\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 3.0091 - accuracy: 0.9264 - val_loss: 4.1900 - val_accuracy: 0.5898\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 2.9866 - accuracy: 0.9184 - val_loss: 4.5285 - val_accuracy: 0.5452\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 3.2933 - accuracy: 0.9060 - val_loss: 4.5364 - val_accuracy: 0.5833\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 3.1333 - accuracy: 0.9255 - val_loss: 4.3722 - val_accuracy: 0.5829\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 3.0560 - accuracy: 0.9287 - val_loss: 4.3266 - val_accuracy: 0.5839\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 2.9697 - accuracy: 0.9293 - val_loss: 4.4685 - val_accuracy: 0.5535\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 2.9605 - accuracy: 0.9291 - val_loss: 4.4393 - val_accuracy: 0.5607\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 3.0089 - accuracy: 0.9278 - val_loss: 4.3838 - val_accuracy: 0.5833\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 3.0414 - accuracy: 0.9199 - val_loss: 4.6000 - val_accuracy: 0.5717\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 3.0270 - accuracy: 0.9281 - val_loss: 4.4030 - val_accuracy: 0.5745\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 3.0909 - accuracy: 0.9214 - val_loss: 4.5390 - val_accuracy: 0.5840\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 3.0734 - accuracy: 0.9260 - val_loss: 4.4229 - val_accuracy: 0.5930\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 3.0832 - accuracy: 0.9299 - val_loss: 4.4497 - val_accuracy: 0.5783\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 3.2000 - accuracy: 0.9224 - val_loss: 4.5331 - val_accuracy: 0.5931\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 3.2472 - accuracy: 0.9255 - val_loss: 4.9349 - val_accuracy: 0.5357\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 3.1092 - accuracy: 0.9318 - val_loss: 4.5029 - val_accuracy: 0.5603\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 2.9378 - accuracy: 0.9376 - val_loss: 4.4076 - val_accuracy: 0.5607\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 2.8510 - accuracy: 0.9385 - val_loss: 4.2822 - val_accuracy: 0.5713\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 2.9141 - accuracy: 0.9337 - val_loss: 4.2975 - val_accuracy: 0.5856\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 2s 160us/sample - loss: 2.9094 - accuracy: 0.9365 - val_loss: 4.3869 - val_accuracy: 0.5763\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 2.9972 - accuracy: 0.9287 - val_loss: 4.3923 - val_accuracy: 0.5948\n",
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 16384)        0           conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 16384)        65536       flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 16384)        0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 200)          3277000     dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 200)          800         dense_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 200)          0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 200)          0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 200)          40200       dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 200)          800         dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 200)          0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 200)          0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 4)            804         dropout_38[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,614,900\n",
      "Trainable params: 3,578,388\n",
      "Non-trainable params: 36,512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180000 samples, validate on 10000 samples\n",
      "Epoch 1/80\n",
      "180000/180000 [==============================] - 27s 151us/sample - loss: 2.5245 - accuracy: 0.5688 - val_loss: 1.7705 - val_accuracy: 0.6704\n",
      "Epoch 2/80\n",
      "180000/180000 [==============================] - 25s 142us/sample - loss: 1.7311 - accuracy: 0.6518 - val_loss: 1.3228 - val_accuracy: 0.8033\n",
      "Epoch 3/80\n",
      "180000/180000 [==============================] - 25s 142us/sample - loss: 1.6713 - accuracy: 0.6923 - val_loss: 1.8674 - val_accuracy: 0.5665\n",
      "Epoch 4/80\n",
      "180000/180000 [==============================] - 25s 142us/sample - loss: 1.6083 - accuracy: 0.7220 - val_loss: 1.9728 - val_accuracy: 0.5537\n",
      "Epoch 5/80\n",
      "180000/180000 [==============================] - 26s 142us/sample - loss: 1.5422 - accuracy: 0.7438 - val_loss: 1.6445 - val_accuracy: 0.7022\n",
      "Epoch 6/80\n",
      "180000/180000 [==============================] - 26s 143us/sample - loss: 1.4847 - accuracy: 0.7612 - val_loss: 1.3528 - val_accuracy: 0.8029\n",
      "Epoch 7/80\n",
      "180000/180000 [==============================] - 26s 143us/sample - loss: 1.4368 - accuracy: 0.7757 - val_loss: 1.6349 - val_accuracy: 0.6813\n",
      "Epoch 8/80\n",
      "180000/180000 [==============================] - 26s 144us/sample - loss: 1.3886 - accuracy: 0.7884 - val_loss: 1.5126 - val_accuracy: 0.7243\n",
      "Epoch 9/80\n",
      "180000/180000 [==============================] - 25s 140us/sample - loss: 1.3484 - accuracy: 0.8008 - val_loss: 1.0625 - val_accuracy: 0.9080\n",
      "Epoch 10/80\n",
      "180000/180000 [==============================] - 26s 143us/sample - loss: 1.3082 - accuracy: 0.8092 - val_loss: 1.3181 - val_accuracy: 0.7884\n",
      "Epoch 11/80\n",
      "180000/180000 [==============================] - 26s 142us/sample - loss: 1.2634 - accuracy: 0.8176 - val_loss: 1.1752 - val_accuracy: 0.8581\n",
      "Epoch 12/80\n",
      "180000/180000 [==============================] - 25s 141us/sample - loss: 1.2316 - accuracy: 0.8257 - val_loss: 1.6075 - val_accuracy: 0.6905\n",
      "Epoch 13/80\n",
      "180000/180000 [==============================] - 26s 143us/sample - loss: 1.1900 - accuracy: 0.8336 - val_loss: 1.2201 - val_accuracy: 0.8146\n",
      "Epoch 14/80\n",
      "180000/180000 [==============================] - 26s 144us/sample - loss: 1.1580 - accuracy: 0.8398 - val_loss: 1.0472 - val_accuracy: 0.8647\n",
      "Epoch 15/80\n",
      "180000/180000 [==============================] - 25s 142us/sample - loss: 1.1285 - accuracy: 0.8465 - val_loss: 1.2145 - val_accuracy: 0.8196\n",
      "Epoch 16/80\n",
      "180000/180000 [==============================] - 26s 143us/sample - loss: 1.0990 - accuracy: 0.8519 - val_loss: 1.3223 - val_accuracy: 0.7484\n",
      "Epoch 17/80\n",
      "180000/180000 [==============================] - 26s 142us/sample - loss: 1.0672 - accuracy: 0.8574 - val_loss: 1.2120 - val_accuracy: 0.7870\n",
      "Epoch 18/80\n",
      "180000/180000 [==============================] - 26s 142us/sample - loss: 1.0390 - accuracy: 0.8624 - val_loss: 1.4979 - val_accuracy: 0.7197\n",
      "Epoch 19/80\n",
      "180000/180000 [==============================] - 26s 144us/sample - loss: 1.0101 - accuracy: 0.8680 - val_loss: 0.9294 - val_accuracy: 0.8883\n",
      "Epoch 20/80\n",
      "180000/180000 [==============================] - 26s 142us/sample - loss: 0.9886 - accuracy: 0.8723 - val_loss: 1.0703 - val_accuracy: 0.8374\n",
      "Epoch 21/80\n",
      "180000/180000 [==============================] - 26s 142us/sample - loss: 0.9637 - accuracy: 0.8763 - val_loss: 2.4649 - val_accuracy: 0.4657\n",
      "Epoch 22/80\n",
      "180000/180000 [==============================] - 25s 141us/sample - loss: 0.9437 - accuracy: 0.8794 - val_loss: 1.0229 - val_accuracy: 0.8494\n",
      "Epoch 23/80\n",
      "180000/180000 [==============================] - 26s 142us/sample - loss: 0.9215 - accuracy: 0.8849 - val_loss: 1.2052 - val_accuracy: 0.7886\n",
      "Epoch 24/80\n",
      "180000/180000 [==============================] - 26s 143us/sample - loss: 0.8986 - accuracy: 0.8882 - val_loss: 1.0933 - val_accuracy: 0.8132\n",
      "Epoch 25/80\n",
      "180000/180000 [==============================] - 26s 145us/sample - loss: 0.8799 - accuracy: 0.8918 - val_loss: 1.1375 - val_accuracy: 0.8023\n",
      "Epoch 26/80\n",
      "180000/180000 [==============================] - 26s 145us/sample - loss: 0.8629 - accuracy: 0.8944 - val_loss: 1.2156 - val_accuracy: 0.7880\n",
      "Epoch 27/80\n",
      "180000/180000 [==============================] - 26s 142us/sample - loss: 0.8416 - accuracy: 0.8983 - val_loss: 1.0846 - val_accuracy: 0.8080\n",
      "Epoch 28/80\n",
      "180000/180000 [==============================] - 25s 141us/sample - loss: 0.8307 - accuracy: 0.8992 - val_loss: 1.5205 - val_accuracy: 0.6855\n",
      "Epoch 29/80\n",
      "180000/180000 [==============================] - 26s 142us/sample - loss: 0.8147 - accuracy: 0.9019 - val_loss: 1.4117 - val_accuracy: 0.7114\n",
      "Epoch 30/80\n",
      "180000/180000 [==============================] - 26s 144us/sample - loss: 0.7963 - accuracy: 0.9056 - val_loss: 1.0633 - val_accuracy: 0.8226\n",
      "Epoch 31/80\n",
      "180000/180000 [==============================] - 25s 142us/sample - loss: 0.7858 - accuracy: 0.9095 - val_loss: 1.2256 - val_accuracy: 0.7931\n",
      "Epoch 32/80\n",
      "180000/180000 [==============================] - 26s 143us/sample - loss: 0.7733 - accuracy: 0.9097 - val_loss: 1.0700 - val_accuracy: 0.8112\n",
      "Epoch 33/80\n",
      "180000/180000 [==============================] - 25s 142us/sample - loss: 0.7605 - accuracy: 0.9136 - val_loss: 1.1117 - val_accuracy: 0.7963\n",
      "Epoch 34/80\n",
      "180000/180000 [==============================] - 25s 142us/sample - loss: 0.7522 - accuracy: 0.9150 - val_loss: 1.3246 - val_accuracy: 0.7474\n",
      "Epoch 35/80\n",
      "180000/180000 [==============================] - 25s 141us/sample - loss: 0.7405 - accuracy: 0.9169 - val_loss: 0.8968 - val_accuracy: 0.8756\n",
      "Epoch 36/80\n",
      "180000/180000 [==============================] - 26s 142us/sample - loss: 0.7304 - accuracy: 0.9197 - val_loss: 1.0385 - val_accuracy: 0.8237\n",
      "Epoch 37/80\n",
      "180000/180000 [==============================] - 26s 143us/sample - loss: 0.7239 - accuracy: 0.9202 - val_loss: 1.0555 - val_accuracy: 0.8179\n",
      "Epoch 38/80\n",
      "180000/180000 [==============================] - 26s 142us/sample - loss: 0.7112 - accuracy: 0.9221 - val_loss: 1.1912 - val_accuracy: 0.8020\n",
      "Epoch 39/80\n",
      "180000/180000 [==============================] - 25s 141us/sample - loss: 0.7028 - accuracy: 0.9248 - val_loss: 1.1176 - val_accuracy: 0.8015\n",
      "Epoch 40/80\n",
      "180000/180000 [==============================] - 25s 142us/sample - loss: 0.6926 - accuracy: 0.9258 - val_loss: 1.1364 - val_accuracy: 0.8103\n",
      "Epoch 41/80\n",
      "180000/180000 [==============================] - 26s 142us/sample - loss: 0.6864 - accuracy: 0.9284 - val_loss: 1.4675 - val_accuracy: 0.7008\n",
      "Epoch 42/80\n",
      "180000/180000 [==============================] - 26s 142us/sample - loss: 0.6767 - accuracy: 0.9303 - val_loss: 1.1478 - val_accuracy: 0.7933\n",
      "Epoch 43/80\n",
      "180000/180000 [==============================] - 25s 141us/sample - loss: 0.6726 - accuracy: 0.9311 - val_loss: 1.4746 - val_accuracy: 0.7196\n",
      "Epoch 44/80\n",
      "180000/180000 [==============================] - 26s 142us/sample - loss: 0.6685 - accuracy: 0.9323 - val_loss: 1.2142 - val_accuracy: 0.7992\n",
      "Epoch 45/80\n",
      "180000/180000 [==============================] - 25s 141us/sample - loss: 0.6611 - accuracy: 0.9336 - val_loss: 1.4201 - val_accuracy: 0.7452\n",
      "Epoch 46/80\n",
      "180000/180000 [==============================] - 26s 144us/sample - loss: 0.6496 - accuracy: 0.9360 - val_loss: 1.1851 - val_accuracy: 0.7937\n",
      "Epoch 47/80\n",
      "180000/180000 [==============================] - 26s 142us/sample - loss: 0.6428 - accuracy: 0.9367 - val_loss: 1.2149 - val_accuracy: 0.7880\n",
      "Epoch 48/80\n",
      "180000/180000 [==============================] - 26s 142us/sample - loss: 0.6430 - accuracy: 0.9374 - val_loss: 1.1919 - val_accuracy: 0.8093\n",
      "Epoch 49/80\n",
      "180000/180000 [==============================] - 25s 142us/sample - loss: 0.6363 - accuracy: 0.9377 - val_loss: 1.3296 - val_accuracy: 0.7653\n",
      "Epoch 50/80\n",
      "180000/180000 [==============================] - 26s 142us/sample - loss: 0.6279 - accuracy: 0.9402 - val_loss: 1.3451 - val_accuracy: 0.7677\n",
      "Epoch 51/80\n",
      "180000/180000 [==============================] - 25s 141us/sample - loss: 0.6224 - accuracy: 0.9404 - val_loss: 1.0499 - val_accuracy: 0.8292\n",
      "Epoch 52/80\n",
      "180000/180000 [==============================] - 25s 140us/sample - loss: 0.6183 - accuracy: 0.9411 - val_loss: 1.0797 - val_accuracy: 0.8161\n",
      "Epoch 53/80\n",
      "180000/180000 [==============================] - 25s 142us/sample - loss: 0.6101 - accuracy: 0.9425 - val_loss: 1.1286 - val_accuracy: 0.8208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/80\n",
      "180000/180000 [==============================] - 25s 141us/sample - loss: 0.6084 - accuracy: 0.9435 - val_loss: 1.1345 - val_accuracy: 0.8059\n",
      "Epoch 55/80\n",
      "180000/180000 [==============================] - 26s 142us/sample - loss: 0.6034 - accuracy: 0.9446 - val_loss: 1.1710 - val_accuracy: 0.7958\n",
      "Epoch 56/80\n",
      "180000/180000 [==============================] - 26s 142us/sample - loss: 0.6016 - accuracy: 0.9440 - val_loss: 1.3023 - val_accuracy: 0.7661\n",
      "Epoch 57/80\n",
      "180000/180000 [==============================] - 25s 141us/sample - loss: 0.5905 - accuracy: 0.9459 - val_loss: 1.4095 - val_accuracy: 0.7609\n",
      "Epoch 58/80\n",
      "180000/180000 [==============================] - 25s 141us/sample - loss: 0.5890 - accuracy: 0.9462 - val_loss: 1.3839 - val_accuracy: 0.7506\n",
      "Epoch 59/80\n",
      "180000/180000 [==============================] - 26s 143us/sample - loss: 0.5846 - accuracy: 0.9477 - val_loss: 1.2929 - val_accuracy: 0.7649\n",
      "Epoch 60/80\n",
      "180000/180000 [==============================] - 26s 143us/sample - loss: 0.5782 - accuracy: 0.9493 - val_loss: 1.1680 - val_accuracy: 0.8039\n",
      "Epoch 61/80\n",
      "180000/180000 [==============================] - 25s 141us/sample - loss: 0.5742 - accuracy: 0.9491 - val_loss: 1.3908 - val_accuracy: 0.7397\n",
      "Epoch 62/80\n",
      "180000/180000 [==============================] - 26s 142us/sample - loss: 0.5754 - accuracy: 0.9492 - val_loss: 0.9852 - val_accuracy: 0.8366\n",
      "Epoch 63/80\n",
      "180000/180000 [==============================] - 26s 143us/sample - loss: 0.5703 - accuracy: 0.9503 - val_loss: 1.2297 - val_accuracy: 0.7869\n",
      "Epoch 64/80\n",
      "180000/180000 [==============================] - 26s 142us/sample - loss: 0.5649 - accuracy: 0.9510 - val_loss: 1.0127 - val_accuracy: 0.8510\n",
      "Epoch 65/80\n",
      "180000/180000 [==============================] - 25s 141us/sample - loss: 0.5598 - accuracy: 0.9519 - val_loss: 1.3339 - val_accuracy: 0.7792\n",
      "Epoch 66/80\n",
      "180000/180000 [==============================] - 25s 141us/sample - loss: 0.5594 - accuracy: 0.9520 - val_loss: 1.1179 - val_accuracy: 0.8103\n",
      "Epoch 67/80\n",
      "180000/180000 [==============================] - 26s 142us/sample - loss: 0.5552 - accuracy: 0.9526 - val_loss: 1.2524 - val_accuracy: 0.7852\n",
      "Epoch 68/80\n",
      "180000/180000 [==============================] - 25s 142us/sample - loss: 0.5520 - accuracy: 0.9543 - val_loss: 1.2507 - val_accuracy: 0.7935\n",
      "Epoch 69/80\n",
      "180000/180000 [==============================] - 25s 140us/sample - loss: 0.5467 - accuracy: 0.9542 - val_loss: 1.2139 - val_accuracy: 0.7925\n",
      "Epoch 70/80\n",
      "180000/180000 [==============================] - 26s 144us/sample - loss: 0.5440 - accuracy: 0.9554 - val_loss: 1.4421 - val_accuracy: 0.7416\n",
      "Epoch 71/80\n",
      "180000/180000 [==============================] - 26s 142us/sample - loss: 0.5441 - accuracy: 0.9553 - val_loss: 1.6344 - val_accuracy: 0.7398\n",
      "Epoch 72/80\n",
      "180000/180000 [==============================] - 26s 144us/sample - loss: 0.5406 - accuracy: 0.9559 - val_loss: 1.3349 - val_accuracy: 0.7765\n",
      "Epoch 73/80\n",
      "180000/180000 [==============================] - 25s 141us/sample - loss: 0.5374 - accuracy: 0.9569 - val_loss: 1.2893 - val_accuracy: 0.7838\n",
      "Epoch 74/80\n",
      "180000/180000 [==============================] - 25s 141us/sample - loss: 0.5346 - accuracy: 0.9573 - val_loss: 0.9422 - val_accuracy: 0.8603\n",
      "Epoch 75/80\n",
      "180000/180000 [==============================] - 26s 143us/sample - loss: 0.5341 - accuracy: 0.9577 - val_loss: 1.2220 - val_accuracy: 0.8065\n",
      "Epoch 76/80\n",
      "180000/180000 [==============================] - 26s 144us/sample - loss: 0.5272 - accuracy: 0.9579 - val_loss: 1.1190 - val_accuracy: 0.8329\n",
      "Epoch 77/80\n",
      "180000/180000 [==============================] - 25s 142us/sample - loss: 0.5303 - accuracy: 0.9584 - val_loss: 1.1354 - val_accuracy: 0.8290\n",
      "Epoch 78/80\n",
      "180000/180000 [==============================] - 26s 142us/sample - loss: 0.5258 - accuracy: 0.9586 - val_loss: 1.0108 - val_accuracy: 0.8389\n",
      "Epoch 79/80\n",
      "180000/180000 [==============================] - 25s 141us/sample - loss: 0.5223 - accuracy: 0.9590 - val_loss: 1.2274 - val_accuracy: 0.7970\n",
      "Epoch 80/80\n",
      "180000/180000 [==============================] - 26s 143us/sample - loss: 0.5237 - accuracy: 0.9595 - val_loss: 1.3819 - val_accuracy: 0.7607\n",
      "INFO:tensorflow:Assets written to: selfsupervised_10/assets\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 16384)        0           conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 16384)        65536       flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 16384)        0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 200)          3277000     dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 200)          800         dense_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 200)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 200)          0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 200)          40200       dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 200)          800         dense_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 200)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 200)          0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 10)           2010        dropout_41[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,473,002\n",
      "Trainable params: 3,438,026\n",
      "Non-trainable params: 34,976\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "20000/20000 [==============================] - 4s 211us/sample - loss: 5.4271 - accuracy: 0.4038 - val_loss: 5.9755 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "20000/20000 [==============================] - 3s 132us/sample - loss: 4.1190 - accuracy: 0.6008 - val_loss: 5.5904 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "20000/20000 [==============================] - 3s 128us/sample - loss: 3.7823 - accuracy: 0.6697 - val_loss: 6.4918 - val_accuracy: 0.1060\n",
      "Epoch 4/50\n",
      "20000/20000 [==============================] - 3s 128us/sample - loss: 3.6673 - accuracy: 0.7161 - val_loss: 5.4729 - val_accuracy: 0.2256\n",
      "Epoch 5/50\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 3.6273 - accuracy: 0.7487 - val_loss: 5.0482 - val_accuracy: 0.3705\n",
      "Epoch 6/50\n",
      "20000/20000 [==============================] - 3s 128us/sample - loss: 3.5693 - accuracy: 0.7734 - val_loss: 4.2887 - val_accuracy: 0.5383\n",
      "Epoch 7/50\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 3.5533 - accuracy: 0.7898 - val_loss: 4.2837 - val_accuracy: 0.5641\n",
      "Epoch 8/50\n",
      "20000/20000 [==============================] - 3s 128us/sample - loss: 3.5045 - accuracy: 0.8012 - val_loss: 4.1348 - val_accuracy: 0.6157\n",
      "Epoch 9/50\n",
      "20000/20000 [==============================] - 3s 129us/sample - loss: 3.4456 - accuracy: 0.8135 - val_loss: 4.1609 - val_accuracy: 0.6086\n",
      "Epoch 10/50\n",
      "20000/20000 [==============================] - 3s 133us/sample - loss: 3.4546 - accuracy: 0.8204 - val_loss: 4.1303 - val_accuracy: 0.6273\n",
      "Epoch 11/50\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 3.4297 - accuracy: 0.8321 - val_loss: 4.1259 - val_accuracy: 0.6216\n",
      "Epoch 12/50\n",
      "20000/20000 [==============================] - 3s 131us/sample - loss: 3.4326 - accuracy: 0.8352 - val_loss: 4.0718 - val_accuracy: 0.6416\n",
      "Epoch 13/50\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 3.4066 - accuracy: 0.8442 - val_loss: 4.2435 - val_accuracy: 0.6094\n",
      "Epoch 14/50\n",
      "20000/20000 [==============================] - 3s 133us/sample - loss: 3.4073 - accuracy: 0.8486 - val_loss: 4.2328 - val_accuracy: 0.6209\n",
      "Epoch 15/50\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 3.3548 - accuracy: 0.8544 - val_loss: 4.2703 - val_accuracy: 0.6197\n",
      "Epoch 16/50\n",
      "20000/20000 [==============================] - 3s 129us/sample - loss: 3.4151 - accuracy: 0.8556 - val_loss: 4.3410 - val_accuracy: 0.6031\n",
      "Epoch 17/50\n",
      "20000/20000 [==============================] - 3s 131us/sample - loss: 3.3392 - accuracy: 0.8636 - val_loss: 4.2582 - val_accuracy: 0.6132\n",
      "Epoch 18/50\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 3.3692 - accuracy: 0.8627 - val_loss: 4.3346 - val_accuracy: 0.6026\n",
      "Epoch 19/50\n",
      "20000/20000 [==============================] - 3s 128us/sample - loss: 3.3546 - accuracy: 0.8701 - val_loss: 4.2684 - val_accuracy: 0.6211\n",
      "Epoch 20/50\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 3.3628 - accuracy: 0.8663 - val_loss: 4.3189 - val_accuracy: 0.6225\n",
      "Epoch 21/50\n",
      "20000/20000 [==============================] - 3s 128us/sample - loss: 3.3450 - accuracy: 0.8724 - val_loss: 4.1855 - val_accuracy: 0.6274\n",
      "Epoch 22/50\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 3.3247 - accuracy: 0.8755 - val_loss: 4.3093 - val_accuracy: 0.6380\n",
      "Epoch 23/50\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 3.3474 - accuracy: 0.8769 - val_loss: 4.3388 - val_accuracy: 0.6259\n",
      "Epoch 24/50\n",
      "20000/20000 [==============================] - 3s 129us/sample - loss: 3.3005 - accuracy: 0.8776 - val_loss: 4.4432 - val_accuracy: 0.6153\n",
      "Epoch 25/50\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 3.3502 - accuracy: 0.8787 - val_loss: 4.2731 - val_accuracy: 0.6246\n",
      "Epoch 26/50\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 3.2461 - accuracy: 0.8870 - val_loss: 4.2053 - val_accuracy: 0.6279\n",
      "Epoch 27/50\n",
      "20000/20000 [==============================] - 3s 131us/sample - loss: 3.2954 - accuracy: 0.8853 - val_loss: 4.2345 - val_accuracy: 0.6342\n",
      "Epoch 28/50\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 3.2783 - accuracy: 0.8865 - val_loss: 4.4077 - val_accuracy: 0.6236\n",
      "Epoch 29/50\n",
      "20000/20000 [==============================] - 3s 131us/sample - loss: 3.3134 - accuracy: 0.8860 - val_loss: 4.3591 - val_accuracy: 0.6133\n",
      "Epoch 30/50\n",
      "20000/20000 [==============================] - 3s 131us/sample - loss: 3.2325 - accuracy: 0.8971 - val_loss: 4.3166 - val_accuracy: 0.6364\n",
      "Epoch 31/50\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 3.2482 - accuracy: 0.8921 - val_loss: 4.3070 - val_accuracy: 0.6249\n",
      "Epoch 32/50\n",
      "20000/20000 [==============================] - 3s 129us/sample - loss: 3.2333 - accuracy: 0.8938 - val_loss: 4.2759 - val_accuracy: 0.6324\n",
      "Epoch 33/50\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 3.2498 - accuracy: 0.8959 - val_loss: 4.3662 - val_accuracy: 0.6220\n",
      "Epoch 34/50\n",
      "20000/20000 [==============================] - 3s 129us/sample - loss: 3.2800 - accuracy: 0.8936 - val_loss: 4.3694 - val_accuracy: 0.6313\n",
      "Epoch 35/50\n",
      "20000/20000 [==============================] - 3s 129us/sample - loss: 3.2842 - accuracy: 0.8959 - val_loss: 4.2711 - val_accuracy: 0.6365\n",
      "Epoch 36/50\n",
      "20000/20000 [==============================] - 3s 129us/sample - loss: 3.2086 - accuracy: 0.9021 - val_loss: 4.3863 - val_accuracy: 0.6154\n",
      "Epoch 37/50\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 3.2388 - accuracy: 0.8996 - val_loss: 4.3579 - val_accuracy: 0.6217\n",
      "Epoch 38/50\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 3.1821 - accuracy: 0.8999 - val_loss: 4.3992 - val_accuracy: 0.6201\n",
      "Epoch 39/50\n",
      "20000/20000 [==============================] - 3s 128us/sample - loss: 3.2348 - accuracy: 0.8960 - val_loss: 4.2978 - val_accuracy: 0.6261\n",
      "Epoch 40/50\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 3.1739 - accuracy: 0.8989 - val_loss: 4.3224 - val_accuracy: 0.6283\n",
      "Epoch 41/50\n",
      "20000/20000 [==============================] - 3s 127us/sample - loss: 3.2056 - accuracy: 0.9003 - val_loss: 4.3344 - val_accuracy: 0.6327\n",
      "Epoch 42/50\n",
      "20000/20000 [==============================] - 3s 129us/sample - loss: 3.2449 - accuracy: 0.9028 - val_loss: 4.3876 - val_accuracy: 0.6201\n",
      "Epoch 43/50\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 3.2315 - accuracy: 0.9012 - val_loss: 4.2053 - val_accuracy: 0.6365\n",
      "Epoch 44/50\n",
      "20000/20000 [==============================] - 2s 125us/sample - loss: 3.1042 - accuracy: 0.9120 - val_loss: 4.3362 - val_accuracy: 0.6199\n",
      "Epoch 45/50\n",
      "20000/20000 [==============================] - 3s 129us/sample - loss: 3.1927 - accuracy: 0.9021 - val_loss: 4.3159 - val_accuracy: 0.6207\n",
      "Epoch 46/50\n",
      "20000/20000 [==============================] - 3s 129us/sample - loss: 3.1532 - accuracy: 0.9086 - val_loss: 4.1661 - val_accuracy: 0.6317\n",
      "Epoch 47/50\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 3.1001 - accuracy: 0.9080 - val_loss: 4.2616 - val_accuracy: 0.6232\n",
      "Epoch 48/50\n",
      "20000/20000 [==============================] - 3s 129us/sample - loss: 3.1383 - accuracy: 0.9089 - val_loss: 4.2825 - val_accuracy: 0.6236\n",
      "Epoch 49/50\n",
      "20000/20000 [==============================] - 3s 128us/sample - loss: 3.1332 - accuracy: 0.9075 - val_loss: 4.4209 - val_accuracy: 0.5997\n",
      "Epoch 50/50\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 3.1494 - accuracy: 0.9068 - val_loss: 4.3112 - val_accuracy: 0.6313\n",
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 16384)        0           conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 16384)        65536       flatten_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 16384)        0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 200)          3277000     dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 200)          800         dense_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 200)          0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 200)          0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_43 (Dense)                (None, 200)          40200       dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 200)          800         dense_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 200)          0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 200)          0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_44 (Dense)                (None, 4)            804         dropout_44[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,614,900\n",
      "Trainable params: 3,578,388\n",
      "Non-trainable params: 36,512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150000 samples, validate on 10000 samples\n",
      "Epoch 1/80\n",
      "150000/150000 [==============================] - 24s 157us/sample - loss: 2.6584 - accuracy: 0.5618 - val_loss: 3.1537 - val_accuracy: 0.1376\n",
      "Epoch 2/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 1.7688 - accuracy: 0.6399 - val_loss: 2.0279 - val_accuracy: 0.5077\n",
      "Epoch 3/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 1.6890 - accuracy: 0.6747 - val_loss: 1.4955 - val_accuracy: 0.7466\n",
      "Epoch 4/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 1.6421 - accuracy: 0.7037 - val_loss: 1.9042 - val_accuracy: 0.5802\n",
      "Epoch 5/80\n",
      "150000/150000 [==============================] - 22s 144us/sample - loss: 1.5868 - accuracy: 0.7294 - val_loss: 1.5532 - val_accuracy: 0.7261\n",
      "Epoch 6/80\n",
      "150000/150000 [==============================] - 22s 148us/sample - loss: 1.5256 - accuracy: 0.7491 - val_loss: 1.3938 - val_accuracy: 0.8030\n",
      "Epoch 7/80\n",
      "150000/150000 [==============================] - 22s 144us/sample - loss: 1.4803 - accuracy: 0.7640 - val_loss: 1.3561 - val_accuracy: 0.7958\n",
      "Epoch 8/80\n",
      "150000/150000 [==============================] - 22s 148us/sample - loss: 1.4283 - accuracy: 0.7771 - val_loss: 1.4631 - val_accuracy: 0.7713\n",
      "Epoch 9/80\n",
      "150000/150000 [==============================] - 22s 147us/sample - loss: 1.3850 - accuracy: 0.7907 - val_loss: 1.6339 - val_accuracy: 0.6692\n",
      "Epoch 10/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 1.3512 - accuracy: 0.7993 - val_loss: 1.2832 - val_accuracy: 0.8156\n",
      "Epoch 11/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 1.3111 - accuracy: 0.8099 - val_loss: 1.5549 - val_accuracy: 0.7051\n",
      "Epoch 12/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 1.2785 - accuracy: 0.8199 - val_loss: 1.3873 - val_accuracy: 0.7778\n",
      "Epoch 13/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 1.2350 - accuracy: 0.8269 - val_loss: 1.3901 - val_accuracy: 0.7367\n",
      "Epoch 14/80\n",
      "150000/150000 [==============================] - 21s 142us/sample - loss: 1.2059 - accuracy: 0.8351 - val_loss: 1.1391 - val_accuracy: 0.8361\n",
      "Epoch 15/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 1.1722 - accuracy: 0.8403 - val_loss: 1.1328 - val_accuracy: 0.8396\n",
      "Epoch 16/80\n",
      "150000/150000 [==============================] - 22s 144us/sample - loss: 1.1403 - accuracy: 0.8488 - val_loss: 1.3112 - val_accuracy: 0.7749\n",
      "Epoch 17/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 1.1130 - accuracy: 0.8544 - val_loss: 1.3474 - val_accuracy: 0.7738\n",
      "Epoch 18/80\n",
      "150000/150000 [==============================] - 22s 144us/sample - loss: 1.0821 - accuracy: 0.8607 - val_loss: 1.2117 - val_accuracy: 0.8053\n",
      "Epoch 19/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 1.0557 - accuracy: 0.8652 - val_loss: 1.3013 - val_accuracy: 0.7926\n",
      "Epoch 20/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 1.0353 - accuracy: 0.8695 - val_loss: 1.0747 - val_accuracy: 0.8384\n",
      "Epoch 21/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 1.0013 - accuracy: 0.8748 - val_loss: 1.0207 - val_accuracy: 0.8629\n",
      "Epoch 22/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 0.9805 - accuracy: 0.8802 - val_loss: 1.8967 - val_accuracy: 0.6401\n",
      "Epoch 23/80\n",
      "150000/150000 [==============================] - 22s 149us/sample - loss: 0.9553 - accuracy: 0.8827 - val_loss: 1.0611 - val_accuracy: 0.8372\n",
      "Epoch 24/80\n",
      "150000/150000 [==============================] - 22s 148us/sample - loss: 0.9391 - accuracy: 0.8871 - val_loss: 1.1449 - val_accuracy: 0.8080\n",
      "Epoch 25/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 0.9179 - accuracy: 0.8910 - val_loss: 1.3632 - val_accuracy: 0.7410\n",
      "Epoch 26/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 0.8977 - accuracy: 0.8950 - val_loss: 1.3123 - val_accuracy: 0.7622\n",
      "Epoch 27/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 0.8786 - accuracy: 0.8988 - val_loss: 1.0633 - val_accuracy: 0.8392\n",
      "Epoch 28/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 0.8586 - accuracy: 0.9013 - val_loss: 1.2016 - val_accuracy: 0.7828\n",
      "Epoch 29/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 0.8409 - accuracy: 0.9060 - val_loss: 0.9863 - val_accuracy: 0.8513\n",
      "Epoch 30/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 0.8254 - accuracy: 0.9075 - val_loss: 1.4030 - val_accuracy: 0.7378\n",
      "Epoch 31/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 0.8124 - accuracy: 0.9094 - val_loss: 1.2584 - val_accuracy: 0.7944\n",
      "Epoch 32/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 0.7982 - accuracy: 0.9129 - val_loss: 1.0303 - val_accuracy: 0.8299\n",
      "Epoch 33/80\n",
      "150000/150000 [==============================] - 22s 147us/sample - loss: 0.7832 - accuracy: 0.9150 - val_loss: 1.7523 - val_accuracy: 0.6728\n",
      "Epoch 34/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 0.7714 - accuracy: 0.9179 - val_loss: 1.0740 - val_accuracy: 0.8387\n",
      "Epoch 35/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 0.7581 - accuracy: 0.9189 - val_loss: 1.3655 - val_accuracy: 0.7545\n",
      "Epoch 36/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 0.7450 - accuracy: 0.9218 - val_loss: 1.2162 - val_accuracy: 0.7893\n",
      "Epoch 37/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 0.7313 - accuracy: 0.9248 - val_loss: 1.4046 - val_accuracy: 0.7243\n",
      "Epoch 38/80\n",
      "150000/150000 [==============================] - 22s 147us/sample - loss: 0.7247 - accuracy: 0.9248 - val_loss: 1.9880 - val_accuracy: 0.6099\n",
      "Epoch 39/80\n",
      "150000/150000 [==============================] - 22s 149us/sample - loss: 0.7163 - accuracy: 0.9266 - val_loss: 0.9148 - val_accuracy: 0.8696\n",
      "Epoch 40/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 0.7017 - accuracy: 0.9301 - val_loss: 1.4572 - val_accuracy: 0.7167\n",
      "Epoch 41/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 0.6925 - accuracy: 0.9306 - val_loss: 1.0985 - val_accuracy: 0.8090\n",
      "Epoch 42/80\n",
      "150000/150000 [==============================] - 22s 144us/sample - loss: 0.6857 - accuracy: 0.9323 - val_loss: 1.7263 - val_accuracy: 0.6609\n",
      "Epoch 43/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 0.6779 - accuracy: 0.9338 - val_loss: 1.2898 - val_accuracy: 0.7725\n",
      "Epoch 44/80\n",
      "150000/150000 [==============================] - 22s 147us/sample - loss: 0.6727 - accuracy: 0.9350 - val_loss: 1.1868 - val_accuracy: 0.7980\n",
      "Epoch 45/80\n",
      "150000/150000 [==============================] - 22s 147us/sample - loss: 0.6586 - accuracy: 0.9364 - val_loss: 1.2793 - val_accuracy: 0.7735\n",
      "Epoch 46/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 0.6524 - accuracy: 0.9378 - val_loss: 1.1831 - val_accuracy: 0.8067\n",
      "Epoch 47/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 0.6431 - accuracy: 0.9393 - val_loss: 1.6429 - val_accuracy: 0.6916\n",
      "Epoch 48/80\n",
      "150000/150000 [==============================] - 22s 147us/sample - loss: 0.6437 - accuracy: 0.9407 - val_loss: 1.0259 - val_accuracy: 0.8472\n",
      "Epoch 49/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 0.6373 - accuracy: 0.9410 - val_loss: 1.5957 - val_accuracy: 0.7064\n",
      "Epoch 50/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 0.6251 - accuracy: 0.9422 - val_loss: 1.2151 - val_accuracy: 0.7695\n",
      "Epoch 51/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 0.6199 - accuracy: 0.9439 - val_loss: 1.3840 - val_accuracy: 0.7819\n",
      "Epoch 52/80\n",
      "150000/150000 [==============================] - 22s 144us/sample - loss: 0.6142 - accuracy: 0.9443 - val_loss: 1.2133 - val_accuracy: 0.8062\n",
      "Epoch 53/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 0.6072 - accuracy: 0.9453 - val_loss: 1.6923 - val_accuracy: 0.6991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 0.6029 - accuracy: 0.9464 - val_loss: 1.4999 - val_accuracy: 0.7175\n",
      "Epoch 55/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 0.5971 - accuracy: 0.9475 - val_loss: 0.8095 - val_accuracy: 0.8914\n",
      "Epoch 56/80\n",
      "150000/150000 [==============================] - 22s 144us/sample - loss: 0.5891 - accuracy: 0.9480 - val_loss: 0.9916 - val_accuracy: 0.8487\n",
      "Epoch 57/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 0.5868 - accuracy: 0.9490 - val_loss: 1.2952 - val_accuracy: 0.7758\n",
      "Epoch 58/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 0.5808 - accuracy: 0.9507 - val_loss: 1.5408 - val_accuracy: 0.7338\n",
      "Epoch 59/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 0.5729 - accuracy: 0.9510 - val_loss: 1.2554 - val_accuracy: 0.7807\n",
      "Epoch 60/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 0.5673 - accuracy: 0.9519 - val_loss: 1.3122 - val_accuracy: 0.7893\n",
      "Epoch 61/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 0.5660 - accuracy: 0.9520 - val_loss: 1.0796 - val_accuracy: 0.8307\n",
      "Epoch 62/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 0.5609 - accuracy: 0.9529 - val_loss: 1.4972 - val_accuracy: 0.7352\n",
      "Epoch 63/80\n",
      "150000/150000 [==============================] - 22s 144us/sample - loss: 0.5567 - accuracy: 0.9537 - val_loss: 1.2160 - val_accuracy: 0.7892\n",
      "Epoch 64/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 0.5517 - accuracy: 0.9547 - val_loss: 1.4039 - val_accuracy: 0.7482\n",
      "Epoch 65/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 0.5480 - accuracy: 0.9553 - val_loss: 1.6066 - val_accuracy: 0.7301\n",
      "Epoch 66/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 0.5517 - accuracy: 0.9548 - val_loss: 1.1207 - val_accuracy: 0.8122\n",
      "Epoch 67/80\n",
      "150000/150000 [==============================] - 22s 147us/sample - loss: 0.5401 - accuracy: 0.9565 - val_loss: 1.7213 - val_accuracy: 0.7009\n",
      "Epoch 68/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 0.5387 - accuracy: 0.9567 - val_loss: 2.1037 - val_accuracy: 0.6505\n",
      "Epoch 69/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 0.5316 - accuracy: 0.9575 - val_loss: 1.0171 - val_accuracy: 0.8439\n",
      "Epoch 70/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 0.5283 - accuracy: 0.9584 - val_loss: 1.4925 - val_accuracy: 0.7446\n",
      "Epoch 71/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 0.5320 - accuracy: 0.9574 - val_loss: 1.3528 - val_accuracy: 0.7722\n",
      "Epoch 72/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 0.5227 - accuracy: 0.9597 - val_loss: 1.2272 - val_accuracy: 0.7986\n",
      "Epoch 73/80\n",
      "150000/150000 [==============================] - 22s 144us/sample - loss: 0.5239 - accuracy: 0.9591 - val_loss: 1.0837 - val_accuracy: 0.8389\n",
      "Epoch 74/80\n",
      "150000/150000 [==============================] - 22s 144us/sample - loss: 0.5176 - accuracy: 0.9604 - val_loss: 1.1870 - val_accuracy: 0.8173\n",
      "Epoch 75/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 0.5187 - accuracy: 0.9598 - val_loss: 1.2965 - val_accuracy: 0.7828\n",
      "Epoch 76/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 0.5168 - accuracy: 0.9604 - val_loss: 1.3474 - val_accuracy: 0.7790\n",
      "Epoch 77/80\n",
      "150000/150000 [==============================] - 22s 147us/sample - loss: 0.5142 - accuracy: 0.9603 - val_loss: 1.5770 - val_accuracy: 0.7344\n",
      "Epoch 78/80\n",
      "150000/150000 [==============================] - 22s 147us/sample - loss: 0.5076 - accuracy: 0.9614 - val_loss: 1.2645 - val_accuracy: 0.7934\n",
      "Epoch 79/80\n",
      "150000/150000 [==============================] - 22s 145us/sample - loss: 0.5042 - accuracy: 0.9619 - val_loss: 1.5926 - val_accuracy: 0.7249\n",
      "Epoch 80/80\n",
      "150000/150000 [==============================] - 22s 146us/sample - loss: 0.5054 - accuracy: 0.9620 - val_loss: 1.5454 - val_accuracy: 0.7325\n",
      "INFO:tensorflow:Assets written to: selfsupervised_25/assets\n",
      "Model: \"model_15\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 16384)        0           conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 16384)        65536       flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 16384)        0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_45 (Dense)                (None, 200)          3277000     dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 200)          800         dense_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 200)          0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 200)          0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_46 (Dense)                (None, 200)          40200       dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 200)          800         dense_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 200)          0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 200)          0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_47 (Dense)                (None, 10)           2010        dropout_47[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,473,002\n",
      "Trainable params: 3,438,026\n",
      "Non-trainable params: 34,976\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 7s 147us/sample - loss: 4.6610 - accuracy: 0.4772 - val_loss: 5.3534 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 6s 116us/sample - loss: 3.6696 - accuracy: 0.6062 - val_loss: 4.2696 - val_accuracy: 0.3832\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 6s 117us/sample - loss: 3.4813 - accuracy: 0.6448 - val_loss: 3.4544 - val_accuracy: 0.6423\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 6s 118us/sample - loss: 3.4146 - accuracy: 0.6716 - val_loss: 3.4412 - val_accuracy: 0.6610\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 6s 114us/sample - loss: 3.3820 - accuracy: 0.6918 - val_loss: 3.4401 - val_accuracy: 0.6746\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 6s 114us/sample - loss: 3.3924 - accuracy: 0.7080 - val_loss: 3.4598 - val_accuracy: 0.6844\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 6s 115us/sample - loss: 3.3753 - accuracy: 0.7178 - val_loss: 3.5302 - val_accuracy: 0.6799\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 6s 115us/sample - loss: 3.4025 - accuracy: 0.7311 - val_loss: 3.5456 - val_accuracy: 0.6812\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 6s 115us/sample - loss: 3.4094 - accuracy: 0.7396 - val_loss: 3.5961 - val_accuracy: 0.6732\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 6s 115us/sample - loss: 3.3973 - accuracy: 0.7501 - val_loss: 3.6040 - val_accuracy: 0.6803\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 6s 115us/sample - loss: 3.4122 - accuracy: 0.7549 - val_loss: 3.6232 - val_accuracy: 0.6871\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 6s 115us/sample - loss: 3.3981 - accuracy: 0.7647 - val_loss: 3.6562 - val_accuracy: 0.6838\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 6s 114us/sample - loss: 3.4084 - accuracy: 0.7700 - val_loss: 3.6853 - val_accuracy: 0.6937\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 6s 116us/sample - loss: 3.4055 - accuracy: 0.7756 - val_loss: 3.6961 - val_accuracy: 0.6868\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 6s 116us/sample - loss: 3.3989 - accuracy: 0.7815 - val_loss: 4.0342 - val_accuracy: 0.6157\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 6s 115us/sample - loss: 3.3828 - accuracy: 0.7855 - val_loss: 3.6477 - val_accuracy: 0.6992\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 6s 115us/sample - loss: 3.3711 - accuracy: 0.7903 - val_loss: 3.6829 - val_accuracy: 0.6859\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 6s 116us/sample - loss: 3.3651 - accuracy: 0.7950 - val_loss: 3.7364 - val_accuracy: 0.6740\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 6s 115us/sample - loss: 3.3549 - accuracy: 0.7970 - val_loss: 3.6977 - val_accuracy: 0.6983\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 6s 115us/sample - loss: 3.3220 - accuracy: 0.8039 - val_loss: 3.7393 - val_accuracy: 0.6638\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 6s 116us/sample - loss: 3.3278 - accuracy: 0.8035 - val_loss: 3.7596 - val_accuracy: 0.6715\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 6s 116us/sample - loss: 3.3204 - accuracy: 0.8062 - val_loss: 3.7298 - val_accuracy: 0.6989\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 6s 115us/sample - loss: 3.3195 - accuracy: 0.8099 - val_loss: 3.7557 - val_accuracy: 0.6877\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 6s 115us/sample - loss: 3.3030 - accuracy: 0.8100 - val_loss: 3.7487 - val_accuracy: 0.6992\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 6s 113us/sample - loss: 3.2875 - accuracy: 0.8163 - val_loss: 3.7903 - val_accuracy: 0.6975\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 6s 121us/sample - loss: 3.2838 - accuracy: 0.8192 - val_loss: 3.7870 - val_accuracy: 0.6675\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 6s 121us/sample - loss: 3.2706 - accuracy: 0.8204 - val_loss: 3.7371 - val_accuracy: 0.6885\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 6s 116us/sample - loss: 3.2737 - accuracy: 0.8198 - val_loss: 3.6988 - val_accuracy: 0.6952\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 6s 114us/sample - loss: 3.2353 - accuracy: 0.8268 - val_loss: 3.7110 - val_accuracy: 0.6918\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 6s 115us/sample - loss: 3.2385 - accuracy: 0.8257 - val_loss: 3.6588 - val_accuracy: 0.7102\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 6s 113us/sample - loss: 3.2048 - accuracy: 0.8299 - val_loss: 3.7915 - val_accuracy: 0.6810\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 6s 115us/sample - loss: 3.1960 - accuracy: 0.8311 - val_loss: 3.7006 - val_accuracy: 0.6929\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 6s 114us/sample - loss: 3.2083 - accuracy: 0.8301 - val_loss: 3.9669 - val_accuracy: 0.6388\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 6s 117us/sample - loss: 3.1722 - accuracy: 0.8340 - val_loss: 3.7468 - val_accuracy: 0.6885\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 6s 116us/sample - loss: 3.1706 - accuracy: 0.8354 - val_loss: 3.7815 - val_accuracy: 0.6831\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 6s 114us/sample - loss: 3.1497 - accuracy: 0.8386 - val_loss: 3.6945 - val_accuracy: 0.6957\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 6s 114us/sample - loss: 3.1353 - accuracy: 0.8376 - val_loss: 3.6442 - val_accuracy: 0.6978\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 6s 115us/sample - loss: 3.1339 - accuracy: 0.8377 - val_loss: 3.7430 - val_accuracy: 0.6873\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 6s 116us/sample - loss: 3.1125 - accuracy: 0.8418 - val_loss: 3.6323 - val_accuracy: 0.7009\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 6s 114us/sample - loss: 3.1118 - accuracy: 0.8413 - val_loss: 3.5978 - val_accuracy: 0.7058\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 6s 116us/sample - loss: 3.0969 - accuracy: 0.8428 - val_loss: 3.7434 - val_accuracy: 0.6716\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 6s 114us/sample - loss: 3.1006 - accuracy: 0.8443 - val_loss: 3.7603 - val_accuracy: 0.6873\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 6s 115us/sample - loss: 3.0719 - accuracy: 0.8441 - val_loss: 3.6411 - val_accuracy: 0.6896\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 6s 116us/sample - loss: 3.0611 - accuracy: 0.8488 - val_loss: 3.7029 - val_accuracy: 0.6832\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 6s 113us/sample - loss: 3.0443 - accuracy: 0.8472 - val_loss: 3.8732 - val_accuracy: 0.6569\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 6s 115us/sample - loss: 3.0526 - accuracy: 0.8468 - val_loss: 3.5805 - val_accuracy: 0.6970\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 6s 114us/sample - loss: 2.9866 - accuracy: 0.8518 - val_loss: 3.8216 - val_accuracy: 0.6547\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 6s 114us/sample - loss: 3.0191 - accuracy: 0.8501 - val_loss: 3.5930 - val_accuracy: 0.6900\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 6s 117us/sample - loss: 3.0081 - accuracy: 0.8533 - val_loss: 3.6165 - val_accuracy: 0.6912\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 6s 120us/sample - loss: 2.9761 - accuracy: 0.8563 - val_loss: 3.6589 - val_accuracy: 0.6844\n",
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 16384)        0           conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 16384)        65536       flatten_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 16384)        0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_48 (Dense)                (None, 200)          3277000     dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 200)          800         dense_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 200)          0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 200)          0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_49 (Dense)                (None, 200)          40200       dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 200)          800         dense_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 200)          0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 200)          0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_50 (Dense)                (None, 4)            804         dropout_50[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,614,900\n",
      "Trainable params: 3,578,388\n",
      "Non-trainable params: 36,512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/80\n",
      "100000/100000 [==============================] - 16s 164us/sample - loss: 2.9715 - accuracy: 0.5320 - val_loss: 2.3379 - val_accuracy: 0.4203\n",
      "Epoch 2/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 1.9060 - accuracy: 0.6109 - val_loss: 2.0761 - val_accuracy: 0.5047\n",
      "Epoch 3/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 1.7460 - accuracy: 0.6428 - val_loss: 1.5171 - val_accuracy: 0.7464\n",
      "Epoch 4/80\n",
      "100000/100000 [==============================] - 14s 142us/sample - loss: 1.6978 - accuracy: 0.6698 - val_loss: 2.3302 - val_accuracy: 0.3863\n",
      "Epoch 5/80\n",
      "100000/100000 [==============================] - 15s 145us/sample - loss: 1.6759 - accuracy: 0.6929 - val_loss: 2.0009 - val_accuracy: 0.5749\n",
      "Epoch 6/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 1.6389 - accuracy: 0.7146 - val_loss: 2.0084 - val_accuracy: 0.6131\n",
      "Epoch 7/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 1.5960 - accuracy: 0.7308 - val_loss: 1.2675 - val_accuracy: 0.8566\n",
      "Epoch 8/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 1.5610 - accuracy: 0.7465 - val_loss: 1.7612 - val_accuracy: 0.6634\n",
      "Epoch 9/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 1.5168 - accuracy: 0.7607 - val_loss: 1.8551 - val_accuracy: 0.6628\n",
      "Epoch 10/80\n",
      "100000/100000 [==============================] - 15s 146us/sample - loss: 1.4828 - accuracy: 0.7732 - val_loss: 1.7314 - val_accuracy: 0.6449\n",
      "Epoch 11/80\n",
      "100000/100000 [==============================] - 15s 147us/sample - loss: 1.4471 - accuracy: 0.7844 - val_loss: 2.1176 - val_accuracy: 0.5125\n",
      "Epoch 12/80\n",
      "100000/100000 [==============================] - 15s 146us/sample - loss: 1.4100 - accuracy: 0.7953 - val_loss: 1.5715 - val_accuracy: 0.7072\n",
      "Epoch 13/80\n",
      "100000/100000 [==============================] - 15s 145us/sample - loss: 1.3876 - accuracy: 0.8049 - val_loss: 1.5213 - val_accuracy: 0.7407\n",
      "Epoch 14/80\n",
      "100000/100000 [==============================] - 15s 146us/sample - loss: 1.3498 - accuracy: 0.8134 - val_loss: 1.6234 - val_accuracy: 0.6900\n",
      "Epoch 15/80\n",
      "100000/100000 [==============================] - 15s 149us/sample - loss: 1.3278 - accuracy: 0.8225 - val_loss: 1.4374 - val_accuracy: 0.7665\n",
      "Epoch 16/80\n",
      "100000/100000 [==============================] - 14s 142us/sample - loss: 1.2932 - accuracy: 0.8306 - val_loss: 2.2469 - val_accuracy: 0.5908\n",
      "Epoch 17/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 1.2673 - accuracy: 0.8391 - val_loss: 1.6638 - val_accuracy: 0.7102\n",
      "Epoch 18/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 1.2370 - accuracy: 0.8447 - val_loss: 1.4765 - val_accuracy: 0.7410\n",
      "Epoch 19/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 1.2071 - accuracy: 0.8531 - val_loss: 1.3184 - val_accuracy: 0.7956\n",
      "Epoch 20/80\n",
      "100000/100000 [==============================] - 14s 145us/sample - loss: 1.1798 - accuracy: 0.8605 - val_loss: 1.5216 - val_accuracy: 0.7159\n",
      "Epoch 21/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 1.1536 - accuracy: 0.8649 - val_loss: 1.6272 - val_accuracy: 0.6972\n",
      "Epoch 22/80\n",
      "100000/100000 [==============================] - 14s 143us/sample - loss: 1.1294 - accuracy: 0.8713 - val_loss: 2.0319 - val_accuracy: 0.5992\n",
      "Epoch 23/80\n",
      "100000/100000 [==============================] - 14s 142us/sample - loss: 1.1023 - accuracy: 0.8789 - val_loss: 1.8692 - val_accuracy: 0.6480\n",
      "Epoch 24/80\n",
      "100000/100000 [==============================] - 14s 142us/sample - loss: 1.0763 - accuracy: 0.8840 - val_loss: 2.2130 - val_accuracy: 0.6008\n",
      "Epoch 25/80\n",
      "100000/100000 [==============================] - 14s 141us/sample - loss: 1.0494 - accuracy: 0.8886 - val_loss: 1.4186 - val_accuracy: 0.7622\n",
      "Epoch 26/80\n",
      "100000/100000 [==============================] - 15s 146us/sample - loss: 1.0425 - accuracy: 0.8915 - val_loss: 1.8873 - val_accuracy: 0.6213\n",
      "Epoch 27/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 1.0056 - accuracy: 0.8964 - val_loss: 1.2781 - val_accuracy: 0.7907\n",
      "Epoch 28/80\n",
      "100000/100000 [==============================] - 14s 145us/sample - loss: 0.9865 - accuracy: 0.9005 - val_loss: 1.2612 - val_accuracy: 0.8075\n",
      "Epoch 29/80\n",
      "100000/100000 [==============================] - 14s 143us/sample - loss: 0.9532 - accuracy: 0.9054 - val_loss: 1.8614 - val_accuracy: 0.6433\n",
      "Epoch 30/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 0.9463 - accuracy: 0.9091 - val_loss: 1.4148 - val_accuracy: 0.7583\n",
      "Epoch 31/80\n",
      "100000/100000 [==============================] - 14s 143us/sample - loss: 0.9213 - accuracy: 0.9123 - val_loss: 1.5388 - val_accuracy: 0.7132\n",
      "Epoch 32/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 0.9075 - accuracy: 0.9149 - val_loss: 1.4469 - val_accuracy: 0.7852\n",
      "Epoch 33/80\n",
      "100000/100000 [==============================] - 15s 145us/sample - loss: 0.8847 - accuracy: 0.9180 - val_loss: 2.3451 - val_accuracy: 0.5758\n",
      "Epoch 34/80\n",
      "100000/100000 [==============================] - 15s 145us/sample - loss: 0.8582 - accuracy: 0.9218 - val_loss: 2.1228 - val_accuracy: 0.6332\n",
      "Epoch 35/80\n",
      "100000/100000 [==============================] - 14s 145us/sample - loss: 0.8440 - accuracy: 0.9235 - val_loss: 1.1060 - val_accuracy: 0.8372\n",
      "Epoch 36/80\n",
      "100000/100000 [==============================] - 14s 145us/sample - loss: 0.8383 - accuracy: 0.9255 - val_loss: 1.3444 - val_accuracy: 0.7878\n",
      "Epoch 37/80\n",
      "100000/100000 [==============================] - 14s 143us/sample - loss: 0.8258 - accuracy: 0.9269 - val_loss: 1.2869 - val_accuracy: 0.8053\n",
      "Epoch 38/80\n",
      "100000/100000 [==============================] - 15s 145us/sample - loss: 0.8100 - accuracy: 0.9287 - val_loss: 1.2210 - val_accuracy: 0.8141\n",
      "Epoch 39/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 0.7857 - accuracy: 0.9314 - val_loss: 1.2665 - val_accuracy: 0.8058\n",
      "Epoch 40/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 0.7763 - accuracy: 0.9341 - val_loss: 2.0148 - val_accuracy: 0.6067\n",
      "Epoch 41/80\n",
      "100000/100000 [==============================] - 14s 143us/sample - loss: 0.7790 - accuracy: 0.9339 - val_loss: 1.3173 - val_accuracy: 0.7806\n",
      "Epoch 42/80\n",
      "100000/100000 [==============================] - 14s 142us/sample - loss: 0.7509 - accuracy: 0.9360 - val_loss: 1.9000 - val_accuracy: 0.6543\n",
      "Epoch 43/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 0.7504 - accuracy: 0.9379 - val_loss: 1.6756 - val_accuracy: 0.7321\n",
      "Epoch 44/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 0.7331 - accuracy: 0.9399 - val_loss: 1.2280 - val_accuracy: 0.8129\n",
      "Epoch 45/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 0.7262 - accuracy: 0.9386 - val_loss: 1.4502 - val_accuracy: 0.7584\n",
      "Epoch 46/80\n",
      "100000/100000 [==============================] - 15s 146us/sample - loss: 0.7048 - accuracy: 0.9445 - val_loss: 1.7232 - val_accuracy: 0.6980\n",
      "Epoch 47/80\n",
      "100000/100000 [==============================] - 14s 143us/sample - loss: 0.7055 - accuracy: 0.9442 - val_loss: 1.4606 - val_accuracy: 0.7551\n",
      "Epoch 48/80\n",
      "100000/100000 [==============================] - 14s 142us/sample - loss: 0.6879 - accuracy: 0.9462 - val_loss: 1.4080 - val_accuracy: 0.7758\n",
      "Epoch 49/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 0.6783 - accuracy: 0.9463 - val_loss: 1.3489 - val_accuracy: 0.7869\n",
      "Epoch 50/80\n",
      "100000/100000 [==============================] - 14s 145us/sample - loss: 0.6724 - accuracy: 0.9472 - val_loss: 1.4246 - val_accuracy: 0.7607\n",
      "Epoch 51/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 0.6687 - accuracy: 0.9475 - val_loss: 1.2470 - val_accuracy: 0.8047\n",
      "Epoch 52/80\n",
      "100000/100000 [==============================] - 14s 143us/sample - loss: 0.6578 - accuracy: 0.9498 - val_loss: 1.4555 - val_accuracy: 0.7555\n",
      "Epoch 53/80\n",
      "100000/100000 [==============================] - 15s 146us/sample - loss: 0.6539 - accuracy: 0.9505 - val_loss: 1.3635 - val_accuracy: 0.7668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/80\n",
      "100000/100000 [==============================] - 14s 142us/sample - loss: 0.6473 - accuracy: 0.9505 - val_loss: 1.5218 - val_accuracy: 0.7332\n",
      "Epoch 55/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 0.6340 - accuracy: 0.9512 - val_loss: 1.3973 - val_accuracy: 0.7687\n",
      "Epoch 56/80\n",
      "100000/100000 [==============================] - 15s 148us/sample - loss: 0.6332 - accuracy: 0.9525 - val_loss: 1.8263 - val_accuracy: 0.7009\n",
      "Epoch 57/80\n",
      "100000/100000 [==============================] - 14s 143us/sample - loss: 0.6223 - accuracy: 0.9539 - val_loss: 1.4504 - val_accuracy: 0.7706\n",
      "Epoch 58/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 0.6115 - accuracy: 0.9538 - val_loss: 1.8674 - val_accuracy: 0.6633\n",
      "Epoch 59/80\n",
      "100000/100000 [==============================] - 14s 145us/sample - loss: 0.6080 - accuracy: 0.9559 - val_loss: 1.5859 - val_accuracy: 0.7290\n",
      "Epoch 60/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 0.6015 - accuracy: 0.9554 - val_loss: 1.0385 - val_accuracy: 0.8497\n",
      "Epoch 61/80\n",
      "100000/100000 [==============================] - 14s 145us/sample - loss: 0.5940 - accuracy: 0.9570 - val_loss: 1.4059 - val_accuracy: 0.7928\n",
      "Epoch 62/80\n",
      "100000/100000 [==============================] - 15s 145us/sample - loss: 0.5886 - accuracy: 0.9574 - val_loss: 1.3162 - val_accuracy: 0.8005\n",
      "Epoch 63/80\n",
      "100000/100000 [==============================] - 14s 145us/sample - loss: 0.5827 - accuracy: 0.9585 - val_loss: 1.6971 - val_accuracy: 0.7153\n",
      "Epoch 64/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 0.5809 - accuracy: 0.9582 - val_loss: 1.4656 - val_accuracy: 0.7939\n",
      "Epoch 65/80\n",
      "100000/100000 [==============================] - 14s 142us/sample - loss: 0.5798 - accuracy: 0.9577 - val_loss: 1.4375 - val_accuracy: 0.7635\n",
      "Epoch 66/80\n",
      "100000/100000 [==============================] - 15s 145us/sample - loss: 0.5706 - accuracy: 0.9600 - val_loss: 1.0575 - val_accuracy: 0.8502\n",
      "Epoch 67/80\n",
      "100000/100000 [==============================] - 14s 145us/sample - loss: 0.5606 - accuracy: 0.9609 - val_loss: 1.5108 - val_accuracy: 0.7651\n",
      "Epoch 68/80\n",
      "100000/100000 [==============================] - 14s 145us/sample - loss: 0.5613 - accuracy: 0.9600 - val_loss: 1.5614 - val_accuracy: 0.7440\n",
      "Epoch 69/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 0.5649 - accuracy: 0.9597 - val_loss: 1.2549 - val_accuracy: 0.7965\n",
      "Epoch 70/80\n",
      "100000/100000 [==============================] - 14s 141us/sample - loss: 0.5471 - accuracy: 0.9619 - val_loss: 1.4656 - val_accuracy: 0.7718\n",
      "Epoch 71/80\n",
      "100000/100000 [==============================] - 14s 143us/sample - loss: 0.5468 - accuracy: 0.9626 - val_loss: 1.5294 - val_accuracy: 0.7362\n",
      "Epoch 72/80\n",
      "100000/100000 [==============================] - 14s 143us/sample - loss: 0.5401 - accuracy: 0.9638 - val_loss: 1.8184 - val_accuracy: 0.6883\n",
      "Epoch 73/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 0.5495 - accuracy: 0.9617 - val_loss: 1.4644 - val_accuracy: 0.7717\n",
      "Epoch 74/80\n",
      "100000/100000 [==============================] - 14s 145us/sample - loss: 0.5292 - accuracy: 0.9648 - val_loss: 2.1798 - val_accuracy: 0.6462\n",
      "Epoch 75/80\n",
      "100000/100000 [==============================] - 14s 143us/sample - loss: 0.5293 - accuracy: 0.9643 - val_loss: 1.8339 - val_accuracy: 0.6923\n",
      "Epoch 76/80\n",
      "100000/100000 [==============================] - 14s 143us/sample - loss: 0.5277 - accuracy: 0.9642 - val_loss: 1.2947 - val_accuracy: 0.7878\n",
      "Epoch 77/80\n",
      "100000/100000 [==============================] - 14s 145us/sample - loss: 0.5295 - accuracy: 0.9649 - val_loss: 1.1866 - val_accuracy: 0.8033\n",
      "Epoch 78/80\n",
      "100000/100000 [==============================] - 14s 145us/sample - loss: 0.5137 - accuracy: 0.9674 - val_loss: 1.4844 - val_accuracy: 0.7650\n",
      "Epoch 79/80\n",
      "100000/100000 [==============================] - 14s 144us/sample - loss: 0.5189 - accuracy: 0.9661 - val_loss: 1.9183 - val_accuracy: 0.6811\n",
      "Epoch 80/80\n",
      "100000/100000 [==============================] - 15s 145us/sample - loss: 0.5164 - accuracy: 0.9659 - val_loss: 1.6037 - val_accuracy: 0.7577\n",
      "INFO:tensorflow:Assets written to: selfsupervised_50/assets\n",
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 16384)        0           conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 16384)        65536       flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 16384)        0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_51 (Dense)                (None, 200)          3277000     dropout_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 200)          800         dense_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 200)          0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 200)          0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_52 (Dense)                (None, 200)          40200       dropout_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 200)          800         dense_52[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 200)          0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, 200)          0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_53 (Dense)                (None, 10)           2010        dropout_53[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,616,106\n",
      "Trainable params: 3,579,594\n",
      "Non-trainable params: 36,512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "100000/100000 [==============================] - 17s 170us/sample - loss: 3.7746 - accuracy: 0.4978 - val_loss: 3.3592 - val_accuracy: 0.4125\n",
      "Epoch 2/50\n",
      "100000/100000 [==============================] - 15s 151us/sample - loss: 2.6458 - accuracy: 0.6385 - val_loss: 2.4379 - val_accuracy: 0.6928\n",
      "Epoch 3/50\n",
      "100000/100000 [==============================] - 15s 148us/sample - loss: 2.4413 - accuracy: 0.6866 - val_loss: 2.4183 - val_accuracy: 0.6849\n",
      "Epoch 4/50\n",
      "100000/100000 [==============================] - 15s 150us/sample - loss: 2.3548 - accuracy: 0.7221 - val_loss: 2.3931 - val_accuracy: 0.7052\n",
      "Epoch 5/50\n",
      "100000/100000 [==============================] - 15s 150us/sample - loss: 2.2917 - accuracy: 0.7474 - val_loss: 2.5171 - val_accuracy: 0.6948\n",
      "Epoch 6/50\n",
      "100000/100000 [==============================] - 15s 149us/sample - loss: 2.2295 - accuracy: 0.7741 - val_loss: 2.3804 - val_accuracy: 0.7301\n",
      "Epoch 7/50\n",
      "100000/100000 [==============================] - 15s 149us/sample - loss: 2.1737 - accuracy: 0.7912 - val_loss: 2.4388 - val_accuracy: 0.7081\n",
      "Epoch 8/50\n",
      "100000/100000 [==============================] - 15s 149us/sample - loss: 2.1281 - accuracy: 0.8100 - val_loss: 2.4703 - val_accuracy: 0.7158\n",
      "Epoch 9/50\n",
      "100000/100000 [==============================] - 15s 147us/sample - loss: 2.0799 - accuracy: 0.8227 - val_loss: 2.4698 - val_accuracy: 0.7129\n",
      "Epoch 10/50\n",
      "100000/100000 [==============================] - 15s 147us/sample - loss: 2.0359 - accuracy: 0.8367 - val_loss: 2.5305 - val_accuracy: 0.6984\n",
      "Epoch 11/50\n",
      "100000/100000 [==============================] - 15s 149us/sample - loss: 1.9793 - accuracy: 0.8508 - val_loss: 2.4505 - val_accuracy: 0.7183\n",
      "Epoch 12/50\n",
      "100000/100000 [==============================] - 15s 148us/sample - loss: 1.9162 - accuracy: 0.8619 - val_loss: 2.4998 - val_accuracy: 0.7034\n",
      "Epoch 13/50\n",
      "100000/100000 [==============================] - 15s 150us/sample - loss: 1.8987 - accuracy: 0.8680 - val_loss: 2.7209 - val_accuracy: 0.6652\n",
      "Epoch 14/50\n",
      "100000/100000 [==============================] - 15s 148us/sample - loss: 1.8458 - accuracy: 0.8773 - val_loss: 2.3960 - val_accuracy: 0.7186\n",
      "Epoch 15/50\n",
      "100000/100000 [==============================] - 15s 148us/sample - loss: 1.7968 - accuracy: 0.8839 - val_loss: 2.4425 - val_accuracy: 0.7248\n",
      "Epoch 16/50\n",
      "100000/100000 [==============================] - 15s 149us/sample - loss: 1.7670 - accuracy: 0.8905 - val_loss: 2.4628 - val_accuracy: 0.7111\n",
      "Epoch 17/50\n",
      "100000/100000 [==============================] - 15s 148us/sample - loss: 1.7138 - accuracy: 0.8959 - val_loss: 2.4510 - val_accuracy: 0.7167\n",
      "Epoch 18/50\n",
      "100000/100000 [==============================] - 15s 149us/sample - loss: 1.6745 - accuracy: 0.9030 - val_loss: 2.5453 - val_accuracy: 0.6798\n",
      "Epoch 19/50\n",
      "100000/100000 [==============================] - 15s 147us/sample - loss: 1.6480 - accuracy: 0.9045 - val_loss: 2.4052 - val_accuracy: 0.7178\n",
      "Epoch 20/50\n",
      "100000/100000 [==============================] - 15s 147us/sample - loss: 1.6009 - accuracy: 0.9087 - val_loss: 2.3847 - val_accuracy: 0.7257\n",
      "Epoch 21/50\n",
      "100000/100000 [==============================] - 15s 149us/sample - loss: 1.5750 - accuracy: 0.9133 - val_loss: 2.4610 - val_accuracy: 0.7147\n",
      "Epoch 22/50\n",
      "100000/100000 [==============================] - 15s 147us/sample - loss: 1.5348 - accuracy: 0.9176 - val_loss: 2.6963 - val_accuracy: 0.6654\n",
      "Epoch 23/50\n",
      "100000/100000 [==============================] - 15s 147us/sample - loss: 1.5078 - accuracy: 0.9202 - val_loss: 2.4229 - val_accuracy: 0.7202\n",
      "Epoch 24/50\n",
      "100000/100000 [==============================] - 15s 148us/sample - loss: 1.4726 - accuracy: 0.9224 - val_loss: 2.2616 - val_accuracy: 0.7111\n",
      "Epoch 25/50\n",
      "100000/100000 [==============================] - 15s 147us/sample - loss: 1.4460 - accuracy: 0.9262 - val_loss: 2.4144 - val_accuracy: 0.7207\n",
      "Epoch 26/50\n",
      "100000/100000 [==============================] - 15s 149us/sample - loss: 1.4354 - accuracy: 0.9264 - val_loss: 2.4822 - val_accuracy: 0.6996\n",
      "Epoch 27/50\n",
      "100000/100000 [==============================] - 15s 150us/sample - loss: 1.3925 - accuracy: 0.9314 - val_loss: 2.4032 - val_accuracy: 0.7174\n",
      "Epoch 28/50\n",
      "100000/100000 [==============================] - 15s 148us/sample - loss: 1.3653 - accuracy: 0.9330 - val_loss: 2.2591 - val_accuracy: 0.7260\n",
      "Epoch 29/50\n",
      "100000/100000 [==============================] - 15s 148us/sample - loss: 1.3366 - accuracy: 0.9334 - val_loss: 2.3788 - val_accuracy: 0.7138\n",
      "Epoch 30/50\n",
      "100000/100000 [==============================] - 15s 147us/sample - loss: 1.3172 - accuracy: 0.9359 - val_loss: 2.2133 - val_accuracy: 0.7252\n",
      "Epoch 31/50\n",
      "100000/100000 [==============================] - 15s 147us/sample - loss: 1.2991 - accuracy: 0.9380 - val_loss: 2.3464 - val_accuracy: 0.7139\n",
      "Epoch 32/50\n",
      "100000/100000 [==============================] - 15s 148us/sample - loss: 1.2835 - accuracy: 0.9380 - val_loss: 2.5072 - val_accuracy: 0.6963\n",
      "Epoch 33/50\n",
      "100000/100000 [==============================] - 15s 149us/sample - loss: 1.2492 - accuracy: 0.9401 - val_loss: 2.4457 - val_accuracy: 0.6701\n",
      "Epoch 34/50\n",
      "100000/100000 [==============================] - 15s 149us/sample - loss: 1.2248 - accuracy: 0.9421 - val_loss: 2.2662 - val_accuracy: 0.7124\n",
      "Epoch 35/50\n",
      "100000/100000 [==============================] - 15s 147us/sample - loss: 1.2142 - accuracy: 0.9435 - val_loss: 2.2943 - val_accuracy: 0.7048\n",
      "Epoch 36/50\n",
      "100000/100000 [==============================] - 15s 149us/sample - loss: 1.1952 - accuracy: 0.9447 - val_loss: 2.5964 - val_accuracy: 0.6782\n",
      "Epoch 37/50\n",
      "100000/100000 [==============================] - 15s 147us/sample - loss: 1.1696 - accuracy: 0.9458 - val_loss: 2.3027 - val_accuracy: 0.7205\n",
      "Epoch 38/50\n",
      "100000/100000 [==============================] - 15s 147us/sample - loss: 1.1649 - accuracy: 0.9459 - val_loss: 2.3380 - val_accuracy: 0.7089\n",
      "Epoch 39/50\n",
      "100000/100000 [==============================] - 15s 147us/sample - loss: 1.1392 - accuracy: 0.9475 - val_loss: 2.2259 - val_accuracy: 0.7236\n",
      "Epoch 40/50\n",
      "100000/100000 [==============================] - 15s 145us/sample - loss: 1.1214 - accuracy: 0.9483 - val_loss: 2.1963 - val_accuracy: 0.7122\n",
      "Epoch 41/50\n",
      "100000/100000 [==============================] - 15s 148us/sample - loss: 1.1137 - accuracy: 0.9483 - val_loss: 2.2695 - val_accuracy: 0.7140\n",
      "Epoch 42/50\n",
      "100000/100000 [==============================] - 15s 148us/sample - loss: 1.0875 - accuracy: 0.9513 - val_loss: 2.2486 - val_accuracy: 0.7049\n",
      "Epoch 43/50\n",
      "100000/100000 [==============================] - 15s 148us/sample - loss: 1.0710 - accuracy: 0.9528 - val_loss: 2.2664 - val_accuracy: 0.7090\n",
      "Epoch 44/50\n",
      "100000/100000 [==============================] - 15s 147us/sample - loss: 1.0668 - accuracy: 0.9513 - val_loss: 2.3092 - val_accuracy: 0.7043\n",
      "Epoch 45/50\n",
      "100000/100000 [==============================] - 15s 150us/sample - loss: 1.0494 - accuracy: 0.9524 - val_loss: 2.2111 - val_accuracy: 0.7132\n",
      "Epoch 46/50\n",
      "100000/100000 [==============================] - 15s 147us/sample - loss: 1.0427 - accuracy: 0.9531 - val_loss: 2.3176 - val_accuracy: 0.7108\n",
      "Epoch 47/50\n",
      "100000/100000 [==============================] - 15s 148us/sample - loss: 1.0275 - accuracy: 0.9546 - val_loss: 2.2537 - val_accuracy: 0.7127\n",
      "Epoch 48/50\n",
      "100000/100000 [==============================] - 15s 149us/sample - loss: 1.0166 - accuracy: 0.9549 - val_loss: 2.1279 - val_accuracy: 0.7195\n",
      "Epoch 49/50\n",
      "100000/100000 [==============================] - 15s 150us/sample - loss: 0.9938 - accuracy: 0.9569 - val_loss: 2.2189 - val_accuracy: 0.7100\n",
      "Epoch 50/50\n",
      "100000/100000 [==============================] - 15s 148us/sample - loss: 0.9965 - accuracy: 0.9563 - val_loss: 2.3377 - val_accuracy: 0.7002\n",
      "Model: \"model_18\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 16384)        0           conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 16384)        65536       flatten_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, 16384)        0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_54 (Dense)                (None, 200)          3277000     dropout_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 200)          800         dense_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 200)          0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, 200)          0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_55 (Dense)                (None, 200)          40200       dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 200)          800         dense_55[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 200)          0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 200)          0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_56 (Dense)                (None, 10)           2010        dropout_56[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,616,106\n",
      "Trainable params: 3,579,594\n",
      "Non-trainable params: 36,512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "200000/200000 [==============================] - 30s 152us/sample - loss: 3.1762 - accuracy: 0.5516 - val_loss: 2.3630 - val_accuracy: 0.6886\n",
      "Epoch 2/50\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 2.3756 - accuracy: 0.6738 - val_loss: 2.1688 - val_accuracy: 0.7296\n",
      "Epoch 3/50\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 2.2656 - accuracy: 0.7169 - val_loss: 2.1539 - val_accuracy: 0.7425\n",
      "Epoch 4/50\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 2.1824 - accuracy: 0.7463 - val_loss: 2.3067 - val_accuracy: 0.7158\n",
      "Epoch 5/50\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 2.1079 - accuracy: 0.7700 - val_loss: 2.2150 - val_accuracy: 0.7352\n",
      "Epoch 6/50\n",
      "200000/200000 [==============================] - 29s 143us/sample - loss: 2.0399 - accuracy: 0.7875 - val_loss: 2.1802 - val_accuracy: 0.7408\n",
      "Epoch 7/50\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 1.9706 - accuracy: 0.8028 - val_loss: 2.1000 - val_accuracy: 0.7567\n",
      "Epoch 8/50\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 1.9045 - accuracy: 0.8172 - val_loss: 2.1392 - val_accuracy: 0.7522\n",
      "Epoch 9/50\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 1.8426 - accuracy: 0.8293 - val_loss: 2.0814 - val_accuracy: 0.7605\n",
      "Epoch 10/50\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 1.7857 - accuracy: 0.8376 - val_loss: 2.0320 - val_accuracy: 0.7565\n",
      "Epoch 11/50\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 1.7222 - accuracy: 0.8475 - val_loss: 2.0056 - val_accuracy: 0.7623\n",
      "Epoch 12/50\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 1.6692 - accuracy: 0.8552 - val_loss: 2.0348 - val_accuracy: 0.7524\n",
      "Epoch 13/50\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 1.6038 - accuracy: 0.8629 - val_loss: 2.0582 - val_accuracy: 0.7454\n",
      "Epoch 14/50\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 1.5596 - accuracy: 0.8697 - val_loss: 1.9438 - val_accuracy: 0.7579\n",
      "Epoch 15/50\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 1.5082 - accuracy: 0.8759 - val_loss: 2.0075 - val_accuracy: 0.7470\n",
      "Epoch 16/50\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 1.4631 - accuracy: 0.8805 - val_loss: 1.9820 - val_accuracy: 0.7463\n",
      "Epoch 17/50\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 1.4214 - accuracy: 0.8858 - val_loss: 1.8578 - val_accuracy: 0.7675\n",
      "Epoch 18/50\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 1.3818 - accuracy: 0.8896 - val_loss: 1.8811 - val_accuracy: 0.7563\n",
      "Epoch 19/50\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 1.3428 - accuracy: 0.8943 - val_loss: 1.8554 - val_accuracy: 0.7579\n",
      "Epoch 20/50\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 1.3111 - accuracy: 0.8972 - val_loss: 1.9155 - val_accuracy: 0.7486\n",
      "Epoch 21/50\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 1.2734 - accuracy: 0.9017 - val_loss: 1.9116 - val_accuracy: 0.7362\n",
      "Epoch 22/50\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 1.2457 - accuracy: 0.9046 - val_loss: 1.7862 - val_accuracy: 0.7659\n",
      "Epoch 23/50\n",
      "200000/200000 [==============================] - 29s 143us/sample - loss: 1.2164 - accuracy: 0.9073 - val_loss: 1.7478 - val_accuracy: 0.7665\n",
      "Epoch 24/50\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 1.1954 - accuracy: 0.9104 - val_loss: 1.7791 - val_accuracy: 0.7616\n",
      "Epoch 25/50\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 1.1693 - accuracy: 0.9121 - val_loss: 1.8411 - val_accuracy: 0.7478\n",
      "Epoch 26/50\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 1.1510 - accuracy: 0.9142 - val_loss: 1.7898 - val_accuracy: 0.7653\n",
      "Epoch 27/50\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 1.1310 - accuracy: 0.9166 - val_loss: 1.8713 - val_accuracy: 0.7398\n",
      "Epoch 28/50\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 1.1072 - accuracy: 0.9190 - val_loss: 2.0026 - val_accuracy: 0.7263\n",
      "Epoch 29/50\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 1.0944 - accuracy: 0.9203 - val_loss: 1.8856 - val_accuracy: 0.7450\n",
      "Epoch 30/50\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 1.0730 - accuracy: 0.9226 - val_loss: 1.7793 - val_accuracy: 0.7518\n",
      "Epoch 31/50\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 1.0609 - accuracy: 0.9244 - val_loss: 1.8248 - val_accuracy: 0.7505\n",
      "Epoch 32/50\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 1.0440 - accuracy: 0.9269 - val_loss: 1.7651 - val_accuracy: 0.7642\n",
      "Epoch 33/50\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 1.0329 - accuracy: 0.9275 - val_loss: 1.8969 - val_accuracy: 0.7476\n",
      "Epoch 34/50\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 1.0124 - accuracy: 0.9300 - val_loss: 1.8488 - val_accuracy: 0.7496\n",
      "Epoch 35/50\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 1.0080 - accuracy: 0.9317 - val_loss: 1.8958 - val_accuracy: 0.7390\n",
      "Epoch 36/50\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.9872 - accuracy: 0.9326 - val_loss: 1.8063 - val_accuracy: 0.7455\n",
      "Epoch 37/50\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 0.9758 - accuracy: 0.9339 - val_loss: 1.9823 - val_accuracy: 0.7285\n",
      "Epoch 38/50\n",
      "200000/200000 [==============================] - 29s 143us/sample - loss: 0.9740 - accuracy: 0.9343 - val_loss: 1.9683 - val_accuracy: 0.7256\n",
      "Epoch 39/50\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.9612 - accuracy: 0.9362 - val_loss: 1.8893 - val_accuracy: 0.7396\n",
      "Epoch 40/50\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.9562 - accuracy: 0.9364 - val_loss: 1.7636 - val_accuracy: 0.7592\n",
      "Epoch 41/50\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.9411 - accuracy: 0.9383 - val_loss: 1.8353 - val_accuracy: 0.7486\n",
      "Epoch 42/50\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 0.9326 - accuracy: 0.9395 - val_loss: 1.7719 - val_accuracy: 0.7584\n",
      "Epoch 43/50\n",
      "200000/200000 [==============================] - 29s 143us/sample - loss: 0.9307 - accuracy: 0.9393 - val_loss: 1.7678 - val_accuracy: 0.7635\n",
      "Epoch 44/50\n",
      "200000/200000 [==============================] - 29s 143us/sample - loss: 0.9214 - accuracy: 0.9413 - val_loss: 1.7390 - val_accuracy: 0.7525\n",
      "Epoch 45/50\n",
      "200000/200000 [==============================] - 29s 143us/sample - loss: 0.9143 - accuracy: 0.9407 - val_loss: 1.8202 - val_accuracy: 0.7532\n",
      "Epoch 46/50\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 0.9052 - accuracy: 0.9438 - val_loss: 1.7979 - val_accuracy: 0.7560\n",
      "Epoch 47/50\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.8988 - accuracy: 0.9438 - val_loss: 1.8701 - val_accuracy: 0.7378\n",
      "Epoch 48/50\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.8839 - accuracy: 0.9449 - val_loss: 1.8086 - val_accuracy: 0.7463\n",
      "Epoch 49/50\n",
      "200000/200000 [==============================] - 28s 139us/sample - loss: 0.8837 - accuracy: 0.9457 - val_loss: 1.9357 - val_accuracy: 0.7390\n",
      "Epoch 50/50\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 0.8836 - accuracy: 0.9453 - val_loss: 1.7878 - val_accuracy: 0.7617\n"
     ]
    }
   ],
   "source": [
    "for p in labelsPercent:\n",
    "    labeled, unlabeled = generate_labeled_unlabeled(examplesForClass, p, False)\n",
    "    #if not os.path.isfile(saved_name + \"_\" + str(p)):\n",
    "    \n",
    "    if not os.path.isfile(saved_name):\n",
    "        logs.append(self_supervised_train(unlabeled, p))\n",
    "    cls_logs.append(fine_tune(labeled, p, True))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Self-supervised with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 495000\n",
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 16384)        0           conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 16384)        65536       flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)            (None, 16384)        0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_57 (Dense)                (None, 200)          3277000     dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 200)          800         dense_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 200)          0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)            (None, 200)          0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_58 (Dense)                (None, 200)          40200       dropout_58[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 200)          800         dense_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 200)          0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)            (None, 200)          0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_59 (Dense)                (None, 10)           2010        dropout_59[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,473,002\n",
      "Trainable params: 3,438,026\n",
      "Non-trainable params: 34,976\n",
      "__________________________________________________________________________________________________\n",
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "5000/5000 [==============================] - 3s 543us/sample - loss: 6.0402 - accuracy: 0.3098 - val_loss: 7.6198 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "5000/5000 [==============================] - 1s 218us/sample - loss: 5.2159 - accuracy: 0.5348 - val_loss: 7.1133 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "5000/5000 [==============================] - 1s 212us/sample - loss: 4.5632 - accuracy: 0.6850 - val_loss: 6.5459 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "5000/5000 [==============================] - 1s 208us/sample - loss: 4.0596 - accuracy: 0.7740 - val_loss: 6.6395 - val_accuracy: 0.1000\n",
      "Epoch 5/50\n",
      "5000/5000 [==============================] - 1s 210us/sample - loss: 3.7262 - accuracy: 0.8302 - val_loss: 6.8041 - val_accuracy: 0.1000\n",
      "Epoch 6/50\n",
      "5000/5000 [==============================] - 1s 214us/sample - loss: 3.5857 - accuracy: 0.8460 - val_loss: 6.0041 - val_accuracy: 0.1000\n",
      "Epoch 7/50\n",
      "5000/5000 [==============================] - 1s 211us/sample - loss: 3.4851 - accuracy: 0.8590 - val_loss: 5.7491 - val_accuracy: 0.1495\n",
      "Epoch 8/50\n",
      "5000/5000 [==============================] - 1s 212us/sample - loss: 3.4836 - accuracy: 0.8780 - val_loss: 5.8911 - val_accuracy: 0.1457\n",
      "Epoch 9/50\n",
      "5000/5000 [==============================] - 1s 211us/sample - loss: 3.5909 - accuracy: 0.8708 - val_loss: 6.1574 - val_accuracy: 0.1000\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 1s 215us/sample - loss: 3.4974 - accuracy: 0.8916 - val_loss: 5.9609 - val_accuracy: 0.1247\n",
      "Epoch 11/50\n",
      "5000/5000 [==============================] - 1s 212us/sample - loss: 3.3928 - accuracy: 0.9032 - val_loss: 5.6167 - val_accuracy: 0.1641\n",
      "Epoch 12/50\n",
      "5000/5000 [==============================] - 1s 213us/sample - loss: 3.4405 - accuracy: 0.8930 - val_loss: 5.6547 - val_accuracy: 0.1043\n",
      "Epoch 13/50\n",
      "5000/5000 [==============================] - 1s 211us/sample - loss: 3.3159 - accuracy: 0.9144 - val_loss: 5.9331 - val_accuracy: 0.1236\n",
      "Epoch 14/50\n",
      "5000/5000 [==============================] - 1s 209us/sample - loss: 3.3149 - accuracy: 0.9160 - val_loss: 5.9983 - val_accuracy: 0.1245\n",
      "Epoch 15/50\n",
      "5000/5000 [==============================] - 1s 206us/sample - loss: 3.1125 - accuracy: 0.9364 - val_loss: 5.5172 - val_accuracy: 0.1758\n",
      "Epoch 16/50\n",
      "5000/5000 [==============================] - 1s 211us/sample - loss: 3.1469 - accuracy: 0.9176 - val_loss: 5.9336 - val_accuracy: 0.1257\n",
      "Epoch 17/50\n",
      "5000/5000 [==============================] - 1s 206us/sample - loss: 3.3968 - accuracy: 0.8938 - val_loss: 6.6479 - val_accuracy: 0.1226\n",
      "Epoch 18/50\n",
      "5000/5000 [==============================] - 1s 216us/sample - loss: 3.4560 - accuracy: 0.9032 - val_loss: 6.0757 - val_accuracy: 0.1886\n",
      "Epoch 19/50\n",
      "5000/5000 [==============================] - 1s 213us/sample - loss: 3.7250 - accuracy: 0.8860 - val_loss: 5.5340 - val_accuracy: 0.3100\n",
      "Epoch 20/50\n",
      "5000/5000 [==============================] - 1s 213us/sample - loss: 3.9285 - accuracy: 0.8820 - val_loss: 5.9007 - val_accuracy: 0.3091\n",
      "Epoch 21/50\n",
      "5000/5000 [==============================] - 1s 213us/sample - loss: 3.8641 - accuracy: 0.9114 - val_loss: 5.6809 - val_accuracy: 0.3686\n",
      "Epoch 22/50\n",
      "5000/5000 [==============================] - 1s 212us/sample - loss: 3.6318 - accuracy: 0.9318 - val_loss: 5.1158 - val_accuracy: 0.4342\n",
      "Epoch 23/50\n",
      "5000/5000 [==============================] - 1s 210us/sample - loss: 3.5652 - accuracy: 0.9112 - val_loss: 4.9971 - val_accuracy: 0.4794\n",
      "Epoch 24/50\n",
      "5000/5000 [==============================] - 1s 217us/sample - loss: 3.2668 - accuracy: 0.9488 - val_loss: 4.7891 - val_accuracy: 0.4665\n",
      "Epoch 25/50\n",
      "5000/5000 [==============================] - 1s 213us/sample - loss: 3.0614 - accuracy: 0.9436 - val_loss: 4.6194 - val_accuracy: 0.4842\n",
      "Epoch 26/50\n",
      "5000/5000 [==============================] - 1s 214us/sample - loss: 3.0118 - accuracy: 0.9446 - val_loss: 4.5001 - val_accuracy: 0.5149\n",
      "Epoch 27/50\n",
      "5000/5000 [==============================] - 1s 213us/sample - loss: 3.0215 - accuracy: 0.9410 - val_loss: 4.6202 - val_accuracy: 0.4935\n",
      "Epoch 28/50\n",
      "5000/5000 [==============================] - 1s 217us/sample - loss: 3.5623 - accuracy: 0.8654 - val_loss: 5.3628 - val_accuracy: 0.4717\n",
      "Epoch 29/50\n",
      "5000/5000 [==============================] - 1s 208us/sample - loss: 3.7616 - accuracy: 0.9026 - val_loss: 5.3444 - val_accuracy: 0.5093\n",
      "Epoch 30/50\n",
      "5000/5000 [==============================] - 1s 211us/sample - loss: 3.5850 - accuracy: 0.9240 - val_loss: 5.0189 - val_accuracy: 0.5175\n",
      "Epoch 31/50\n",
      "5000/5000 [==============================] - 1s 213us/sample - loss: 3.2138 - accuracy: 0.9554 - val_loss: 4.5860 - val_accuracy: 0.5270\n",
      "Epoch 32/50\n",
      "5000/5000 [==============================] - 1s 211us/sample - loss: 2.9378 - accuracy: 0.9556 - val_loss: 4.3462 - val_accuracy: 0.5316\n",
      "Epoch 33/50\n",
      "5000/5000 [==============================] - 1s 213us/sample - loss: 3.0121 - accuracy: 0.9410 - val_loss: 4.6028 - val_accuracy: 0.5235\n",
      "Epoch 34/50\n",
      "5000/5000 [==============================] - 1s 211us/sample - loss: 3.1337 - accuracy: 0.9352 - val_loss: 4.8134 - val_accuracy: 0.4978\n",
      "Epoch 35/50\n",
      "5000/5000 [==============================] - 1s 210us/sample - loss: 3.1966 - accuracy: 0.9338 - val_loss: 4.9858 - val_accuracy: 0.4931\n",
      "Epoch 36/50\n",
      "5000/5000 [==============================] - 1s 209us/sample - loss: 3.2816 - accuracy: 0.9282 - val_loss: 4.9216 - val_accuracy: 0.5092\n",
      "Epoch 37/50\n",
      "5000/5000 [==============================] - 1s 210us/sample - loss: 3.4549 - accuracy: 0.9172 - val_loss: 5.1794 - val_accuracy: 0.5113\n",
      "Epoch 38/50\n",
      "5000/5000 [==============================] - 1s 215us/sample - loss: 3.8115 - accuracy: 0.8990 - val_loss: 5.5621 - val_accuracy: 0.5184\n",
      "Epoch 39/50\n",
      "5000/5000 [==============================] - 1s 210us/sample - loss: 3.6747 - accuracy: 0.9390 - val_loss: 5.1883 - val_accuracy: 0.5322\n",
      "Epoch 40/50\n",
      "5000/5000 [==============================] - 1s 206us/sample - loss: 3.4095 - accuracy: 0.9494 - val_loss: 4.8985 - val_accuracy: 0.5111\n",
      "Epoch 41/50\n",
      "5000/5000 [==============================] - 1s 213us/sample - loss: 3.4698 - accuracy: 0.9282 - val_loss: 5.0879 - val_accuracy: 0.5185\n",
      "Epoch 42/50\n",
      "5000/5000 [==============================] - 1s 219us/sample - loss: 3.4878 - accuracy: 0.9346 - val_loss: 4.9906 - val_accuracy: 0.5243\n",
      "Epoch 43/50\n",
      "5000/5000 [==============================] - 1s 207us/sample - loss: 3.5862 - accuracy: 0.9320 - val_loss: 5.1912 - val_accuracy: 0.5245\n",
      "Epoch 44/50\n",
      "5000/5000 [==============================] - 1s 210us/sample - loss: 3.7224 - accuracy: 0.9182 - val_loss: 5.3388 - val_accuracy: 0.5162\n",
      "Epoch 45/50\n",
      "5000/5000 [==============================] - 1s 212us/sample - loss: 3.7174 - accuracy: 0.9246 - val_loss: 5.2619 - val_accuracy: 0.5158\n",
      "Epoch 46/50\n",
      "5000/5000 [==============================] - 1s 212us/sample - loss: 3.4393 - accuracy: 0.9548 - val_loss: 4.9714 - val_accuracy: 0.5131\n",
      "Epoch 47/50\n",
      "5000/5000 [==============================] - 1s 208us/sample - loss: 3.2664 - accuracy: 0.9500 - val_loss: 4.8809 - val_accuracy: 0.5016\n",
      "Epoch 48/50\n",
      "5000/5000 [==============================] - 1s 215us/sample - loss: 3.4200 - accuracy: 0.9336 - val_loss: 5.0671 - val_accuracy: 0.5048\n",
      "Epoch 49/50\n",
      "5000/5000 [==============================] - 1s 216us/sample - loss: 3.5314 - accuracy: 0.9406 - val_loss: 5.1952 - val_accuracy: 0.5063\n",
      "Epoch 50/50\n",
      "5000/5000 [==============================] - 1s 212us/sample - loss: 3.4377 - accuracy: 0.9462 - val_loss: 5.0201 - val_accuracy: 0.5199\n",
      "10000 490000\n",
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 16384)        0           conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 16384)        65536       flatten_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)            (None, 16384)        0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_60 (Dense)                (None, 200)          3277000     dropout_60[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 200)          800         dense_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 200)          0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)            (None, 200)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_61 (Dense)                (None, 200)          40200       dropout_61[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 200)          800         dense_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 200)          0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)            (None, 200)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_62 (Dense)                (None, 10)           2010        dropout_62[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,473,002\n",
      "Trainable params: 3,438,026\n",
      "Non-trainable params: 34,976\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 3s 312us/sample - loss: 5.7757 - accuracy: 0.3669 - val_loss: 6.8485 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 2s 162us/sample - loss: 4.6450 - accuracy: 0.5871 - val_loss: 6.4541 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 2s 158us/sample - loss: 4.0060 - accuracy: 0.7021 - val_loss: 6.5027 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 3.6578 - accuracy: 0.7759 - val_loss: 6.6357 - val_accuracy: 0.1000\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 3.5340 - accuracy: 0.7990 - val_loss: 6.3520 - val_accuracy: 0.1000\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 3.4792 - accuracy: 0.8218 - val_loss: 6.9328 - val_accuracy: 0.1000\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 3.3843 - accuracy: 0.8474 - val_loss: 6.1802 - val_accuracy: 0.1166\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 3.3494 - accuracy: 0.8575 - val_loss: 6.5593 - val_accuracy: 0.1507\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 2s 159us/sample - loss: 3.3934 - accuracy: 0.8545 - val_loss: 6.1848 - val_accuracy: 0.1478\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 3.3974 - accuracy: 0.8662 - val_loss: 5.0006 - val_accuracy: 0.3501\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 3.3150 - accuracy: 0.8806 - val_loss: 4.7305 - val_accuracy: 0.4441\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 3.4878 - accuracy: 0.8646 - val_loss: 4.6505 - val_accuracy: 0.5229\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 3.4355 - accuracy: 0.8806 - val_loss: 4.4513 - val_accuracy: 0.5448\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 3.4070 - accuracy: 0.8832 - val_loss: 4.3145 - val_accuracy: 0.5906\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 2s 158us/sample - loss: 3.2293 - accuracy: 0.8970 - val_loss: 4.2732 - val_accuracy: 0.5906\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 3.2526 - accuracy: 0.8959 - val_loss: 4.3703 - val_accuracy: 0.5778\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 3.1101 - accuracy: 0.9098 - val_loss: 4.3211 - val_accuracy: 0.5758\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 2s 158us/sample - loss: 3.4486 - accuracy: 0.8789 - val_loss: 4.5621 - val_accuracy: 0.5833\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 2s 159us/sample - loss: 3.3952 - accuracy: 0.8972 - val_loss: 4.5748 - val_accuracy: 0.5567\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 3.2146 - accuracy: 0.9090 - val_loss: 4.4260 - val_accuracy: 0.5593\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 3.0283 - accuracy: 0.9174 - val_loss: 4.2474 - val_accuracy: 0.5785\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 3.2062 - accuracy: 0.9023 - val_loss: 4.4465 - val_accuracy: 0.5834\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 3.2049 - accuracy: 0.9118 - val_loss: 4.4390 - val_accuracy: 0.5722\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 3.1602 - accuracy: 0.9143 - val_loss: 4.3912 - val_accuracy: 0.5723\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 3.1853 - accuracy: 0.9088 - val_loss: 4.4721 - val_accuracy: 0.5802\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 3.1283 - accuracy: 0.9191 - val_loss: 4.3951 - val_accuracy: 0.5802\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 3.0910 - accuracy: 0.9206 - val_loss: 4.4074 - val_accuracy: 0.5828\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 3.1072 - accuracy: 0.9211 - val_loss: 4.3956 - val_accuracy: 0.5612\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 2s 159us/sample - loss: 3.2856 - accuracy: 0.9038 - val_loss: 4.5879 - val_accuracy: 0.5798\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 3.0491 - accuracy: 0.9320 - val_loss: 4.1602 - val_accuracy: 0.5908\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 2.9551 - accuracy: 0.9299 - val_loss: 4.3843 - val_accuracy: 0.5532\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 3.0449 - accuracy: 0.9193 - val_loss: 4.3630 - val_accuracy: 0.5839\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 2s 158us/sample - loss: 3.0792 - accuracy: 0.9242 - val_loss: 4.5328 - val_accuracy: 0.5608\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 3.1241 - accuracy: 0.9257 - val_loss: 4.4592 - val_accuracy: 0.5817\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 3.0427 - accuracy: 0.9289 - val_loss: 4.4175 - val_accuracy: 0.5697\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 2s 159us/sample - loss: 3.3744 - accuracy: 0.9039 - val_loss: 4.7351 - val_accuracy: 0.5860\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 2s 160us/sample - loss: 3.0890 - accuracy: 0.9388 - val_loss: 4.2729 - val_accuracy: 0.5891\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 2s 160us/sample - loss: 2.8148 - accuracy: 0.9450 - val_loss: 4.2510 - val_accuracy: 0.5582\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 2.9130 - accuracy: 0.9292 - val_loss: 4.3688 - val_accuracy: 0.5745\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 2s 158us/sample - loss: 2.9749 - accuracy: 0.9337 - val_loss: 4.5682 - val_accuracy: 0.5586\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 2s 161us/sample - loss: 3.0403 - accuracy: 0.9294 - val_loss: 4.5626 - val_accuracy: 0.5688\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 2.9585 - accuracy: 0.9366 - val_loss: 4.3308 - val_accuracy: 0.5719\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 2.9890 - accuracy: 0.9322 - val_loss: 4.4158 - val_accuracy: 0.5742\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 3.0688 - accuracy: 0.9295 - val_loss: 4.4305 - val_accuracy: 0.5844\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 3.0261 - accuracy: 0.9319 - val_loss: 4.4668 - val_accuracy: 0.5808\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 2s 159us/sample - loss: 3.0995 - accuracy: 0.9323 - val_loss: 4.4524 - val_accuracy: 0.5854\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 3.0738 - accuracy: 0.9330 - val_loss: 4.7368 - val_accuracy: 0.5395\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 3.3399 - accuracy: 0.9171 - val_loss: 4.7541 - val_accuracy: 0.5851\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 2s 158us/sample - loss: 3.2326 - accuracy: 0.9348 - val_loss: 4.6510 - val_accuracy: 0.5534\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 3.0013 - accuracy: 0.9391 - val_loss: 4.3688 - val_accuracy: 0.5854\n",
      "15000 485000\n",
      "Model: \"model_21\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 16384)        0           conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 16384)        65536       flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)            (None, 16384)        0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_63 (Dense)                (None, 200)          3277000     dropout_63[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 200)          800         dense_63[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 200)          0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_64 (Dropout)            (None, 200)          0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_64 (Dense)                (None, 200)          40200       dropout_64[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 200)          800         dense_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 200)          0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)            (None, 200)          0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_65 (Dense)                (None, 10)           2010        dropout_65[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,473,002\n",
      "Trainable params: 3,438,026\n",
      "Non-trainable params: 34,976\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "15000/15000 [==============================] - 4s 248us/sample - loss: 5.6022 - accuracy: 0.3891 - val_loss: 6.5456 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 4.3161 - accuracy: 0.6047 - val_loss: 5.9900 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 3.8210 - accuracy: 0.6915 - val_loss: 6.3663 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 3.6743 - accuracy: 0.7413 - val_loss: 5.9263 - val_accuracy: 0.1049\n",
      "Epoch 5/50\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 3.5525 - accuracy: 0.7786 - val_loss: 6.2273 - val_accuracy: 0.1315\n",
      "Epoch 6/50\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 3.5351 - accuracy: 0.7984 - val_loss: 5.9199 - val_accuracy: 0.1927\n",
      "Epoch 7/50\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 3.5220 - accuracy: 0.8105 - val_loss: 5.2128 - val_accuracy: 0.3149\n",
      "Epoch 8/50\n",
      "15000/15000 [==============================] - 2s 148us/sample - loss: 3.4358 - accuracy: 0.8305 - val_loss: 4.4813 - val_accuracy: 0.4918\n",
      "Epoch 9/50\n",
      "15000/15000 [==============================] - 2s 149us/sample - loss: 3.4288 - accuracy: 0.8384 - val_loss: 4.1887 - val_accuracy: 0.6018\n",
      "Epoch 10/50\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 3.4787 - accuracy: 0.8382 - val_loss: 4.1926 - val_accuracy: 0.6201\n",
      "Epoch 11/50\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 3.3729 - accuracy: 0.8556 - val_loss: 4.1204 - val_accuracy: 0.6225\n",
      "Epoch 12/50\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 3.3794 - accuracy: 0.8594 - val_loss: 4.2615 - val_accuracy: 0.6002\n",
      "Epoch 13/50\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 3.3022 - accuracy: 0.8675 - val_loss: 4.3534 - val_accuracy: 0.5882\n",
      "Epoch 14/50\n",
      "15000/15000 [==============================] - 2s 142us/sample - loss: 3.3665 - accuracy: 0.8683 - val_loss: 4.2629 - val_accuracy: 0.6131\n",
      "Epoch 15/50\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 3.2714 - accuracy: 0.8763 - val_loss: 4.2314 - val_accuracy: 0.6052\n",
      "Epoch 16/50\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 3.3467 - accuracy: 0.8735 - val_loss: 4.2246 - val_accuracy: 0.6287\n",
      "Epoch 17/50\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 3.2542 - accuracy: 0.8826 - val_loss: 4.2539 - val_accuracy: 0.6089\n",
      "Epoch 18/50\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 3.2771 - accuracy: 0.8845 - val_loss: 4.2328 - val_accuracy: 0.6251\n",
      "Epoch 19/50\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 3.2725 - accuracy: 0.8897 - val_loss: 4.1904 - val_accuracy: 0.6232\n",
      "Epoch 20/50\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 3.1980 - accuracy: 0.8949 - val_loss: 4.3170 - val_accuracy: 0.6030\n",
      "Epoch 21/50\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 3.2941 - accuracy: 0.8886 - val_loss: 4.2256 - val_accuracy: 0.6177\n",
      "Epoch 22/50\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 3.1673 - accuracy: 0.8981 - val_loss: 4.2584 - val_accuracy: 0.6150\n",
      "Epoch 23/50\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 3.3317 - accuracy: 0.8915 - val_loss: 4.3967 - val_accuracy: 0.6145\n",
      "Epoch 24/50\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 3.3638 - accuracy: 0.8895 - val_loss: 4.3147 - val_accuracy: 0.6173\n",
      "Epoch 25/50\n",
      "15000/15000 [==============================] - 2s 147us/sample - loss: 3.1885 - accuracy: 0.9063 - val_loss: 4.2346 - val_accuracy: 0.6168\n",
      "Epoch 26/50\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 3.1552 - accuracy: 0.9059 - val_loss: 4.3437 - val_accuracy: 0.5989\n",
      "Epoch 27/50\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 3.2162 - accuracy: 0.9023 - val_loss: 4.3438 - val_accuracy: 0.6122\n",
      "Epoch 28/50\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 3.2157 - accuracy: 0.9023 - val_loss: 4.3319 - val_accuracy: 0.6171\n",
      "Epoch 29/50\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 3.1440 - accuracy: 0.9086 - val_loss: 4.3891 - val_accuracy: 0.5930\n",
      "Epoch 30/50\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 3.2076 - accuracy: 0.9049 - val_loss: 4.5399 - val_accuracy: 0.5805\n",
      "Epoch 31/50\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 3.0941 - accuracy: 0.9127 - val_loss: 4.3975 - val_accuracy: 0.6109\n",
      "Epoch 32/50\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 3.2136 - accuracy: 0.9081 - val_loss: 4.4098 - val_accuracy: 0.6110\n",
      "Epoch 33/50\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 3.1141 - accuracy: 0.9145 - val_loss: 4.2808 - val_accuracy: 0.6294\n",
      "Epoch 34/50\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 3.2473 - accuracy: 0.9061 - val_loss: 4.4163 - val_accuracy: 0.6049\n",
      "Epoch 35/50\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 3.1181 - accuracy: 0.9163 - val_loss: 4.2729 - val_accuracy: 0.6162\n",
      "Epoch 36/50\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 3.0460 - accuracy: 0.9141 - val_loss: 4.3224 - val_accuracy: 0.6141\n",
      "Epoch 37/50\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 3.2613 - accuracy: 0.9066 - val_loss: 4.3579 - val_accuracy: 0.6222\n",
      "Epoch 38/50\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 3.0918 - accuracy: 0.9193 - val_loss: 4.3476 - val_accuracy: 0.6101\n",
      "Epoch 39/50\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 3.1599 - accuracy: 0.9140 - val_loss: 4.3436 - val_accuracy: 0.6083\n",
      "Epoch 40/50\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 3.1977 - accuracy: 0.9123 - val_loss: 4.4510 - val_accuracy: 0.5873\n",
      "Epoch 41/50\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 3.1463 - accuracy: 0.9176 - val_loss: 4.3237 - val_accuracy: 0.6154\n",
      "Epoch 42/50\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 3.0596 - accuracy: 0.9221 - val_loss: 4.3282 - val_accuracy: 0.6067\n",
      "Epoch 43/50\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 3.1327 - accuracy: 0.9166 - val_loss: 4.2754 - val_accuracy: 0.6040\n",
      "Epoch 44/50\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 3.0063 - accuracy: 0.9251 - val_loss: 4.4001 - val_accuracy: 0.5912\n",
      "Epoch 45/50\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 3.0627 - accuracy: 0.9231 - val_loss: 4.4121 - val_accuracy: 0.5869\n",
      "Epoch 46/50\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 3.1223 - accuracy: 0.9167 - val_loss: 4.3130 - val_accuracy: 0.6220\n",
      "Epoch 47/50\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 3.1377 - accuracy: 0.9193 - val_loss: 4.3925 - val_accuracy: 0.6086\n",
      "Epoch 48/50\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 3.1710 - accuracy: 0.9171 - val_loss: 4.4581 - val_accuracy: 0.6049\n",
      "Epoch 49/50\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 2.9882 - accuracy: 0.9296 - val_loss: 4.3343 - val_accuracy: 0.5969\n",
      "Epoch 50/50\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 3.0238 - accuracy: 0.9238 - val_loss: 4.3591 - val_accuracy: 0.5932\n",
      "25000 475000\n",
      "Model: \"model_22\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 16384)        0           conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 16384)        65536       flatten_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_66 (Dropout)            (None, 16384)        0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_66 (Dense)                (None, 200)          3277000     dropout_66[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 200)          800         dense_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 200)          0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_67 (Dropout)            (None, 200)          0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_67 (Dense)                (None, 200)          40200       dropout_67[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 200)          800         dense_67[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 200)          0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_68 (Dropout)            (None, 200)          0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_68 (Dense)                (None, 10)           2010        dropout_68[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,473,002\n",
      "Trainable params: 3,438,026\n",
      "Non-trainable params: 34,976\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "25000/25000 [==============================] - 5s 190us/sample - loss: 5.1936 - accuracy: 0.4356 - val_loss: 6.5895 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "25000/25000 [==============================] - 3s 126us/sample - loss: 3.9627 - accuracy: 0.6144 - val_loss: 6.3456 - val_accuracy: 0.1391\n",
      "Epoch 3/50\n",
      "25000/25000 [==============================] - 3s 126us/sample - loss: 3.7176 - accuracy: 0.6738 - val_loss: 5.7111 - val_accuracy: 0.1307\n",
      "Epoch 4/50\n",
      "25000/25000 [==============================] - 3s 126us/sample - loss: 3.6409 - accuracy: 0.7097 - val_loss: 4.7212 - val_accuracy: 0.3472\n",
      "Epoch 5/50\n",
      "25000/25000 [==============================] - 3s 127us/sample - loss: 3.5776 - accuracy: 0.7392 - val_loss: 4.1112 - val_accuracy: 0.5538\n",
      "Epoch 6/50\n",
      "25000/25000 [==============================] - 3s 125us/sample - loss: 3.5529 - accuracy: 0.7525 - val_loss: 4.0102 - val_accuracy: 0.6179\n",
      "Epoch 7/50\n",
      "25000/25000 [==============================] - 3s 125us/sample - loss: 3.5083 - accuracy: 0.7728 - val_loss: 3.9958 - val_accuracy: 0.6242\n",
      "Epoch 8/50\n",
      "25000/25000 [==============================] - 3s 125us/sample - loss: 3.5013 - accuracy: 0.7832 - val_loss: 3.9189 - val_accuracy: 0.6501\n",
      "Epoch 9/50\n",
      "25000/25000 [==============================] - 3s 125us/sample - loss: 3.4515 - accuracy: 0.8010 - val_loss: 4.0066 - val_accuracy: 0.6371\n",
      "Epoch 10/50\n",
      "25000/25000 [==============================] - 3s 125us/sample - loss: 3.4521 - accuracy: 0.8080 - val_loss: 4.1484 - val_accuracy: 0.6289\n",
      "Epoch 11/50\n",
      "25000/25000 [==============================] - 3s 125us/sample - loss: 3.4459 - accuracy: 0.8110 - val_loss: 4.0354 - val_accuracy: 0.6412\n",
      "Epoch 12/50\n",
      "25000/25000 [==============================] - 3s 126us/sample - loss: 3.4384 - accuracy: 0.8230 - val_loss: 4.0673 - val_accuracy: 0.6554\n",
      "Epoch 13/50\n",
      "25000/25000 [==============================] - 3s 125us/sample - loss: 3.4189 - accuracy: 0.8306 - val_loss: 4.1061 - val_accuracy: 0.6474\n",
      "Epoch 14/50\n",
      "25000/25000 [==============================] - 3s 125us/sample - loss: 3.4181 - accuracy: 0.8362 - val_loss: 4.1873 - val_accuracy: 0.6339\n",
      "Epoch 15/50\n",
      "25000/25000 [==============================] - 3s 126us/sample - loss: 3.3979 - accuracy: 0.8403 - val_loss: 4.1026 - val_accuracy: 0.6444\n",
      "Epoch 16/50\n",
      "25000/25000 [==============================] - 3s 125us/sample - loss: 3.3920 - accuracy: 0.8453 - val_loss: 4.5767 - val_accuracy: 0.5907\n",
      "Epoch 17/50\n",
      "25000/25000 [==============================] - 3s 124us/sample - loss: 3.4019 - accuracy: 0.8494 - val_loss: 4.1849 - val_accuracy: 0.6437\n",
      "Epoch 18/50\n",
      "25000/25000 [==============================] - 3s 125us/sample - loss: 3.3887 - accuracy: 0.8522 - val_loss: 4.3312 - val_accuracy: 0.6150\n",
      "Epoch 19/50\n",
      "25000/25000 [==============================] - 3s 125us/sample - loss: 3.3618 - accuracy: 0.8566 - val_loss: 4.2051 - val_accuracy: 0.6488\n",
      "Epoch 20/50\n",
      "25000/25000 [==============================] - 3s 127us/sample - loss: 3.3539 - accuracy: 0.8592 - val_loss: 4.1983 - val_accuracy: 0.6408\n",
      "Epoch 21/50\n",
      "25000/25000 [==============================] - 3s 126us/sample - loss: 3.3712 - accuracy: 0.8593 - val_loss: 4.2094 - val_accuracy: 0.6483\n",
      "Epoch 22/50\n",
      "25000/25000 [==============================] - 3s 125us/sample - loss: 3.3500 - accuracy: 0.8658 - val_loss: 4.2414 - val_accuracy: 0.6483\n",
      "Epoch 23/50\n",
      "25000/25000 [==============================] - 3s 127us/sample - loss: 3.4020 - accuracy: 0.8645 - val_loss: 4.3254 - val_accuracy: 0.6226\n",
      "Epoch 24/50\n",
      "25000/25000 [==============================] - 3s 128us/sample - loss: 3.3646 - accuracy: 0.8688 - val_loss: 4.2415 - val_accuracy: 0.6432\n",
      "Epoch 25/50\n",
      "25000/25000 [==============================] - 3s 126us/sample - loss: 3.3612 - accuracy: 0.8720 - val_loss: 4.3674 - val_accuracy: 0.6237\n",
      "Epoch 26/50\n",
      "25000/25000 [==============================] - 3s 125us/sample - loss: 3.3501 - accuracy: 0.8742 - val_loss: 4.1461 - val_accuracy: 0.6518\n",
      "Epoch 27/50\n",
      "25000/25000 [==============================] - 3s 124us/sample - loss: 3.3229 - accuracy: 0.8756 - val_loss: 4.2988 - val_accuracy: 0.6368\n",
      "Epoch 28/50\n",
      "25000/25000 [==============================] - 3s 125us/sample - loss: 3.3546 - accuracy: 0.8738 - val_loss: 4.2417 - val_accuracy: 0.6430\n",
      "Epoch 29/50\n",
      "25000/25000 [==============================] - 3s 125us/sample - loss: 3.3049 - accuracy: 0.8814 - val_loss: 4.4970 - val_accuracy: 0.6111\n",
      "Epoch 30/50\n",
      "25000/25000 [==============================] - 3s 126us/sample - loss: 3.3427 - accuracy: 0.8771 - val_loss: 4.3541 - val_accuracy: 0.6230\n",
      "Epoch 31/50\n",
      "25000/25000 [==============================] - 3s 125us/sample - loss: 3.2670 - accuracy: 0.8850 - val_loss: 4.1454 - val_accuracy: 0.6495\n",
      "Epoch 32/50\n",
      "25000/25000 [==============================] - 3s 126us/sample - loss: 3.2857 - accuracy: 0.8821 - val_loss: 4.3512 - val_accuracy: 0.6167\n",
      "Epoch 33/50\n",
      "25000/25000 [==============================] - 3s 133us/sample - loss: 3.2747 - accuracy: 0.8850 - val_loss: 4.2133 - val_accuracy: 0.6410\n",
      "Epoch 34/50\n",
      "25000/25000 [==============================] - 3s 126us/sample - loss: 3.2272 - accuracy: 0.8893 - val_loss: 4.2863 - val_accuracy: 0.6425\n",
      "Epoch 35/50\n",
      "25000/25000 [==============================] - 3s 128us/sample - loss: 3.2941 - accuracy: 0.8833 - val_loss: 4.2746 - val_accuracy: 0.6304\n",
      "Epoch 36/50\n",
      "25000/25000 [==============================] - 3s 125us/sample - loss: 3.2500 - accuracy: 0.8896 - val_loss: 4.3351 - val_accuracy: 0.6417\n",
      "Epoch 37/50\n",
      "25000/25000 [==============================] - 3s 126us/sample - loss: 3.2170 - accuracy: 0.8903 - val_loss: 4.2062 - val_accuracy: 0.6482\n",
      "Epoch 38/50\n",
      "25000/25000 [==============================] - 3s 123us/sample - loss: 3.2145 - accuracy: 0.8906 - val_loss: 4.2369 - val_accuracy: 0.6446\n",
      "Epoch 39/50\n",
      "25000/25000 [==============================] - 3s 124us/sample - loss: 3.2242 - accuracy: 0.8916 - val_loss: 4.2875 - val_accuracy: 0.6395\n",
      "Epoch 40/50\n",
      "25000/25000 [==============================] - 3s 125us/sample - loss: 3.2384 - accuracy: 0.8893 - val_loss: 4.1834 - val_accuracy: 0.6358\n",
      "Epoch 41/50\n",
      "25000/25000 [==============================] - 3s 128us/sample - loss: 3.2003 - accuracy: 0.8941 - val_loss: 4.2113 - val_accuracy: 0.6502\n",
      "Epoch 42/50\n",
      "25000/25000 [==============================] - 3s 126us/sample - loss: 3.1863 - accuracy: 0.8919 - val_loss: 4.2683 - val_accuracy: 0.6406\n",
      "Epoch 43/50\n",
      "25000/25000 [==============================] - 3s 124us/sample - loss: 3.2165 - accuracy: 0.8922 - val_loss: 4.3543 - val_accuracy: 0.6248\n",
      "Epoch 44/50\n",
      "25000/25000 [==============================] - 3s 126us/sample - loss: 3.1839 - accuracy: 0.8928 - val_loss: 4.1728 - val_accuracy: 0.6455\n",
      "Epoch 45/50\n",
      "25000/25000 [==============================] - 3s 126us/sample - loss: 3.1432 - accuracy: 0.8980 - val_loss: 4.2412 - val_accuracy: 0.6271\n",
      "Epoch 46/50\n",
      "25000/25000 [==============================] - 3s 126us/sample - loss: 3.1498 - accuracy: 0.8999 - val_loss: 4.1353 - val_accuracy: 0.6426\n",
      "Epoch 47/50\n",
      "25000/25000 [==============================] - 3s 125us/sample - loss: 3.0919 - accuracy: 0.9015 - val_loss: 4.2767 - val_accuracy: 0.6361\n",
      "Epoch 48/50\n",
      "25000/25000 [==============================] - 3s 128us/sample - loss: 3.1359 - accuracy: 0.8999 - val_loss: 4.1739 - val_accuracy: 0.6545\n",
      "Epoch 49/50\n",
      "25000/25000 [==============================] - 3s 126us/sample - loss: 3.1381 - accuracy: 0.9012 - val_loss: 4.2353 - val_accuracy: 0.6387\n",
      "Epoch 50/50\n",
      "25000/25000 [==============================] - 3s 125us/sample - loss: 3.0754 - accuracy: 0.9038 - val_loss: 4.2253 - val_accuracy: 0.6242\n",
      "50000 450000\n",
      "Model: \"model_23\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 16384)        0           conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 16384)        65536       flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_69 (Dropout)            (None, 16384)        0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_69 (Dense)                (None, 200)          3277000     dropout_69[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 200)          800         dense_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 200)          0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_70 (Dropout)            (None, 200)          0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_70 (Dense)                (None, 200)          40200       dropout_70[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 200)          800         dense_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 200)          0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_71 (Dropout)            (None, 200)          0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_71 (Dense)                (None, 10)           2010        dropout_71[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,473,002\n",
      "Trainable params: 3,438,026\n",
      "Non-trainable params: 34,976\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 7s 143us/sample - loss: 4.6272 - accuracy: 0.4847 - val_loss: 5.7316 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 6s 113us/sample - loss: 3.6161 - accuracy: 0.6169 - val_loss: 5.0064 - val_accuracy: 0.2092\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 6s 113us/sample - loss: 3.4576 - accuracy: 0.6568 - val_loss: 3.4289 - val_accuracy: 0.6488\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 6s 111us/sample - loss: 3.3802 - accuracy: 0.6808 - val_loss: 3.3426 - val_accuracy: 0.6801\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 6s 113us/sample - loss: 3.3367 - accuracy: 0.6979 - val_loss: 3.3567 - val_accuracy: 0.6773\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 6s 113us/sample - loss: 3.3163 - accuracy: 0.7105 - val_loss: 3.4044 - val_accuracy: 0.6845\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 6s 113us/sample - loss: 3.3394 - accuracy: 0.7247 - val_loss: 3.4859 - val_accuracy: 0.6903\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 6s 113us/sample - loss: 3.3514 - accuracy: 0.7361 - val_loss: 3.4962 - val_accuracy: 0.6793\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 6s 112us/sample - loss: 3.3544 - accuracy: 0.7475 - val_loss: 3.6016 - val_accuracy: 0.6771\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 6s 113us/sample - loss: 3.3528 - accuracy: 0.7526 - val_loss: 3.5235 - val_accuracy: 0.6988\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 6s 112us/sample - loss: 3.3567 - accuracy: 0.7635 - val_loss: 3.5124 - val_accuracy: 0.6950\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 6s 112us/sample - loss: 3.3487 - accuracy: 0.7694 - val_loss: 3.6412 - val_accuracy: 0.6828\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 6s 112us/sample - loss: 3.3527 - accuracy: 0.7752 - val_loss: 3.6731 - val_accuracy: 0.6744\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 6s 112us/sample - loss: 3.3121 - accuracy: 0.7860 - val_loss: 3.6362 - val_accuracy: 0.6920\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 6s 113us/sample - loss: 3.3454 - accuracy: 0.7862 - val_loss: 3.5779 - val_accuracy: 0.7008\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 6s 111us/sample - loss: 3.3187 - accuracy: 0.7912 - val_loss: 3.6850 - val_accuracy: 0.6863\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 6s 112us/sample - loss: 3.3265 - accuracy: 0.7974 - val_loss: 3.7495 - val_accuracy: 0.6780\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 6s 116us/sample - loss: 3.3155 - accuracy: 0.8005 - val_loss: 3.6698 - val_accuracy: 0.6948\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 6s 117us/sample - loss: 3.2825 - accuracy: 0.8044 - val_loss: 3.6980 - val_accuracy: 0.6743\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 6s 112us/sample - loss: 3.2793 - accuracy: 0.8091 - val_loss: 3.6606 - val_accuracy: 0.6994\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 6s 113us/sample - loss: 3.2715 - accuracy: 0.8112 - val_loss: 3.7273 - val_accuracy: 0.6842\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 6s 113us/sample - loss: 3.2838 - accuracy: 0.8114 - val_loss: 3.6902 - val_accuracy: 0.6887\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 6s 114us/sample - loss: 3.2514 - accuracy: 0.8169 - val_loss: 3.7002 - val_accuracy: 0.6914\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 6s 113us/sample - loss: 3.2631 - accuracy: 0.8169 - val_loss: 3.7619 - val_accuracy: 0.6831\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 6s 113us/sample - loss: 3.2465 - accuracy: 0.8244 - val_loss: 3.6459 - val_accuracy: 0.6978\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 6s 112us/sample - loss: 3.2166 - accuracy: 0.8245 - val_loss: 3.6389 - val_accuracy: 0.7008\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 6s 112us/sample - loss: 3.2171 - accuracy: 0.8243 - val_loss: 3.7686 - val_accuracy: 0.6848\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 6s 113us/sample - loss: 3.1897 - accuracy: 0.8287 - val_loss: 3.7016 - val_accuracy: 0.6933\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 6s 114us/sample - loss: 3.1971 - accuracy: 0.8308 - val_loss: 3.6554 - val_accuracy: 0.6981\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 6s 112us/sample - loss: 3.1546 - accuracy: 0.8372 - val_loss: 3.6334 - val_accuracy: 0.6925\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 6s 114us/sample - loss: 3.1812 - accuracy: 0.8325 - val_loss: 3.6658 - val_accuracy: 0.6894\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 6s 113us/sample - loss: 3.1492 - accuracy: 0.8393 - val_loss: 3.6609 - val_accuracy: 0.7062\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 6s 110us/sample - loss: 3.1318 - accuracy: 0.8383 - val_loss: 3.7996 - val_accuracy: 0.6489\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 6s 112us/sample - loss: 3.1547 - accuracy: 0.8371 - val_loss: 3.6284 - val_accuracy: 0.7024\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 6s 114us/sample - loss: 3.1171 - accuracy: 0.8410 - val_loss: 3.6885 - val_accuracy: 0.6821\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 6s 113us/sample - loss: 3.0991 - accuracy: 0.8415 - val_loss: 3.5993 - val_accuracy: 0.7066\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 6s 112us/sample - loss: 3.0689 - accuracy: 0.8461 - val_loss: 3.5508 - val_accuracy: 0.7039\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 6s 112us/sample - loss: 3.0533 - accuracy: 0.8438 - val_loss: 3.6970 - val_accuracy: 0.6736\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 6s 112us/sample - loss: 3.0576 - accuracy: 0.8451 - val_loss: 3.7684 - val_accuracy: 0.6488\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 6s 111us/sample - loss: 3.0397 - accuracy: 0.8476 - val_loss: 3.6156 - val_accuracy: 0.6869\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 6s 112us/sample - loss: 3.0297 - accuracy: 0.8483 - val_loss: 3.5484 - val_accuracy: 0.6993\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 6s 113us/sample - loss: 3.0213 - accuracy: 0.8496 - val_loss: 3.5793 - val_accuracy: 0.6972\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 6s 112us/sample - loss: 3.0207 - accuracy: 0.8506 - val_loss: 3.6190 - val_accuracy: 0.6925\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 6s 110us/sample - loss: 2.9780 - accuracy: 0.8546 - val_loss: 3.6494 - val_accuracy: 0.6738\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 6s 111us/sample - loss: 2.9770 - accuracy: 0.8533 - val_loss: 3.5449 - val_accuracy: 0.6986\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 6s 112us/sample - loss: 2.9513 - accuracy: 0.8556 - val_loss: 3.5650 - val_accuracy: 0.6866\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 5s 109us/sample - loss: 2.9355 - accuracy: 0.8551 - val_loss: 3.5287 - val_accuracy: 0.7025\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 6s 111us/sample - loss: 2.9282 - accuracy: 0.8586 - val_loss: 3.4864 - val_accuracy: 0.7010\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 6s 114us/sample - loss: 2.9360 - accuracy: 0.8591 - val_loss: 3.5377 - val_accuracy: 0.6982\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 6s 112us/sample - loss: 2.9403 - accuracy: 0.8557 - val_loss: 3.5434 - val_accuracy: 0.6853\n",
      "125000 375000\n",
      "Model: \"model_24\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_24 (Flatten)            (None, 16384)        0           conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 16384)        65536       flatten_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_72 (Dropout)            (None, 16384)        0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_72 (Dense)                (None, 200)          3277000     dropout_72[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 200)          800         dense_72[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 200)          0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, 200)          0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_73 (Dense)                (None, 200)          40200       dropout_73[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 200)          800         dense_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 200)          0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_74 (Dropout)            (None, 200)          0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_74 (Dense)                (None, 10)           2010        dropout_74[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,473,002\n",
      "Trainable params: 3,438,026\n",
      "Non-trainable params: 34,976\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 125000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "125000/125000 [==============================] - 15s 121us/sample - loss: 4.0142 - accuracy: 0.5319 - val_loss: 3.3890 - val_accuracy: 0.5786\n",
      "Epoch 2/50\n",
      "125000/125000 [==============================] - 13s 107us/sample - loss: 3.2527 - accuracy: 0.6199 - val_loss: 3.0300 - val_accuracy: 0.6711\n",
      "Epoch 3/50\n",
      "125000/125000 [==============================] - 13s 107us/sample - loss: 3.1853 - accuracy: 0.6450 - val_loss: 3.0289 - val_accuracy: 0.7097\n",
      "Epoch 4/50\n",
      "125000/125000 [==============================] - 13s 107us/sample - loss: 3.1872 - accuracy: 0.6638 - val_loss: 3.1911 - val_accuracy: 0.6677\n",
      "Epoch 5/50\n",
      "125000/125000 [==============================] - 13s 108us/sample - loss: 3.1854 - accuracy: 0.6774 - val_loss: 3.0335 - val_accuracy: 0.7207\n",
      "Epoch 6/50\n",
      "125000/125000 [==============================] - 14s 108us/sample - loss: 3.1758 - accuracy: 0.6890 - val_loss: 3.0691 - val_accuracy: 0.7327\n",
      "Epoch 7/50\n",
      "125000/125000 [==============================] - 14s 109us/sample - loss: 3.1543 - accuracy: 0.6990 - val_loss: 3.0295 - val_accuracy: 0.7310\n",
      "Epoch 8/50\n",
      "125000/125000 [==============================] - 14s 109us/sample - loss: 3.1189 - accuracy: 0.7054 - val_loss: 3.0899 - val_accuracy: 0.7178\n",
      "Epoch 9/50\n",
      "125000/125000 [==============================] - 14s 109us/sample - loss: 3.1062 - accuracy: 0.7119 - val_loss: 2.9545 - val_accuracy: 0.7387\n",
      "Epoch 10/50\n",
      "125000/125000 [==============================] - 13s 107us/sample - loss: 3.0740 - accuracy: 0.7192 - val_loss: 2.9560 - val_accuracy: 0.7493\n",
      "Epoch 11/50\n",
      "125000/125000 [==============================] - 14s 108us/sample - loss: 3.0509 - accuracy: 0.7234 - val_loss: 3.0245 - val_accuracy: 0.7263\n",
      "Epoch 12/50\n",
      "125000/125000 [==============================] - 13s 106us/sample - loss: 3.0443 - accuracy: 0.7297 - val_loss: 3.0665 - val_accuracy: 0.7154\n",
      "Epoch 13/50\n",
      "125000/125000 [==============================] - 13s 107us/sample - loss: 3.0089 - accuracy: 0.7339 - val_loss: 2.9974 - val_accuracy: 0.7214\n",
      "Epoch 14/50\n",
      "125000/125000 [==============================] - 13s 107us/sample - loss: 2.9871 - accuracy: 0.7390 - val_loss: 2.9768 - val_accuracy: 0.7305\n",
      "Epoch 15/50\n",
      "125000/125000 [==============================] - 13s 107us/sample - loss: 2.9523 - accuracy: 0.7423 - val_loss: 3.1428 - val_accuracy: 0.6783\n",
      "Epoch 16/50\n",
      "125000/125000 [==============================] - 13s 108us/sample - loss: 2.9299 - accuracy: 0.7465 - val_loss: 2.9757 - val_accuracy: 0.7280\n",
      "Epoch 17/50\n",
      "125000/125000 [==============================] - 13s 107us/sample - loss: 2.9082 - accuracy: 0.7480 - val_loss: 2.8451 - val_accuracy: 0.7491\n",
      "Epoch 18/50\n",
      "125000/125000 [==============================] - 14s 108us/sample - loss: 2.8791 - accuracy: 0.7527 - val_loss: 3.0071 - val_accuracy: 0.7207\n",
      "Epoch 19/50\n",
      "125000/125000 [==============================] - 14s 109us/sample - loss: 2.8663 - accuracy: 0.7550 - val_loss: 2.8265 - val_accuracy: 0.7437\n",
      "Epoch 20/50\n",
      "125000/125000 [==============================] - 14s 109us/sample - loss: 2.8348 - accuracy: 0.7570 - val_loss: 2.8223 - val_accuracy: 0.7476\n",
      "Epoch 21/50\n",
      "125000/125000 [==============================] - 13s 107us/sample - loss: 2.8121 - accuracy: 0.7595 - val_loss: 2.8686 - val_accuracy: 0.7456\n",
      "Epoch 22/50\n",
      "125000/125000 [==============================] - 13s 108us/sample - loss: 2.7815 - accuracy: 0.7639 - val_loss: 2.7777 - val_accuracy: 0.7537\n",
      "Epoch 23/50\n",
      "125000/125000 [==============================] - 14s 108us/sample - loss: 2.7573 - accuracy: 0.7644 - val_loss: 2.8980 - val_accuracy: 0.7239\n",
      "Epoch 24/50\n",
      "125000/125000 [==============================] - 14s 108us/sample - loss: 2.7481 - accuracy: 0.7671 - val_loss: 2.8775 - val_accuracy: 0.7281\n",
      "Epoch 25/50\n",
      "125000/125000 [==============================] - 13s 107us/sample - loss: 2.7330 - accuracy: 0.7696 - val_loss: 2.8832 - val_accuracy: 0.7225\n",
      "Epoch 26/50\n",
      "125000/125000 [==============================] - 13s 107us/sample - loss: 2.7188 - accuracy: 0.7719 - val_loss: 2.7604 - val_accuracy: 0.7544\n",
      "Epoch 27/50\n",
      "125000/125000 [==============================] - 13s 107us/sample - loss: 2.7019 - accuracy: 0.7729 - val_loss: 2.8070 - val_accuracy: 0.7430\n",
      "Epoch 28/50\n",
      "125000/125000 [==============================] - 13s 107us/sample - loss: 2.6736 - accuracy: 0.7752 - val_loss: 2.7539 - val_accuracy: 0.7549\n",
      "Epoch 29/50\n",
      "125000/125000 [==============================] - 13s 108us/sample - loss: 2.6635 - accuracy: 0.7762 - val_loss: 2.7612 - val_accuracy: 0.7491\n",
      "Epoch 30/50\n",
      "125000/125000 [==============================] - 14s 108us/sample - loss: 2.6302 - accuracy: 0.7771 - val_loss: 2.7197 - val_accuracy: 0.7460\n",
      "Epoch 31/50\n",
      "125000/125000 [==============================] - 14s 109us/sample - loss: 2.6016 - accuracy: 0.7807 - val_loss: 2.7604 - val_accuracy: 0.7289\n",
      "Epoch 32/50\n",
      "125000/125000 [==============================] - 13s 108us/sample - loss: 2.5902 - accuracy: 0.7805 - val_loss: 2.6770 - val_accuracy: 0.7528\n",
      "Epoch 33/50\n",
      "125000/125000 [==============================] - 13s 108us/sample - loss: 2.5701 - accuracy: 0.7829 - val_loss: 2.7291 - val_accuracy: 0.7415\n",
      "Epoch 34/50\n",
      "125000/125000 [==============================] - 13s 108us/sample - loss: 2.5618 - accuracy: 0.7845 - val_loss: 2.7365 - val_accuracy: 0.7204\n",
      "Epoch 35/50\n",
      "125000/125000 [==============================] - 14s 108us/sample - loss: 2.5369 - accuracy: 0.7863 - val_loss: 2.6430 - val_accuracy: 0.7526\n",
      "Epoch 36/50\n",
      "125000/125000 [==============================] - 14s 108us/sample - loss: 2.5282 - accuracy: 0.7884 - val_loss: 2.6826 - val_accuracy: 0.7460\n",
      "Epoch 37/50\n",
      "125000/125000 [==============================] - 13s 108us/sample - loss: 2.5188 - accuracy: 0.7880 - val_loss: 2.6669 - val_accuracy: 0.7383\n",
      "Epoch 38/50\n",
      "125000/125000 [==============================] - 13s 107us/sample - loss: 2.4992 - accuracy: 0.7907 - val_loss: 2.6359 - val_accuracy: 0.7461\n",
      "Epoch 39/50\n",
      "125000/125000 [==============================] - 13s 107us/sample - loss: 2.4756 - accuracy: 0.7922 - val_loss: 2.7282 - val_accuracy: 0.7273\n",
      "Epoch 40/50\n",
      "125000/125000 [==============================] - 13s 107us/sample - loss: 2.4683 - accuracy: 0.7923 - val_loss: 2.6478 - val_accuracy: 0.7445\n",
      "Epoch 41/50\n",
      "125000/125000 [==============================] - 13s 107us/sample - loss: 2.4552 - accuracy: 0.7942 - val_loss: 2.6861 - val_accuracy: 0.7295\n",
      "Epoch 42/50\n",
      "125000/125000 [==============================] - 13s 108us/sample - loss: 2.4460 - accuracy: 0.7929 - val_loss: 2.6101 - val_accuracy: 0.7464\n",
      "Epoch 43/50\n",
      "125000/125000 [==============================] - 13s 107us/sample - loss: 2.4217 - accuracy: 0.7972 - val_loss: 2.5383 - val_accuracy: 0.7537\n",
      "Epoch 44/50\n",
      "125000/125000 [==============================] - 13s 108us/sample - loss: 2.4092 - accuracy: 0.7968 - val_loss: 2.6276 - val_accuracy: 0.7339\n",
      "Epoch 45/50\n",
      "125000/125000 [==============================] - 14s 112us/sample - loss: 2.3964 - accuracy: 0.7962 - val_loss: 2.5524 - val_accuracy: 0.7520\n",
      "Epoch 46/50\n",
      "125000/125000 [==============================] - 14s 108us/sample - loss: 2.3862 - accuracy: 0.7990 - val_loss: 2.6219 - val_accuracy: 0.7258\n",
      "Epoch 47/50\n",
      "125000/125000 [==============================] - 14s 108us/sample - loss: 2.3758 - accuracy: 0.8007 - val_loss: 2.5357 - val_accuracy: 0.7379\n",
      "Epoch 48/50\n",
      "125000/125000 [==============================] - 14s 109us/sample - loss: 2.3488 - accuracy: 0.8014 - val_loss: 2.5117 - val_accuracy: 0.7499\n",
      "Epoch 49/50\n",
      "125000/125000 [==============================] - 13s 107us/sample - loss: 2.3538 - accuracy: 0.8013 - val_loss: 2.4992 - val_accuracy: 0.7444\n",
      "Epoch 50/50\n",
      "125000/125000 [==============================] - 13s 107us/sample - loss: 2.3344 - accuracy: 0.8042 - val_loss: 2.5495 - val_accuracy: 0.7518\n",
      "250000 250000\n",
      "Model: \"model_25\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 16384)        0           conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 16384)        65536       flatten_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_75 (Dropout)            (None, 16384)        0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_75 (Dense)                (None, 200)          3277000     dropout_75[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 200)          800         dense_75[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 200)          0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_76 (Dropout)            (None, 200)          0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_76 (Dense)                (None, 200)          40200       dropout_76[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 200)          800         dense_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 200)          0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_77 (Dropout)            (None, 200)          0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_77 (Dense)                (None, 10)           2010        dropout_77[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,616,106\n",
      "Trainable params: 3,579,594\n",
      "Non-trainable params: 36,512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 250000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "250000/250000 [==============================] - 38s 152us/sample - loss: 3.0571 - accuracy: 0.5719 - val_loss: 2.3302 - val_accuracy: 0.6831\n",
      "Epoch 2/50\n",
      "250000/250000 [==============================] - 36s 143us/sample - loss: 2.3421 - accuracy: 0.6859 - val_loss: 2.2572 - val_accuracy: 0.7171\n",
      "Epoch 3/50\n",
      "250000/250000 [==============================] - 36s 143us/sample - loss: 2.2309 - accuracy: 0.7283 - val_loss: 2.1578 - val_accuracy: 0.7523\n",
      "Epoch 4/50\n",
      "250000/250000 [==============================] - 36s 146us/sample - loss: 2.1390 - accuracy: 0.7581 - val_loss: 2.0911 - val_accuracy: 0.7661\n",
      "Epoch 5/50\n",
      "250000/250000 [==============================] - 36s 144us/sample - loss: 2.0536 - accuracy: 0.7775 - val_loss: 2.0947 - val_accuracy: 0.7650\n",
      "Epoch 6/50\n",
      "250000/250000 [==============================] - 36s 145us/sample - loss: 1.9654 - accuracy: 0.7945 - val_loss: 2.0253 - val_accuracy: 0.7709\n",
      "Epoch 7/50\n",
      "250000/250000 [==============================] - 36s 145us/sample - loss: 1.8938 - accuracy: 0.8081 - val_loss: 2.0460 - val_accuracy: 0.7563\n",
      "Epoch 8/50\n",
      "250000/250000 [==============================] - 36s 143us/sample - loss: 1.8246 - accuracy: 0.8211 - val_loss: 1.9894 - val_accuracy: 0.7602\n",
      "Epoch 9/50\n",
      "250000/250000 [==============================] - 36s 143us/sample - loss: 1.7530 - accuracy: 0.8328 - val_loss: 2.0919 - val_accuracy: 0.7383\n",
      "Epoch 10/50\n",
      "250000/250000 [==============================] - 36s 143us/sample - loss: 1.6928 - accuracy: 0.8406 - val_loss: 1.8230 - val_accuracy: 0.7789\n",
      "Epoch 11/50\n",
      "250000/250000 [==============================] - 36s 142us/sample - loss: 1.6269 - accuracy: 0.8502 - val_loss: 1.8503 - val_accuracy: 0.7805\n",
      "Epoch 12/50\n",
      "250000/250000 [==============================] - 35s 142us/sample - loss: 1.5626 - accuracy: 0.8565 - val_loss: 1.9459 - val_accuracy: 0.7576\n",
      "Epoch 13/50\n",
      "250000/250000 [==============================] - 36s 143us/sample - loss: 1.5141 - accuracy: 0.8639 - val_loss: 1.8337 - val_accuracy: 0.7674\n",
      "Epoch 14/50\n",
      "250000/250000 [==============================] - 36s 144us/sample - loss: 1.4595 - accuracy: 0.8695 - val_loss: 1.8432 - val_accuracy: 0.7714\n",
      "Epoch 15/50\n",
      "250000/250000 [==============================] - 36s 144us/sample - loss: 1.4137 - accuracy: 0.8750 - val_loss: 1.8674 - val_accuracy: 0.7560\n",
      "Epoch 16/50\n",
      "250000/250000 [==============================] - 36s 144us/sample - loss: 1.3712 - accuracy: 0.8791 - val_loss: 1.8337 - val_accuracy: 0.7590\n",
      "Epoch 17/50\n",
      "250000/250000 [==============================] - 36s 144us/sample - loss: 1.3258 - accuracy: 0.8838 - val_loss: 1.7260 - val_accuracy: 0.7745\n",
      "Epoch 18/50\n",
      "250000/250000 [==============================] - 36s 142us/sample - loss: 1.2935 - accuracy: 0.8878 - val_loss: 1.6707 - val_accuracy: 0.7845\n",
      "Epoch 19/50\n",
      "250000/250000 [==============================] - 35s 142us/sample - loss: 1.2597 - accuracy: 0.8919 - val_loss: 1.7302 - val_accuracy: 0.7696\n",
      "Epoch 20/50\n",
      "250000/250000 [==============================] - 36s 142us/sample - loss: 1.2284 - accuracy: 0.8955 - val_loss: 1.6560 - val_accuracy: 0.7795\n",
      "Epoch 21/50\n",
      "250000/250000 [==============================] - 36s 145us/sample - loss: 1.2004 - accuracy: 0.8991 - val_loss: 1.9668 - val_accuracy: 0.7024\n",
      "Epoch 22/50\n",
      "250000/250000 [==============================] - 36s 144us/sample - loss: 1.1751 - accuracy: 0.9018 - val_loss: 1.8915 - val_accuracy: 0.7527\n",
      "Epoch 23/50\n",
      "250000/250000 [==============================] - 36s 145us/sample - loss: 1.1505 - accuracy: 0.9048 - val_loss: 1.7107 - val_accuracy: 0.7725\n",
      "Epoch 24/50\n",
      "250000/250000 [==============================] - 36s 145us/sample - loss: 1.1396 - accuracy: 0.9067 - val_loss: 1.6850 - val_accuracy: 0.7692\n",
      "Epoch 25/50\n",
      "250000/250000 [==============================] - 36s 143us/sample - loss: 1.1130 - accuracy: 0.9098 - val_loss: 1.7484 - val_accuracy: 0.7576\n",
      "Epoch 26/50\n",
      "250000/250000 [==============================] - 36s 144us/sample - loss: 1.0969 - accuracy: 0.9119 - val_loss: 1.7266 - val_accuracy: 0.7512\n",
      "Epoch 27/50\n",
      "250000/250000 [==============================] - 36s 143us/sample - loss: 1.0750 - accuracy: 0.9144 - val_loss: 1.7640 - val_accuracy: 0.7619\n",
      "Epoch 28/50\n",
      "250000/250000 [==============================] - 36s 143us/sample - loss: 1.0725 - accuracy: 0.9157 - val_loss: 1.7039 - val_accuracy: 0.7721\n",
      "Epoch 29/50\n",
      "250000/250000 [==============================] - 36s 144us/sample - loss: 1.0433 - accuracy: 0.9185 - val_loss: 1.7155 - val_accuracy: 0.7652\n",
      "Epoch 30/50\n",
      "250000/250000 [==============================] - 36s 143us/sample - loss: 1.0384 - accuracy: 0.9192 - val_loss: 1.7915 - val_accuracy: 0.7506\n",
      "Epoch 31/50\n",
      "250000/250000 [==============================] - 36s 143us/sample - loss: 1.0269 - accuracy: 0.9220 - val_loss: 1.6952 - val_accuracy: 0.7711\n",
      "Epoch 32/50\n",
      "250000/250000 [==============================] - 36s 146us/sample - loss: 1.0141 - accuracy: 0.9234 - val_loss: 1.6449 - val_accuracy: 0.7701\n",
      "Epoch 33/50\n",
      "250000/250000 [==============================] - 36s 144us/sample - loss: 0.9987 - accuracy: 0.9251 - val_loss: 1.7296 - val_accuracy: 0.7696\n",
      "Epoch 34/50\n",
      "250000/250000 [==============================] - 36s 143us/sample - loss: 0.9950 - accuracy: 0.9262 - val_loss: 1.6501 - val_accuracy: 0.7795\n",
      "Epoch 35/50\n",
      "250000/250000 [==============================] - 36s 142us/sample - loss: 0.9786 - accuracy: 0.9273 - val_loss: 1.7313 - val_accuracy: 0.7613\n",
      "Epoch 36/50\n",
      "250000/250000 [==============================] - 36s 144us/sample - loss: 0.9692 - accuracy: 0.9297 - val_loss: 1.7342 - val_accuracy: 0.7643\n",
      "Epoch 37/50\n",
      "250000/250000 [==============================] - 36s 144us/sample - loss: 0.9599 - accuracy: 0.9306 - val_loss: 1.7788 - val_accuracy: 0.7608\n",
      "Epoch 38/50\n",
      "250000/250000 [==============================] - 36s 144us/sample - loss: 0.9513 - accuracy: 0.9310 - val_loss: 1.7223 - val_accuracy: 0.7695\n",
      "Epoch 39/50\n",
      "250000/250000 [==============================] - 36s 144us/sample - loss: 0.9428 - accuracy: 0.9327 - val_loss: 1.6438 - val_accuracy: 0.7767\n",
      "Epoch 40/50\n",
      "250000/250000 [==============================] - 36s 144us/sample - loss: 0.9379 - accuracy: 0.9336 - val_loss: 1.7158 - val_accuracy: 0.7556\n",
      "Epoch 41/50\n",
      "250000/250000 [==============================] - 36s 143us/sample - loss: 0.9313 - accuracy: 0.9342 - val_loss: 1.7404 - val_accuracy: 0.7624\n",
      "Epoch 42/50\n",
      "250000/250000 [==============================] - 36s 145us/sample - loss: 0.9223 - accuracy: 0.9359 - val_loss: 1.6912 - val_accuracy: 0.7720\n",
      "Epoch 43/50\n",
      "250000/250000 [==============================] - 36s 145us/sample - loss: 0.9169 - accuracy: 0.9366 - val_loss: 1.6227 - val_accuracy: 0.7769\n",
      "Epoch 44/50\n",
      "250000/250000 [==============================] - 36s 144us/sample - loss: 0.9111 - accuracy: 0.9382 - val_loss: 1.6798 - val_accuracy: 0.7665\n",
      "Epoch 45/50\n",
      "250000/250000 [==============================] - 36s 143us/sample - loss: 0.9008 - accuracy: 0.9391 - val_loss: 1.7283 - val_accuracy: 0.7606\n",
      "Epoch 46/50\n",
      "250000/250000 [==============================] - 36s 146us/sample - loss: 0.8968 - accuracy: 0.9403 - val_loss: 1.7231 - val_accuracy: 0.7681\n",
      "Epoch 47/50\n",
      "250000/250000 [==============================] - 36s 143us/sample - loss: 0.8997 - accuracy: 0.9398 - val_loss: 1.7379 - val_accuracy: 0.7571\n",
      "Epoch 48/50\n",
      "250000/250000 [==============================] - 36s 143us/sample - loss: 0.8876 - accuracy: 0.9419 - val_loss: 1.7179 - val_accuracy: 0.7675\n",
      "Epoch 49/50\n",
      "250000/250000 [==============================] - 36s 144us/sample - loss: 0.8834 - accuracy: 0.9425 - val_loss: 1.6719 - val_accuracy: 0.7710\n",
      "Epoch 50/50\n",
      "250000/250000 [==============================] - 36s 144us/sample - loss: 0.8742 - accuracy: 0.9438 - val_loss: 1.6850 - val_accuracy: 0.7709\n",
      "500000 0\n",
      "Model: \"model_26\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_26 (Flatten)            (None, 16384)        0           conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 16384)        65536       flatten_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_78 (Dropout)            (None, 16384)        0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_78 (Dense)                (None, 200)          3277000     dropout_78[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 200)          800         dense_78[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 200)          0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_79 (Dropout)            (None, 200)          0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_79 (Dense)                (None, 200)          40200       dropout_79[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 200)          800         dense_79[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 200)          0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_80 (Dropout)            (None, 200)          0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_80 (Dense)                (None, 10)           2010        dropout_80[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,616,106\n",
      "Trainable params: 3,579,594\n",
      "Non-trainable params: 36,512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "500000/500000 [==============================] - 72s 144us/sample - loss: 2.6919 - accuracy: 0.6198 - val_loss: 2.0629 - val_accuracy: 0.7419\n",
      "Epoch 2/50\n",
      "500000/500000 [==============================] - 70s 139us/sample - loss: 2.1792 - accuracy: 0.7209 - val_loss: 2.0608 - val_accuracy: 0.7549\n",
      "Epoch 3/50\n",
      "500000/500000 [==============================] - 70s 140us/sample - loss: 2.0334 - accuracy: 0.7567 - val_loss: 1.9546 - val_accuracy: 0.7736\n",
      "Epoch 4/50\n",
      "500000/500000 [==============================] - 69s 137us/sample - loss: 1.8989 - accuracy: 0.7804 - val_loss: 1.7861 - val_accuracy: 0.7974\n",
      "Epoch 5/50\n",
      "500000/500000 [==============================] - 69s 138us/sample - loss: 1.7734 - accuracy: 0.7976 - val_loss: 1.7892 - val_accuracy: 0.7821\n",
      "Epoch 6/50\n",
      "500000/500000 [==============================] - 69s 137us/sample - loss: 1.6626 - accuracy: 0.8100 - val_loss: 1.6954 - val_accuracy: 0.7954\n",
      "Epoch 7/50\n",
      "500000/500000 [==============================] - 69s 138us/sample - loss: 1.5600 - accuracy: 0.8218 - val_loss: 1.5882 - val_accuracy: 0.7960\n",
      "Epoch 8/50\n",
      "500000/500000 [==============================] - 69s 139us/sample - loss: 1.4744 - accuracy: 0.8314 - val_loss: 1.5560 - val_accuracy: 0.7897\n",
      "Epoch 9/50\n",
      "500000/500000 [==============================] - 70s 140us/sample - loss: 1.4002 - accuracy: 0.8382 - val_loss: 1.5697 - val_accuracy: 0.7849\n",
      "Epoch 10/50\n",
      "500000/500000 [==============================] - 69s 138us/sample - loss: 1.3431 - accuracy: 0.8444 - val_loss: 1.4701 - val_accuracy: 0.8013\n",
      "Epoch 11/50\n",
      "500000/500000 [==============================] - 70s 140us/sample - loss: 1.2887 - accuracy: 0.8505 - val_loss: 1.4234 - val_accuracy: 0.8046\n",
      "Epoch 12/50\n",
      "500000/500000 [==============================] - 70s 140us/sample - loss: 1.2490 - accuracy: 0.8554 - val_loss: 1.4334 - val_accuracy: 0.7912\n",
      "Epoch 13/50\n",
      "500000/500000 [==============================] - 70s 140us/sample - loss: 1.2128 - accuracy: 0.8605 - val_loss: 1.3933 - val_accuracy: 0.8024\n",
      "Epoch 14/50\n",
      "500000/500000 [==============================] - 70s 139us/sample - loss: 1.1807 - accuracy: 0.8656 - val_loss: 1.3260 - val_accuracy: 0.8163\n",
      "Epoch 15/50\n",
      "500000/500000 [==============================] - 70s 140us/sample - loss: 1.1535 - accuracy: 0.8694 - val_loss: 1.3442 - val_accuracy: 0.8120\n",
      "Epoch 16/50\n",
      "500000/500000 [==============================] - 69s 139us/sample - loss: 1.1310 - accuracy: 0.8727 - val_loss: 1.3360 - val_accuracy: 0.8222\n",
      "Epoch 17/50\n",
      "500000/500000 [==============================] - 70s 139us/sample - loss: 1.1154 - accuracy: 0.8759 - val_loss: 1.3331 - val_accuracy: 0.8126\n",
      "Epoch 18/50\n",
      "500000/500000 [==============================] - 70s 140us/sample - loss: 1.0971 - accuracy: 0.8784 - val_loss: 1.3667 - val_accuracy: 0.8008\n",
      "Epoch 19/50\n",
      "500000/500000 [==============================] - 70s 140us/sample - loss: 1.0767 - accuracy: 0.8823 - val_loss: 1.3845 - val_accuracy: 0.7982\n",
      "Epoch 20/50\n",
      "500000/500000 [==============================] - 70s 140us/sample - loss: 1.0605 - accuracy: 0.8853 - val_loss: 1.2796 - val_accuracy: 0.8275\n",
      "Epoch 21/50\n",
      "500000/500000 [==============================] - 69s 137us/sample - loss: 1.0453 - accuracy: 0.8880 - val_loss: 1.2601 - val_accuracy: 0.8297\n",
      "Epoch 22/50\n",
      "500000/500000 [==============================] - 70s 140us/sample - loss: 1.0352 - accuracy: 0.8901 - val_loss: 1.2959 - val_accuracy: 0.8238\n",
      "Epoch 23/50\n",
      "500000/500000 [==============================] - 70s 140us/sample - loss: 1.0237 - accuracy: 0.8926 - val_loss: 1.2754 - val_accuracy: 0.8233\n",
      "Epoch 24/50\n",
      "500000/500000 [==============================] - 70s 140us/sample - loss: 1.0115 - accuracy: 0.8948 - val_loss: 1.3279 - val_accuracy: 0.8201\n",
      "Epoch 25/50\n",
      "500000/500000 [==============================] - 70s 140us/sample - loss: 1.0053 - accuracy: 0.8971 - val_loss: 1.3639 - val_accuracy: 0.8121\n",
      "Epoch 26/50\n",
      "500000/500000 [==============================] - 70s 139us/sample - loss: 0.9966 - accuracy: 0.8980 - val_loss: 1.2406 - val_accuracy: 0.8306\n",
      "Epoch 27/50\n",
      "500000/500000 [==============================] - 70s 139us/sample - loss: 0.9849 - accuracy: 0.9008 - val_loss: 1.2055 - val_accuracy: 0.8268\n",
      "Epoch 28/50\n",
      "500000/500000 [==============================] - 70s 140us/sample - loss: 0.9735 - accuracy: 0.9024 - val_loss: 1.2285 - val_accuracy: 0.8289\n",
      "Epoch 29/50\n",
      "500000/500000 [==============================] - 69s 139us/sample - loss: 0.9675 - accuracy: 0.9044 - val_loss: 1.3171 - val_accuracy: 0.8139\n",
      "Epoch 30/50\n",
      "500000/500000 [==============================] - 70s 139us/sample - loss: 0.9623 - accuracy: 0.9057 - val_loss: 1.4258 - val_accuracy: 0.7965\n",
      "Epoch 31/50\n",
      "500000/500000 [==============================] - 70s 139us/sample - loss: 0.9564 - accuracy: 0.9068 - val_loss: 1.2834 - val_accuracy: 0.8199\n",
      "Epoch 32/50\n",
      "500000/500000 [==============================] - 70s 139us/sample - loss: 0.9472 - accuracy: 0.9092 - val_loss: 1.3348 - val_accuracy: 0.8217\n",
      "Epoch 33/50\n",
      "500000/500000 [==============================] - 69s 139us/sample - loss: 0.9395 - accuracy: 0.9106 - val_loss: 1.2982 - val_accuracy: 0.8210\n",
      "Epoch 34/50\n",
      "500000/500000 [==============================] - 69s 138us/sample - loss: 0.9347 - accuracy: 0.9117 - val_loss: 1.3231 - val_accuracy: 0.8124\n",
      "Epoch 35/50\n",
      "500000/500000 [==============================] - 69s 139us/sample - loss: 0.9285 - accuracy: 0.9130 - val_loss: 1.2624 - val_accuracy: 0.8252\n",
      "Epoch 36/50\n",
      "500000/500000 [==============================] - 69s 139us/sample - loss: 0.9230 - accuracy: 0.9144 - val_loss: 1.2962 - val_accuracy: 0.8208\n",
      "Epoch 37/50\n",
      "500000/500000 [==============================] - 70s 141us/sample - loss: 0.9171 - accuracy: 0.9155 - val_loss: 1.3245 - val_accuracy: 0.8122\n",
      "Epoch 38/50\n",
      "500000/500000 [==============================] - 70s 140us/sample - loss: 0.9105 - accuracy: 0.9169 - val_loss: 1.3217 - val_accuracy: 0.8200\n",
      "Epoch 39/50\n",
      "500000/500000 [==============================] - 70s 139us/sample - loss: 0.9097 - accuracy: 0.9176 - val_loss: 1.2971 - val_accuracy: 0.8215\n",
      "Epoch 40/50\n",
      "500000/500000 [==============================] - 69s 139us/sample - loss: 0.9031 - accuracy: 0.9186 - val_loss: 1.3471 - val_accuracy: 0.8101\n",
      "Epoch 41/50\n",
      "500000/500000 [==============================] - 69s 139us/sample - loss: 0.8996 - accuracy: 0.9200 - val_loss: 1.3283 - val_accuracy: 0.8201\n",
      "Epoch 42/50\n",
      "500000/500000 [==============================] - 69s 139us/sample - loss: 0.8933 - accuracy: 0.9209 - val_loss: 1.3642 - val_accuracy: 0.8089\n",
      "Epoch 43/50\n",
      "500000/500000 [==============================] - 70s 139us/sample - loss: 0.8907 - accuracy: 0.9217 - val_loss: 1.3818 - val_accuracy: 0.8119\n",
      "Epoch 44/50\n",
      "500000/500000 [==============================] - 69s 138us/sample - loss: 0.8842 - accuracy: 0.9231 - val_loss: 1.3364 - val_accuracy: 0.8199\n",
      "Epoch 45/50\n",
      "500000/500000 [==============================] - 70s 139us/sample - loss: 0.8785 - accuracy: 0.9238 - val_loss: 1.3648 - val_accuracy: 0.8080\n",
      "Epoch 46/50\n",
      "500000/500000 [==============================] - 70s 141us/sample - loss: 0.8753 - accuracy: 0.9250 - val_loss: 1.3396 - val_accuracy: 0.8213\n",
      "Epoch 47/50\n",
      "500000/500000 [==============================] - 70s 140us/sample - loss: 0.8726 - accuracy: 0.9259 - val_loss: 1.3139 - val_accuracy: 0.8210\n",
      "Epoch 48/50\n",
      "500000/500000 [==============================] - 70s 139us/sample - loss: 0.8680 - accuracy: 0.9262 - val_loss: 1.3231 - val_accuracy: 0.8183\n",
      "Epoch 49/50\n",
      "500000/500000 [==============================] - 69s 139us/sample - loss: 0.8686 - accuracy: 0.9267 - val_loss: 1.3708 - val_accuracy: 0.8110\n",
      "Epoch 50/50\n",
      "500000/500000 [==============================] - 69s 139us/sample - loss: 0.8646 - accuracy: 0.9279 - val_loss: 1.3409 - val_accuracy: 0.8140\n"
     ]
    }
   ],
   "source": [
    "for p in labelsPercent:\n",
    "    labeled, unlabeled = generate_augmented_labeled_unlabeled(augmentedExamplesForClass, p, False)\n",
    "    print(len(labeled), len(unlabeled))\n",
    "    cls_logs.append(fine_tune(labeled, p, True, True))        \n",
    "#[0.5505, 0.6021, 0.6382, 0.6691, 0.7135, 0.7671, 0.8060, 0.8448]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [6.198288536071777, 5.465572204589844, 5.042212745666504, 4.603378246307373, 4.21432876777649, 3.8798529510498048, 3.5870174255371094, 3.3245502891540526, 3.0923918476104735, 2.8729465045928957, 2.6541122608184815, 2.461081949234009, 2.2994060401916503, 2.1756460342407227, 2.091653762817383, 2.07822741317749, 2.0805465431213377, 2.0821636981964113, 2.08416300201416, 2.07394863319397, 2.0592278480529784, 2.0677625732421876, 2.071196897506714, 2.0665870418548584, 2.092849552154541, 2.0844737071990966, 2.0836500205993653, 2.081047088623047, 2.1038965129852296, 2.0963546504974366, 2.0914951343536377, 2.087072002410889, 2.088178394317627, 2.04937961769104, 2.0114091358184814, 2.0079333381652833, 2.0066117782592774, 2.009059087753296, 2.0019045581817627, 2.0090810737609863, 2.0138811073303224, 2.027307514190674, 2.053928695678711, 2.063140110015869, 2.056549615859985, 2.042499235153198, 1.9891199359893799, 1.9006591749191284, 1.794421983718872, 1.732493698120117], 'accuracy': [0.247, 0.497, 0.638, 0.7655, 0.848, 0.894, 0.9245, 0.9445, 0.9595, 0.9665, 0.974, 0.9765, 0.9815, 0.9765, 0.975, 0.969, 0.97, 0.9675, 0.9685, 0.9715, 0.9675, 0.973, 0.9755, 0.97, 0.965, 0.972, 0.971, 0.976, 0.971, 0.972, 0.9725, 0.974, 0.973, 0.9765, 0.9735, 0.9765, 0.978, 0.9785, 0.9805, 0.978, 0.975, 0.974, 0.9745, 0.971, 0.975, 0.976, 0.979, 0.986, 0.9855, 0.9825], 'val_loss': [8.078598933410644, 7.5415093719482424, 7.053771376037598, 6.660769619750977, 6.429116413879394, 6.750877458190918, 6.52663652267456, 6.810849429321289, 5.960440194702149, 5.579022355651856, 6.094472049713135, 5.933410987854004, 5.492620254516601, 6.285142347717285, 4.743273843383789, 5.872897657775879, 5.6214087860107425, 6.404688208007813, 5.534653736877441, 7.122834300994873, 7.1050371726989745, 4.901820889282226, 4.92306841430664, 5.2086296562194825, 5.488681833648681, 5.89514252243042, 7.428100309753418, 6.130780674743653, 5.8999337997436525, 5.567250518035888, 6.432747666168213, 6.046038237762451, 5.167139199829101, 4.933654232788086, 5.72700856628418, 5.866695239257813, 5.308986061859131, 5.170334895324707, 6.411837760162354, 5.484225072479248, 4.484202117156983, 6.420472677612305, 5.012453787231445, 4.786070667266846, 5.317877672576905, 4.93916166305542, 4.890271139526368, 5.15698099822998, 4.705457260131836, 4.331476843261719], 'val_accuracy': [0.1, 0.1, 0.1244, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1224, 0.1001, 0.1, 0.1, 0.1, 0.1241, 0.1089, 0.1294, 0.1, 0.1198, 0.147, 0.184, 0.1059, 0.112, 0.1373, 0.1614, 0.1091, 0.1292, 0.1923, 0.1407, 0.2014, 0.2008, 0.1893, 0.2169, 0.2413, 0.2326, 0.246, 0.2846]}\n",
      "{'loss': [6.134948234558106, 5.327072853088379, 4.679890861511231, 4.157394348144531, 3.7208632698059083, 3.3794367847442626, 3.127804275512695, 2.9976709899902345, 2.875764051437378, 2.7788392658233643, 2.740183422088623, 2.7055042629241943, 2.704549467086792, 2.7021435413360595, 2.729258720397949, 2.8051618537902834, 2.844447027206421, 2.811842475891113, 2.691608346939087, 2.601890962600708, 2.5684038200378416, 2.616293775558472, 2.6617096939086915, 2.7262969093322753, 2.72919775390625, 2.664423816680908, 2.5553946914672854, 2.532387950897217, 2.570954229354858, 2.618329069137573, 2.6107135105133055, 2.580209217071533, 2.580235782623291, 2.6408147735595704, 2.6326456966400147, 2.582760326385498, 2.5241547832489015, 2.615033893585205, 2.594316307067871, 2.449740182876587, 2.3650499210357667, 2.3887170753479006, 2.554471347808838, 2.6966277561187746, 2.676866172790527, 2.5774215412139894, 2.5018739528656004, 2.444828687667847, 2.3940528259277345, 2.337619869232178], 'accuracy': [0.2885, 0.5145, 0.66775, 0.76825, 0.8415, 0.88175, 0.901, 0.913, 0.926, 0.92725, 0.93075, 0.93325, 0.9305, 0.93875, 0.929, 0.925, 0.934, 0.93925, 0.94725, 0.9515, 0.95125, 0.944, 0.94425, 0.94025, 0.945, 0.9545, 0.962, 0.95275, 0.95525, 0.94875, 0.9545, 0.95675, 0.9535, 0.949, 0.95325, 0.958, 0.96, 0.947, 0.963, 0.971, 0.9645, 0.96, 0.9475, 0.94875, 0.958, 0.9665, 0.963, 0.9655, 0.967, 0.9705], 'val_loss': [9.252199960327149, 8.43478839416504, 7.429635763549805, 6.300401796722412, 6.266916110229492, 5.765335464477539, 5.8850571792602535, 6.006070918273926, 5.782930751800537, 6.019630506134034, 6.366744200134278, 6.362406070709229, 6.351343208312988, 6.033737068176269, 6.021668470001221, 6.295518738555908, 6.266857229614258, 6.8636509201049805, 5.847329254150391, 5.544235372924804, 5.496628377532959, 5.334061322021484, 5.6332134765625, 4.897620751953125, 4.823375733947754, 4.5716430305480955, 4.454645777130127, 4.513992699432373, 4.780687953186035, 4.720262809753418, 4.520674198150635, 4.665036276245117, 4.375824739074707, 4.743557120895386, 4.4876114364624025, 4.264490994262696, 4.469752346801758, 4.475784288024903, 4.369707444763184, 4.119408480072021, 4.183059785461426, 4.351434278869629, 4.570217169189453, 4.671197660827636, 4.621224028015137, 4.535114073181153, 4.4643792633056645, 4.396359700012207, 4.2159862823486325, 4.278580805969238], 'val_accuracy': [0.1064, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1001, 0.1, 0.1, 0.1144, 0.1012, 0.1011, 0.1178, 0.1045, 0.1529, 0.1125, 0.1702, 0.1837, 0.1468, 0.154, 0.2021, 0.3082, 0.3235, 0.3605, 0.398, 0.4012, 0.4038, 0.4267, 0.4672, 0.4492, 0.4847, 0.4507, 0.4944, 0.5069, 0.4886, 0.5023, 0.5086, 0.5035, 0.5032, 0.4928, 0.4908, 0.4792, 0.4982, 0.4746, 0.4862, 0.5069, 0.5143, 0.496]}\n",
      "{'loss': [6.001016390482585, 5.028138624827067, 4.3239935124715165, 3.7865899225870767, 3.433631155649821, 3.1908649768829345, 3.047246297200521, 2.9719117069244385, 2.97207657623291, 2.972949556350708, 2.9447335809071857, 2.9490552749633787, 2.8742637023925783, 2.8531256415049233, 2.8796123250325523, 2.8908669147491457, 2.846067914326986, 2.8707028942108153, 2.8269859873453775, 2.8444882469177246, 2.798687629699707, 2.7668812421162925, 2.8144909051259357, 2.757449806849162, 2.6879214604695636, 2.666918352762858, 2.747469509124756, 2.7791678460439044, 2.758911886850993, 2.6766248785654705, 2.6600603942871093, 2.6978538360595703, 2.659961762110392, 2.666347229639689, 2.732711725870768, 2.70515390141805, 2.600982655207316, 2.5709724280039468, 2.5280045490264893, 2.517081818898519, 2.644819185892741, 2.6511332461039223, 2.653168710072835, 2.661723190307617, 2.600768708546956, 2.550923994700114, 2.6444768835703534, 2.6350096441904705, 2.52439448483785, 2.4668559296925863], 'accuracy': [0.31616667, 0.562, 0.69416666, 0.78583336, 0.8385, 0.8695, 0.891, 0.8948333, 0.89716667, 0.9033333, 0.90816665, 0.9113333, 0.92, 0.9245, 0.91466665, 0.9253333, 0.92966664, 0.92983335, 0.931, 0.92983335, 0.94166666, 0.9368333, 0.9371667, 0.94533336, 0.94916666, 0.9456667, 0.93416667, 0.9415, 0.9485, 0.945, 0.9511667, 0.94383335, 0.9493333, 0.94233334, 0.943, 0.9443333, 0.95566666, 0.9571667, 0.9573333, 0.95133334, 0.9458333, 0.953, 0.9475, 0.95033336, 0.9601667, 0.95033336, 0.947, 0.957, 0.9648333, 0.9585], 'val_loss': [7.847401691436768, 6.812857083129883, 6.370967859649658, 5.995228221130371, 5.651408897399902, 5.604056122589111, 6.736747537231445, 6.3401516342163085, 6.530369538116455, 6.455129260253906, 7.0750936447143555, 6.154370650482178, 6.855884719848633, 5.406930192565918, 5.761688009643555, 5.938756031799317, 5.196960301208496, 5.2505612998962405, 4.945112586975098, 4.885153479003907, 4.409790132904052, 4.41790843963623, 4.410591619873047, 4.299590002441406, 4.432766323852539, 4.146485464477539, 4.37727970237732, 4.399245414733887, 4.24138512802124, 4.296432252502441, 4.315022637176513, 4.471757653045654, 4.1983221237182615, 4.337461530685425, 4.630886489868164, 4.395270135498047, 4.202670835113525, 4.192006694030762, 4.140874631500244, 4.196317586517334, 4.442482753753662, 4.354694638061523, 4.406586454772949, 4.432135209655762, 4.206662379455566, 4.599598300170898, 4.4050712944030765, 4.472842953491211, 4.272088793945312, 4.363626281738282], 'val_accuracy': [0.1, 0.1, 0.1, 0.1, 0.1, 0.1363, 0.1, 0.118, 0.1023, 0.1002, 0.1686, 0.2316, 0.1319, 0.1648, 0.2027, 0.2071, 0.3396, 0.3536, 0.395, 0.4252, 0.5029, 0.4957, 0.5264, 0.5357, 0.4893, 0.5419, 0.5367, 0.5309, 0.5464, 0.5355, 0.5296, 0.5185, 0.5398, 0.5401, 0.5076, 0.5299, 0.5357, 0.5318, 0.5399, 0.5297, 0.5166, 0.5359, 0.5286, 0.5374, 0.5408, 0.5067, 0.5366, 0.5276, 0.531, 0.5131]}\n",
      "{'loss': [5.8602629516601565, 4.68781577911377, 3.9751876876831056, 3.7057034538269042, 3.556711069869995, 3.4919835330963136, 3.4610953659057615, 3.5016553550720215, 3.5654703399658203, 3.42682541847229, 3.309833582687378, 3.234943576812744, 3.190795650100708, 3.19544573059082, 3.19536307220459, 3.2711966522216795, 3.3401853691101073, 3.451562201690674, 3.1513152973175047, 3.2775616622924804, 3.2196184371948244, 3.1957240173339843, 3.15367911567688, 3.2820060203552246, 3.261225155258179, 3.024732096862793, 2.973932536697388, 3.0587848640441893, 3.1165759147644043, 3.009088888168335, 2.986578727340698, 3.293285361480713, 3.133347245788574, 3.0559889854431153, 2.9696729415893555, 2.96050827293396, 3.0089223384857178, 3.041397547531128, 3.026960152435303, 3.0909379005432127, 3.0734456478118894, 3.0832206817626955, 3.1999861751556398, 3.247180961227417, 3.1091876491546633, 2.9377537155151368, 2.8509544631958006, 2.914101811981201, 2.909438485336304, 2.9971990688323973], 'accuracy': [0.3465, 0.581, 0.6951, 0.7538, 0.7966, 0.8165, 0.8319, 0.8364, 0.8451, 0.8638, 0.8764, 0.8857, 0.8864, 0.8896, 0.8925, 0.8849, 0.8928, 0.8849, 0.9179, 0.8991, 0.9101, 0.9082, 0.9139, 0.9008, 0.91, 0.9263, 0.9244, 0.9166, 0.9146, 0.9264, 0.9184, 0.906, 0.9255, 0.9287, 0.9293, 0.9291, 0.9278, 0.9199, 0.9281, 0.9214, 0.926, 0.9299, 0.9224, 0.9255, 0.9318, 0.9376, 0.9385, 0.9337, 0.9365, 0.9287], 'val_loss': [7.808488494873047, 6.713788075256348, 6.152985980224609, 5.744108869934082, 5.8699539855957035, 5.965953324890137, 6.003407047271729, 6.766402031707764, 5.78932007598877, 5.084709676361084, 4.684827903747559, 4.501328583526611, 4.617159580230713, 4.531640459442139, 4.225774903488159, 4.42864792137146, 4.412490422058106, 4.726688569641113, 4.286064747619629, 4.495793370056153, 4.378921297454834, 4.4112921203613285, 4.299684635925293, 4.489427770996094, 4.423261337280273, 4.244527233505249, 4.411951885986328, 4.322755016326904, 4.425654387664795, 4.189970574569702, 4.528470812225342, 4.536427756500244, 4.372176302337646, 4.326648771667481, 4.468526923370361, 4.439305409240722, 4.383792403411865, 4.599971891784668, 4.402955526733399, 4.539029119491577, 4.42286012840271, 4.449703511047363, 4.533115998840332, 4.934877920532227, 4.502879571533203, 4.407563106918335, 4.282198592376709, 4.297493888092041, 4.386924114990235, 4.392332138824463], 'val_accuracy': [0.1, 0.1, 0.1, 0.0999, 0.1013, 0.1061, 0.1515, 0.1184, 0.1795, 0.3317, 0.4479, 0.5035, 0.5022, 0.5196, 0.5806, 0.5763, 0.5732, 0.544, 0.5623, 0.5611, 0.58, 0.5754, 0.5924, 0.5785, 0.5771, 0.5739, 0.5563, 0.5779, 0.5729, 0.5898, 0.5452, 0.5833, 0.5829, 0.5839, 0.5535, 0.5607, 0.5833, 0.5717, 0.5745, 0.584, 0.593, 0.5783, 0.5931, 0.5357, 0.5603, 0.5607, 0.5713, 0.5856, 0.5763, 0.5948]}\n",
      "{'loss': [5.427090196228027, 4.118999746704102, 3.782328931808472, 3.667345222091675, 3.6272848125457764, 3.5692593490600584, 3.5532887481689452, 3.504504669189453, 3.445561250305176, 3.4546316738128664, 3.4296663131713867, 3.4326196964263915, 3.40660853767395, 3.4073021629333495, 3.3547642086029055, 3.4150911418914793, 3.3391546047210694, 3.369182970046997, 3.3546492034912108, 3.362789263534546, 3.34497119140625, 3.324664087677002, 3.347394733428955, 3.300487817001343, 3.350228955078125, 3.246137154388428, 3.2953669273376467, 3.2783307586669923, 3.3133806518554687, 3.2324649265289307, 3.248216361999512, 3.233311112213135, 3.24982896194458, 3.280040669631958, 3.2841994773864744, 3.2086056564331056, 3.2387976413726807, 3.1820570240020754, 3.2347688652038573, 3.1739102531433105, 3.205601279830933, 3.2448528602600097, 3.231512837982178, 3.1041942153930666, 3.192687200164795, 3.1532105190277098, 3.100051672744751, 3.138322455596924, 3.133178747177124, 3.1493709800720215], 'accuracy': [0.4038, 0.6008, 0.6697, 0.7161, 0.74865, 0.7734, 0.78985, 0.8012, 0.8135, 0.8204, 0.83205, 0.83515, 0.84415, 0.84865, 0.8544, 0.8556, 0.8636, 0.86275, 0.8701, 0.8663, 0.8724, 0.8755, 0.8769, 0.87755, 0.8787, 0.887, 0.88535, 0.8865, 0.88595, 0.89715, 0.89205, 0.8938, 0.8959, 0.8936, 0.8959, 0.9021, 0.89965, 0.89985, 0.896, 0.89895, 0.90035, 0.9028, 0.9012, 0.912, 0.90205, 0.90855, 0.908, 0.90885, 0.9075, 0.90685], 'val_loss': [5.975496800994873, 5.590381932067871, 6.4918271286010745, 5.472893820953369, 5.048173728942871, 4.288660095214844, 4.283684033203125, 4.13483344039917, 4.160930062103271, 4.130284834289551, 4.125878351974487, 4.071782423782349, 4.243519342041016, 4.2327620208740235, 4.270265696716309, 4.341001140594482, 4.258216918945313, 4.334624354553223, 4.268374217224121, 4.318904449462891, 4.185478216552735, 4.309308139801026, 4.338805254364014, 4.443216505432129, 4.273123086547852, 4.205324618530273, 4.2344544303894045, 4.407736741638184, 4.3591210014343265, 4.3166127838134765, 4.306970376586914, 4.275874472045898, 4.366249826049804, 4.369407077026367, 4.271128939819336, 4.3862880424499515, 4.3579135887146, 4.399160224914551, 4.2977593170166015, 4.322414032745361, 4.334433692932129, 4.387558275604248, 4.205328553771973, 4.336235140991211, 4.315942282104492, 4.1660858963012695, 4.261638648986817, 4.282513150024414, 4.420892337036133, 4.311244637298584], 'val_accuracy': [0.1, 0.1, 0.106, 0.2256, 0.3705, 0.5383, 0.5641, 0.6157, 0.6086, 0.6273, 0.6216, 0.6416, 0.6094, 0.6209, 0.6197, 0.6031, 0.6132, 0.6026, 0.6211, 0.6225, 0.6274, 0.638, 0.6259, 0.6153, 0.6246, 0.6279, 0.6342, 0.6236, 0.6133, 0.6364, 0.6249, 0.6324, 0.622, 0.6313, 0.6365, 0.6154, 0.6217, 0.6201, 0.6261, 0.6283, 0.6327, 0.6201, 0.6365, 0.6199, 0.6207, 0.6317, 0.6232, 0.6236, 0.5997, 0.6313]}\n",
      "{'loss': [4.6610321095275875, 3.6696478099823, 3.481254054107666, 3.4146171028900145, 3.382009443359375, 3.3924025337982178, 3.375263523712158, 3.4024772332000732, 3.4093872775268554, 3.3973458644104, 3.412163006896973, 3.398094475479126, 3.408421720428467, 3.4055142642974854, 3.3989355935668946, 3.382804659423828, 3.371113350830078, 3.365116917953491, 3.354884194641113, 3.321986395263672, 3.3277646257019042, 3.320383930358887, 3.3194613140869142, 3.303024164276123, 3.287503743133545, 3.2837536238098144, 3.2706215771484377, 3.2737393701171875, 3.235323418884277, 3.2385192760467527, 3.2048009143066407, 3.1959748858642576, 3.208275648345947, 3.1722383266448975, 3.1706338259887694, 3.1497394797515867, 3.135327080535889, 3.1338511136627196, 3.1124586752319336, 3.111838461227417, 3.0969395081329347, 3.1005615005493166, 3.0719075696563722, 3.061123237609863, 3.044316997833252, 3.052615338897705, 2.9865846798706053, 3.0190863671875, 3.008055587158203, 2.976122790374756], 'accuracy': [0.47716, 0.60624, 0.64482, 0.67164, 0.69176, 0.70802, 0.71782, 0.73106, 0.73962, 0.75014, 0.75494, 0.76468, 0.77002, 0.77556, 0.7815, 0.78548, 0.79034, 0.79504, 0.79704, 0.80388, 0.8035, 0.80618, 0.80986, 0.81004, 0.81632, 0.81918, 0.82036, 0.81978, 0.82684, 0.82568, 0.82992, 0.83114, 0.83014, 0.83404, 0.83538, 0.83862, 0.83758, 0.8377, 0.84176, 0.84126, 0.84278, 0.8443, 0.84408, 0.84884, 0.84716, 0.84676, 0.8518, 0.85012, 0.85332, 0.85626], 'val_loss': [5.3533853263854985, 4.269642781829834, 3.454423434448242, 3.4411885681152343, 3.4400521881103514, 3.4598216278076173, 3.530188292694092, 3.5455846267700197, 3.5961429264068605, 3.6039721824645996, 3.6231735374450684, 3.6561943214416504, 3.6853143379211426, 3.696091313934326, 4.034232664489746, 3.647715055465698, 3.6828604286193847, 3.7363622314453124, 3.697715340423584, 3.739251152420044, 3.759596242904663, 3.729812001800537, 3.7556706985473634, 3.7486538455963134, 3.7903353984832764, 3.786998161315918, 3.7370783210754395, 3.698807041168213, 3.7110001979827882, 3.6588112098693846, 3.79151190032959, 3.7006429306030273, 3.96690325088501, 3.7468038330078124, 3.7814801151275637, 3.6945160972595215, 3.6441988403320313, 3.742984051513672, 3.6322660350799563, 3.5977566970825197, 3.7433526428222654, 3.760332540893555, 3.6411377952575683, 3.702904377746582, 3.8731782115936277, 3.5805105293273924, 3.821633821105957, 3.592961082458496, 3.616495629119873, 3.6588688278198243], 'val_accuracy': [0.1, 0.3832, 0.6423, 0.661, 0.6746, 0.6844, 0.6799, 0.6812, 0.6732, 0.6803, 0.6871, 0.6838, 0.6937, 0.6868, 0.6157, 0.6992, 0.6859, 0.674, 0.6983, 0.6638, 0.6715, 0.6989, 0.6877, 0.6992, 0.6975, 0.6675, 0.6885, 0.6952, 0.6918, 0.7102, 0.681, 0.6929, 0.6388, 0.6885, 0.6831, 0.6957, 0.6978, 0.6873, 0.7009, 0.7058, 0.6716, 0.6873, 0.6896, 0.6832, 0.6569, 0.697, 0.6547, 0.69, 0.6912, 0.6844]}\n",
      "{'loss': [3.774636473388672, 2.6457906455993654, 2.441294974594116, 2.3548475817871095, 2.291669311141968, 2.2294693867492676, 2.1737323416900636, 2.1281285943603514, 2.079941220474243, 2.035891608581543, 1.9792724028015136, 1.9161522219085694, 1.8987099961090088, 1.8457716093444825, 1.7967899639892577, 1.766985507583618, 1.713830057182312, 1.674526435546875, 1.6479527595901489, 1.6009406356048583, 1.5749806311035157, 1.5348318303298951, 1.507827974472046, 1.4725868613052369, 1.4460364485931396, 1.4354199462890624, 1.392493671875, 1.3653404595184326, 1.336601726989746, 1.317169052658081, 1.2991164979553222, 1.2835208069610595, 1.2492349068069457, 1.2247953371429443, 1.2142306381988526, 1.1952140170288086, 1.1695901501464843, 1.1648996523666382, 1.1392157761764525, 1.1214284872817992, 1.1137058408355713, 1.0874645189857484, 1.0710400674819947, 1.0668227251434326, 1.049364945602417, 1.0426936632537842, 1.0275103848266602, 1.0165741457748414, 0.9938362884902954, 0.9964618840789795], 'accuracy': [0.49776, 0.63853, 0.68664, 0.72212, 0.74744, 0.77413, 0.79125, 0.80999, 0.82273, 0.83668, 0.85078, 0.86188, 0.86802, 0.8773, 0.88392, 0.89046, 0.89588, 0.90296, 0.90454, 0.90866, 0.91334, 0.91756, 0.92021, 0.92237, 0.9262, 0.92637, 0.93139, 0.93295, 0.93341, 0.93592, 0.93795, 0.93797, 0.9401, 0.94212, 0.94346, 0.94468, 0.94576, 0.94594, 0.94753, 0.9483, 0.94828, 0.95125, 0.9528, 0.95129, 0.95237, 0.95306, 0.95463, 0.95494, 0.95689, 0.9563], 'val_loss': [3.3591752220153808, 2.4379483661651613, 2.4182986999511717, 2.3931080478668214, 2.5171271560668944, 2.380374594116211, 2.438805767059326, 2.4703455184936525, 2.469846186065674, 2.5304671546936035, 2.450510764312744, 2.499767124938965, 2.7209428073883055, 2.39599867477417, 2.4424841148376464, 2.4628472259521486, 2.450957048034668, 2.545347940826416, 2.405206329727173, 2.3846978874206544, 2.461034427642822, 2.69632735748291, 2.4228514148712157, 2.261581337738037, 2.4143618019104003, 2.4822069957733155, 2.4031773513793944, 2.2590670097351073, 2.378835711288452, 2.2132982376098633, 2.3463773513793944, 2.507210615158081, 2.4457382776260377, 2.2662202043533326, 2.2942542140960693, 2.5964316858291627, 2.302734473800659, 2.3379754358291627, 2.225921487045288, 2.196288498687744, 2.2695271419525147, 2.2485532062530518, 2.266400745010376, 2.3092311180114744, 2.211095058441162, 2.317550786590576, 2.2536874839782715, 2.127858790206909, 2.218868237686157, 2.337718831253052], 'val_accuracy': [0.4125, 0.6928, 0.6849, 0.7052, 0.6948, 0.7301, 0.7081, 0.7158, 0.7129, 0.6984, 0.7183, 0.7034, 0.6652, 0.7186, 0.7248, 0.7111, 0.7167, 0.6798, 0.7178, 0.7257, 0.7147, 0.6654, 0.7202, 0.7111, 0.7207, 0.6996, 0.7174, 0.726, 0.7138, 0.7252, 0.7139, 0.6963, 0.6701, 0.7124, 0.7048, 0.6782, 0.7205, 0.7089, 0.7236, 0.7122, 0.714, 0.7049, 0.709, 0.7043, 0.7132, 0.7108, 0.7127, 0.7195, 0.71, 0.7002]}\n",
      "{'loss': [3.176183391571045, 2.3755907928466797, 2.265603648376465, 2.1823949424362183, 2.1079479640960694, 2.0399271355438233, 1.9705708253097534, 1.9045480040740967, 1.8426047171020508, 1.7857005575180054, 1.7222227562713623, 1.6692305934143066, 1.603826773223877, 1.5595566438674926, 1.5081980331039428, 1.4630961099624633, 1.4213847149658203, 1.3817931998825073, 1.3427861353683472, 1.3110998027801513, 1.2733785522079468, 1.2456674266815186, 1.2164268590545655, 1.1953551684188843, 1.1693014139556885, 1.1510457110977173, 1.1309540060043335, 1.1072008554458619, 1.0944076380920411, 1.0730037000656127, 1.0609321306991577, 1.0439822101211549, 1.032864430732727, 1.0124229724121094, 1.0079552219772339, 0.9871836722564697, 0.9757836812591553, 0.9740103802871705, 0.9611595251846313, 0.9561790116500855, 0.9410509457588195, 0.9325602488708497, 0.9306732475280761, 0.9214296199417115, 0.9143198637771607, 0.9051808800125122, 0.8987675846481323, 0.8839189284706116, 0.8837369506835937, 0.8835827678871154], 'accuracy': [0.55161, 0.673755, 0.716875, 0.7463, 0.77003, 0.78755, 0.802755, 0.81717, 0.829285, 0.83759, 0.84751, 0.85519, 0.8629, 0.869705, 0.875855, 0.88054, 0.885825, 0.88964, 0.894285, 0.897155, 0.90167, 0.90459, 0.907305, 0.91041, 0.912095, 0.91417, 0.916585, 0.91898, 0.920265, 0.922635, 0.92439, 0.92687, 0.92753, 0.930015, 0.9317, 0.93255, 0.933915, 0.93429, 0.93624, 0.936365, 0.93834, 0.93948, 0.939255, 0.94129, 0.94073, 0.94384, 0.94382, 0.94488, 0.94572, 0.945315], 'val_loss': [2.3630181327819826, 2.168821384048462, 2.1539312280654905, 2.306694829940796, 2.215047198295593, 2.180173873901367, 2.1000025747299196, 2.1391765354156496, 2.0814309421539305, 2.032024959754944, 2.0055513259887694, 2.0348390743255615, 2.058150857543945, 1.943797491836548, 2.0074654998779295, 1.9819844161987306, 1.857780404663086, 1.8810611015319825, 1.8553898113250733, 1.915485415649414, 1.9115999519348144, 1.7862418844223023, 1.7477538665771484, 1.7791331802368164, 1.8411250001907349, 1.789771157836914, 1.8713263595581056, 2.002619145965576, 1.8856133708953857, 1.7793198673248292, 1.8247736066818236, 1.7650654716491698, 1.8968569396972657, 1.848838356399536, 1.895772709274292, 1.8063068605422974, 1.9822645923614501, 1.9683077209472657, 1.8893022804260253, 1.7635786014556885, 1.8353117612838745, 1.7718897117614747, 1.7677756519317627, 1.7390215282440185, 1.8201819526672363, 1.7979211826324464, 1.8700611145019532, 1.8085724746704102, 1.9357094207763672, 1.78782954788208], 'val_accuracy': [0.6886, 0.7296, 0.7425, 0.7158, 0.7352, 0.7408, 0.7567, 0.7522, 0.7605, 0.7565, 0.7623, 0.7524, 0.7454, 0.7579, 0.747, 0.7463, 0.7675, 0.7563, 0.7579, 0.7486, 0.7362, 0.7659, 0.7665, 0.7616, 0.7478, 0.7653, 0.7398, 0.7263, 0.745, 0.7518, 0.7505, 0.7642, 0.7476, 0.7496, 0.739, 0.7455, 0.7285, 0.7256, 0.7396, 0.7592, 0.7486, 0.7584, 0.7635, 0.7525, 0.7532, 0.756, 0.7378, 0.7463, 0.739, 0.7617]}\n",
      "{'loss': [6.040189352416992, 5.215942189025879, 4.5631504776000975, 4.059641606140136, 3.726222703552246, 3.5857194988250733, 3.485131894683838, 3.4836428413391114, 3.5908547176361085, 3.4974008800506593, 3.3928052169799803, 3.440543901824951, 3.315863733673096, 3.3148765869140626, 3.1124700527191163, 3.1469329025268555, 3.396820831298828, 3.4559858543395996, 3.725022174835205, 3.928452508544922, 3.8640623703002928, 3.631834935760498, 3.5652391708374025, 3.2667700729370117, 3.0613854404449463, 3.011779459762573, 3.0215493141174314, 3.562276690673828, 3.7615618194580076, 3.5849919300079347, 3.213785422897339, 2.9377837173461914, 3.0120568565368653, 3.1336542560577394, 3.1965681816101075, 3.2815740684509276, 3.4548920639038085, 3.811521803665161, 3.6746862705230714, 3.4095332931518554, 3.469834087753296, 3.487796871185303, 3.586221647644043, 3.7224135200500488, 3.7173641326904296, 3.439275759124756, 3.266433377075195, 3.4199552139282225, 3.531425548171997, 3.4377208675384523], 'accuracy': [0.3098, 0.5348, 0.685, 0.774, 0.8302, 0.846, 0.859, 0.878, 0.8708, 0.8916, 0.9032, 0.893, 0.9144, 0.916, 0.9364, 0.9176, 0.8938, 0.9032, 0.886, 0.882, 0.9114, 0.9318, 0.9112, 0.9488, 0.9436, 0.9446, 0.941, 0.8654, 0.9026, 0.924, 0.9554, 0.9556, 0.941, 0.9352, 0.9338, 0.9282, 0.9172, 0.899, 0.939, 0.9494, 0.9282, 0.9346, 0.932, 0.9182, 0.9246, 0.9548, 0.95, 0.9336, 0.9406, 0.9462], 'val_loss': [7.619785134887695, 7.113250164794922, 6.545914251708984, 6.639496115112305, 6.804051881408691, 6.004119574737548, 5.749098196411133, 5.891066645050048, 6.157361688232422, 5.960860671997071, 5.6167262802124025, 5.654683584594727, 5.933111329650879, 5.998341371154785, 5.517191911315918, 5.933555776214599, 6.647915952301026, 6.075744554901123, 5.533994551086426, 5.900741877746582, 5.680919333648681, 5.115843574523926, 4.9970530364990235, 4.78910352859497, 4.619366804504395, 4.500081980133056, 4.620181029510498, 5.362823226928711, 5.3443860397338865, 5.0188795181274415, 4.585977489471436, 4.346150128936768, 4.602845071411132, 4.8134105575561525, 4.985809610748291, 4.921627787017822, 5.179365771484375, 5.562088700866699, 5.18828791809082, 4.898524743652343, 5.087852166748047, 4.990556552124024, 5.191183563232422, 5.338774458312988, 5.261923765563965, 4.97144309387207, 4.880862280273438, 5.067139879608154, 5.1952128448486325, 5.020102497863769], 'val_accuracy': [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1495, 0.1457, 0.1, 0.1247, 0.1641, 0.1043, 0.1236, 0.1245, 0.1758, 0.1257, 0.1226, 0.1886, 0.31, 0.3091, 0.3686, 0.4342, 0.4794, 0.4665, 0.4842, 0.5149, 0.4935, 0.4717, 0.5093, 0.5175, 0.527, 0.5316, 0.5235, 0.4978, 0.4931, 0.5092, 0.5113, 0.5184, 0.5322, 0.5111, 0.5185, 0.5243, 0.5245, 0.5162, 0.5158, 0.5131, 0.5016, 0.5048, 0.5063, 0.5199]}\n",
      "{'loss': [5.775725841522217, 4.645045611572265, 4.006000305175781, 3.657811923980713, 3.5339504013061522, 3.479200687408447, 3.3843168823242187, 3.349430123138428, 3.3934303215026858, 3.3973595218658446, 3.3150329254150392, 3.4878383728027345, 3.435460551452637, 3.407045116043091, 3.2292818672180177, 3.252614066314697, 3.110127262878418, 3.44855032081604, 3.39523662109375, 3.2146002864837646, 3.0282862369537353, 3.2061776432037354, 3.2048989082336425, 3.160219438171387, 3.185259769439697, 3.1282913719177246, 3.0910227188110353, 3.1071514961242674, 3.2855705280303953, 3.049063522720337, 2.9550794212341307, 3.0449486137390136, 3.079249751663208, 3.12406865272522, 3.0427403274536133, 3.374408039855957, 3.0890454681396484, 2.814841251373291, 2.9130269786834715, 2.9749102699279786, 3.0403148441314696, 2.958453038787842, 2.988968259048462, 3.0687898063659667, 3.026146385192871, 3.0995123596191405, 3.0738097270965574, 3.33993046875, 3.2326398990631104, 3.0012974937438965], 'accuracy': [0.3669, 0.5871, 0.7021, 0.7759, 0.799, 0.8218, 0.8474, 0.8575, 0.8545, 0.8662, 0.8806, 0.8646, 0.8806, 0.8832, 0.897, 0.8959, 0.9098, 0.8789, 0.8972, 0.909, 0.9174, 0.9023, 0.9118, 0.9143, 0.9088, 0.9191, 0.9206, 0.9211, 0.9038, 0.932, 0.9299, 0.9193, 0.9242, 0.9257, 0.9289, 0.9039, 0.9388, 0.945, 0.9292, 0.9337, 0.9294, 0.9366, 0.9322, 0.9295, 0.9319, 0.9323, 0.933, 0.9171, 0.9348, 0.9391], 'val_loss': [6.848513484191894, 6.45409952468872, 6.502682127380371, 6.635671062469482, 6.351973953247071, 6.932807228851319, 6.180157405090332, 6.559259028625489, 6.184796499633789, 5.000590371704101, 4.730506197357178, 4.650504335021973, 4.45125230178833, 4.314536127471924, 4.273157442855835, 4.370294641876221, 4.321106366348267, 4.5620650421142575, 4.574827336120605, 4.425998309326172, 4.247442448806763, 4.446542036437989, 4.438979811859131, 4.391175198364258, 4.472086838531494, 4.395082627105713, 4.407438810729981, 4.395590783691406, 4.587880561828613, 4.1602118225097655, 4.384317862701416, 4.362968586730957, 4.532811569213867, 4.459241794204712, 4.417548921203613, 4.735116418457031, 4.2728604934692385, 4.251011228561401, 4.368765419769287, 4.5682151382446285, 4.562578510284424, 4.3308263061523435, 4.41576598854065, 4.430534715270996, 4.466821060943603, 4.452363778686523, 4.7367753982543945, 4.754129147338867, 4.651007099151611, 4.368751179504395], 'val_accuracy': [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1166, 0.1507, 0.1478, 0.3501, 0.4441, 0.5229, 0.5448, 0.5906, 0.5906, 0.5778, 0.5758, 0.5833, 0.5567, 0.5593, 0.5785, 0.5834, 0.5722, 0.5723, 0.5802, 0.5802, 0.5828, 0.5612, 0.5798, 0.5908, 0.5532, 0.5839, 0.5608, 0.5817, 0.5697, 0.586, 0.5891, 0.5582, 0.5745, 0.5586, 0.5688, 0.5719, 0.5742, 0.5844, 0.5808, 0.5854, 0.5395, 0.5851, 0.5534, 0.5854]}\n",
      "{'loss': [5.602171360270182, 4.316135227711995, 3.820995199330648, 3.6743244818369547, 3.5524771695454915, 3.5351417098999023, 3.521964667892456, 3.4358454519907635, 3.42883564453125, 3.4786927230834963, 3.3729482162475586, 3.3794118063608805, 3.302207608159383, 3.366478568903605, 3.2714432118733723, 3.346666943359375, 3.254181027475993, 3.2770642225901288, 3.2724814529418946, 3.1980497871398925, 3.2941341102600097, 3.1673106870015464, 3.3317324160257975, 3.3638378129323323, 3.1884755139668783, 3.155162078730265, 3.216187532043457, 3.215716013590495, 3.1439942039489748, 3.2076190895080567, 3.0941412315368653, 3.213595852661133, 3.114057450993856, 3.247287764867147, 3.118074338658651, 3.046008484395345, 3.261276248931885, 3.091794548543294, 3.1599140786488853, 3.1977272351582844, 3.146276354344686, 3.0595584504445394, 3.1327397965749104, 3.0063260762532553, 3.0627311887105306, 3.122273632558187, 3.137722964859009, 3.1709685119628905, 2.9881903531392418, 3.0238015569051107], 'accuracy': [0.38913333, 0.60466665, 0.6914667, 0.74126667, 0.7786, 0.7984, 0.81046665, 0.8305333, 0.8384, 0.8382, 0.8556, 0.8594, 0.8674667, 0.86826664, 0.87633336, 0.8734667, 0.8826, 0.88453335, 0.8897333, 0.89486665, 0.8886, 0.89813334, 0.8914667, 0.88953334, 0.9063333, 0.9059333, 0.9023333, 0.9022667, 0.9086, 0.9048667, 0.9127333, 0.9080667, 0.9145333, 0.90606666, 0.9162667, 0.9140667, 0.9066, 0.91926664, 0.914, 0.9122667, 0.9176, 0.9221333, 0.9166, 0.92506665, 0.9231333, 0.9166667, 0.91926664, 0.91713333, 0.9296, 0.9238], 'val_loss': [6.5456269821166995, 5.989993974304199, 6.366255696105957, 5.926277610778809, 6.227315658569336, 5.919860818481445, 5.212791386413574, 4.4812677963256835, 4.188739620971679, 4.192586749267578, 4.120440110778809, 4.261538377380371, 4.35338071899414, 4.262883148956298, 4.231413170623779, 4.224603674316406, 4.253858358764648, 4.232767995452881, 4.19035323638916, 4.316963931274414, 4.225592964172363, 4.258375748443603, 4.396690034484863, 4.314749784851074, 4.234639763641358, 4.343684413146972, 4.34376356048584, 4.331872914886475, 4.389149217224121, 4.539866129302979, 4.397546589660645, 4.409783106994629, 4.280831838989258, 4.416256489562988, 4.272898622131348, 4.322356941223145, 4.357875843811035, 4.347552754211426, 4.343632355499268, 4.450967834472657, 4.323705465698242, 4.328217729187012, 4.275384804534912, 4.4000529792785645, 4.412142540740967, 4.312962205505371, 4.392505142211914, 4.458074113464355, 4.334284226989746, 4.359057173156739], 'val_accuracy': [0.1, 0.1, 0.1, 0.1049, 0.1315, 0.1927, 0.3149, 0.4918, 0.6018, 0.6201, 0.6225, 0.6002, 0.5882, 0.6131, 0.6052, 0.6287, 0.6089, 0.6251, 0.6232, 0.603, 0.6177, 0.615, 0.6145, 0.6173, 0.6168, 0.5989, 0.6122, 0.6171, 0.593, 0.5805, 0.6109, 0.611, 0.6294, 0.6049, 0.6162, 0.6141, 0.6222, 0.6101, 0.6083, 0.5873, 0.6154, 0.6067, 0.604, 0.5912, 0.5869, 0.622, 0.6086, 0.6049, 0.5969, 0.5932]}\n",
      "{'loss': [5.1936078744506835, 3.962731379699707, 3.717616454162598, 3.640894669189453, 3.577645456161499, 3.552872891693115, 3.5082748921203613, 3.5012774532318116, 3.451509594955444, 3.4520747251892088, 3.4458779904174803, 3.4383506350708006, 3.4188717388916015, 3.4181016090393066, 3.3979449224090574, 3.391976811065674, 3.401879471588135, 3.3887225627136233, 3.3617800425720215, 3.3538570264434813, 3.371219694442749, 3.3499966959381102, 3.4019745847320557, 3.364591040420532, 3.3612190964508057, 3.3500803605651854, 3.3229475009918215, 3.354570682525635, 3.304936579666138, 3.3427429573822023, 3.267026889038086, 3.2857299687194823, 3.2747174448394776, 3.227189765319824, 3.2940956562805175, 3.2500368955230714, 3.216962995071411, 3.2144803330230713, 3.2242082427978516, 3.238408234634399, 3.2002799712371828, 3.186339160308838, 3.2164805311584472, 3.183936668548584, 3.143168043899536, 3.149818638534546, 3.091887000350952, 3.135869394607544, 3.1380664963531495, 3.0753709313201902], 'accuracy': [0.4356, 0.61444, 0.6738, 0.70968, 0.73916, 0.75252, 0.7728, 0.78316, 0.80096, 0.80804, 0.81096, 0.82296, 0.83064, 0.83616, 0.84028, 0.84532, 0.8494, 0.8522, 0.85664, 0.85916, 0.85928, 0.86584, 0.86448, 0.86884, 0.87204, 0.87424, 0.87556, 0.8738, 0.88136, 0.87712, 0.885, 0.88212, 0.885, 0.88932, 0.88332, 0.8896, 0.89032, 0.89064, 0.89164, 0.88928, 0.89408, 0.89188, 0.89216, 0.89276, 0.89796, 0.89988, 0.90148, 0.89988, 0.9012, 0.90376], 'val_loss': [6.589528731536865, 6.345562460327148, 5.711114492034912, 4.721227543640136, 4.111186504364014, 4.010200814819336, 3.9957926826477053, 3.918949468612671, 4.006625037384033, 4.148384780883789, 4.035436857604981, 4.06732822341919, 4.106077642822266, 4.187332970046997, 4.102585452270508, 4.576671059417724, 4.1848899978637695, 4.331184146118164, 4.205050379943848, 4.198290162658691, 4.209375661468506, 4.24140432434082, 4.325422734069824, 4.241457697677612, 4.3674432586669925, 4.146094426345825, 4.298772958374023, 4.241739261627197, 4.497037259674072, 4.354095604705811, 4.145446808624268, 4.3512033203125, 4.213266172027588, 4.286282860565185, 4.274611709594726, 4.335137292480469, 4.206154227447509, 4.236935623550415, 4.287537873840332, 4.183370452880859, 4.2112938747406, 4.268276303863526, 4.354268608856201, 4.172839107513428, 4.241234497070312, 4.1353353454589845, 4.276670881652832, 4.173923839569092, 4.235324034118652, 4.225260897827148], 'val_accuracy': [0.1, 0.1391, 0.1307, 0.3472, 0.5538, 0.6179, 0.6242, 0.6501, 0.6371, 0.6289, 0.6412, 0.6554, 0.6474, 0.6339, 0.6444, 0.5907, 0.6437, 0.615, 0.6488, 0.6408, 0.6483, 0.6483, 0.6226, 0.6432, 0.6237, 0.6518, 0.6368, 0.643, 0.6111, 0.623, 0.6495, 0.6167, 0.641, 0.6425, 0.6304, 0.6417, 0.6482, 0.6446, 0.6395, 0.6358, 0.6502, 0.6406, 0.6248, 0.6455, 0.6271, 0.6426, 0.6361, 0.6545, 0.6387, 0.6242]}\n",
      "{'loss': [4.6272126038360595, 3.616138896179199, 3.4576346450805664, 3.3801882286071776, 3.3366514872741697, 3.3162862995910642, 3.33937899810791, 3.3514234397888183, 3.3543848709106445, 3.3527765019226075, 3.3567421785736085, 3.348696202011108, 3.3527313107299803, 3.312105167236328, 3.3454043991088867, 3.3187183778381346, 3.3265212426757813, 3.315492565383911, 3.282463387756348, 3.2792616580200193, 3.2714750663757326, 3.283784369812012, 3.2513935653686525, 3.263138786468506, 3.2464943033599853, 3.2165849656677246, 3.2170786503601074, 3.189681220855713, 3.197127324752808, 3.154648435974121, 3.1811922064208984, 3.1491617144775392, 3.1318172972106932, 3.1547117808532716, 3.1171445764923096, 3.099131997299194, 3.068923538894653, 3.053260767593384, 3.057611607055664, 3.039674034194946, 3.029731290817261, 3.0212699938964844, 3.020739830856323, 2.978021159210205, 2.9770118033599853, 2.9512591220855713, 2.9354709826660157, 2.9281947845458984, 2.9359823645782472, 2.9403137259674073], 'accuracy': [0.4847, 0.61688, 0.65682, 0.68084, 0.69788, 0.71048, 0.72472, 0.73606, 0.74754, 0.7526, 0.76352, 0.76938, 0.77522, 0.78604, 0.78622, 0.7912, 0.79736, 0.80052, 0.80438, 0.80912, 0.8112, 0.8114, 0.81694, 0.8169, 0.82436, 0.82454, 0.82426, 0.82866, 0.8308, 0.83722, 0.83254, 0.8393, 0.83832, 0.83712, 0.84096, 0.8415, 0.84612, 0.84376, 0.84506, 0.84764, 0.84826, 0.84964, 0.85056, 0.85462, 0.85332, 0.85558, 0.8551, 0.85856, 0.85914, 0.85572], 'val_loss': [5.731621973419189, 5.006370007324219, 3.4289407009124755, 3.342616609954834, 3.3566624717712403, 3.404383341217041, 3.4858902477264406, 3.496153253173828, 3.6016103515625, 3.5234622055053713, 3.512394821166992, 3.6412017013549804, 3.6731202186584473, 3.636160452270508, 3.577902982711792, 3.6850076564788816, 3.749493013381958, 3.6698395927429197, 3.698022389221191, 3.660567706298828, 3.7273308563232423, 3.690199740600586, 3.70021443901062, 3.7619104511260986, 3.6459195457458495, 3.6388598022460936, 3.76856137008667, 3.701590728378296, 3.6554142650604247, 3.6334047325134278, 3.665776208496094, 3.6609259346008303, 3.7996340755462645, 3.6283790992736815, 3.688516666793823, 3.5993225471496584, 3.550814301300049, 3.697015191268921, 3.7684176094055175, 3.615570654678345, 3.5484000034332275, 3.579341544342041, 3.619020188140869, 3.6493935607910157, 3.5448568058013916, 3.565001695251465, 3.5286607765197755, 3.4864038623809814, 3.5377476409912108, 3.5434065338134766], 'val_accuracy': [0.1, 0.2092, 0.6488, 0.6801, 0.6773, 0.6845, 0.6903, 0.6793, 0.6771, 0.6988, 0.695, 0.6828, 0.6744, 0.692, 0.7008, 0.6863, 0.678, 0.6948, 0.6743, 0.6994, 0.6842, 0.6887, 0.6914, 0.6831, 0.6978, 0.7008, 0.6848, 0.6933, 0.6981, 0.6925, 0.6894, 0.7062, 0.6489, 0.7024, 0.6821, 0.7066, 0.7039, 0.6736, 0.6488, 0.6869, 0.6993, 0.6972, 0.6925, 0.6738, 0.6986, 0.6866, 0.7025, 0.701, 0.6982, 0.6853]}\n",
      "{'loss': [4.014236957473755, 3.252729726867676, 3.185290929031372, 3.1871831200408938, 3.1853819840698243, 3.1757579522399904, 3.1542796638946533, 3.118922476760864, 3.106208762939453, 3.073982576828003, 3.050915894729614, 3.0443214796142577, 3.0089482417907716, 2.9871398568267824, 2.952276499786377, 2.929860994293213, 2.9082036569519043, 2.879088940170288, 2.8663069939575196, 2.834799733215332, 2.8121456466674806, 2.781479416381836, 2.7573207292175295, 2.7481192511444092, 2.7330408558807373, 2.7188277737579347, 2.7018543176116943, 2.6735882947387695, 2.6634745995788576, 2.630197065994263, 2.6015728238220213, 2.590152914840698, 2.5700850741729737, 2.5618382329254152, 2.536851237197876, 2.5281757981262207, 2.5188043466949464, 2.499153677307129, 2.475577792877197, 2.468270882873535, 2.4552020829315184, 2.446024743713379, 2.4216500813598634, 2.4091883602142334, 2.3963706918640137, 2.3862037088928223, 2.3758013989257813, 2.3488354068756103, 2.3538203199005125, 2.334387562576294], 'accuracy': [0.531896, 0.61992, 0.644984, 0.66376, 0.67744, 0.68904, 0.698984, 0.705376, 0.711928, 0.719152, 0.723432, 0.729744, 0.733944, 0.738992, 0.742304, 0.746456, 0.74804, 0.752688, 0.755, 0.757032, 0.759496, 0.763904, 0.764392, 0.767096, 0.769648, 0.771936, 0.77288, 0.775248, 0.776208, 0.77712, 0.780688, 0.780504, 0.782872, 0.784472, 0.786288, 0.788376, 0.787992, 0.790672, 0.792184, 0.792256, 0.79416, 0.79288, 0.797184, 0.7968, 0.796192, 0.798992, 0.800704, 0.801384, 0.801312, 0.804248], 'val_loss': [3.3889869834899904, 3.029957526397705, 3.028944485473633, 3.1911404014587403, 3.0334941024780275, 3.0690735889434815, 3.029547206878662, 3.089913223266602, 2.954536796951294, 2.955960710144043, 3.024485775375366, 3.066503006362915, 2.997350605010986, 2.976787063598633, 3.142828998565674, 2.9757073093414306, 2.8450605796813964, 3.007071054840088, 2.8265058654785156, 2.822289543914795, 2.8685619701385496, 2.777712469482422, 2.898018206787109, 2.8775491836547853, 2.8832195545196533, 2.7603569732666013, 2.8070201232910157, 2.7539238441467284, 2.7611902923583984, 2.7197097671508788, 2.76041626625061, 2.677006247711182, 2.7290570533752443, 2.7364512886047363, 2.643025595855713, 2.682617164993286, 2.6669119697570802, 2.635911463165283, 2.72821304397583, 2.647818884277344, 2.6860765800476076, 2.6100827007293703, 2.538310699081421, 2.627563999938965, 2.5523867446899415, 2.621937843704224, 2.535669585418701, 2.511738291168213, 2.4992358589172365, 2.5494909255981444], 'val_accuracy': [0.5786, 0.6711, 0.7097, 0.6677, 0.7207, 0.7327, 0.731, 0.7178, 0.7387, 0.7493, 0.7263, 0.7154, 0.7214, 0.7305, 0.6783, 0.728, 0.7491, 0.7207, 0.7437, 0.7476, 0.7456, 0.7537, 0.7239, 0.7281, 0.7225, 0.7544, 0.743, 0.7549, 0.7491, 0.746, 0.7289, 0.7528, 0.7415, 0.7204, 0.7526, 0.746, 0.7383, 0.7461, 0.7273, 0.7445, 0.7295, 0.7464, 0.7537, 0.7339, 0.752, 0.7258, 0.7379, 0.7499, 0.7444, 0.7518]}\n",
      "{'loss': [3.057134475479126, 2.3421130065307616, 2.2309449375305177, 2.1390145541381838, 2.0535631993255614, 1.9654353644256592, 1.8938496242370606, 1.8246377872543336, 1.75303499118042, 1.6928190389404296, 1.6269264185485839, 1.5626182981567383, 1.5141106042404175, 1.4595486831207276, 1.4136833684997558, 1.3712338531951904, 1.3257733284301758, 1.2935272789764405, 1.2596904215240479, 1.2283698400115968, 1.2004207921447754, 1.1751065713806153, 1.1505005565795898, 1.1395824041137694, 1.1130261857528687, 1.0969432321166992, 1.0750299140014647, 1.0724561083908082, 1.0433076775665284, 1.0383538194503785, 1.0269458875274657, 1.0141020351486205, 0.998681532737732, 0.9950333742752075, 0.9785892270889283, 0.9692019011917115, 0.9598835301437378, 0.9512713754425048, 0.9428067904052735, 0.9379344130401611, 0.9312754971313476, 0.9223364832611084, 0.9168992336730957, 0.9111235453720092, 0.9008490471420288, 0.8968252511520386, 0.8996656526947021, 0.8876265707893372, 0.8833729119110107, 0.8742042737045288], 'accuracy': [0.57194, 0.685928, 0.728344, 0.758148, 0.777504, 0.794456, 0.80812, 0.821132, 0.832804, 0.840608, 0.850156, 0.85654, 0.863932, 0.869488, 0.874988, 0.879136, 0.883816, 0.88776, 0.891932, 0.895536, 0.899128, 0.901752, 0.904788, 0.906688, 0.909808, 0.911888, 0.914364, 0.915704, 0.9185, 0.919212, 0.922016, 0.923392, 0.925124, 0.92616, 0.927288, 0.929716, 0.930632, 0.930956, 0.932724, 0.93356, 0.934212, 0.935864, 0.936608, 0.938208, 0.93906, 0.94026, 0.939832, 0.941948, 0.942544, 0.943792], 'val_loss': [2.3301784286499023, 2.2571951122283935, 2.1578248218536378, 2.0910684461593627, 2.094674196624756, 2.025250114440918, 2.045971012878418, 1.9893762126922607, 2.0918524909973146, 1.82304291305542, 1.8503299484252929, 1.9459396560668945, 1.8336894153594971, 1.8432183589935303, 1.867409160041809, 1.8337478803634644, 1.7259958393096924, 1.6706936227798461, 1.730205585861206, 1.6560176523208618, 1.9668429569244386, 1.8915473161697387, 1.7106556255340577, 1.6849855869293213, 1.748360001373291, 1.7266182256698608, 1.7640328281402589, 1.7039362941741942, 1.7154848510742187, 1.7915211421966553, 1.6952329277038574, 1.6449398323059081, 1.7295715393066406, 1.6500565536499023, 1.731347285079956, 1.734183023262024, 1.7788125787734985, 1.7222883060455323, 1.643798670578003, 1.715841651535034, 1.7404238636016847, 1.6911738422393798, 1.6226608325958252, 1.6798165996551513, 1.7282821216583253, 1.723142094039917, 1.7379306381225585, 1.7179323192596436, 1.6718899515151977, 1.684992507171631], 'val_accuracy': [0.6831, 0.7171, 0.7523, 0.7661, 0.765, 0.7709, 0.7563, 0.7602, 0.7383, 0.7789, 0.7805, 0.7576, 0.7674, 0.7714, 0.756, 0.759, 0.7745, 0.7845, 0.7696, 0.7795, 0.7024, 0.7527, 0.7725, 0.7692, 0.7576, 0.7512, 0.7619, 0.7721, 0.7652, 0.7506, 0.7711, 0.7701, 0.7696, 0.7795, 0.7613, 0.7643, 0.7608, 0.7695, 0.7767, 0.7556, 0.7624, 0.772, 0.7769, 0.7665, 0.7606, 0.7681, 0.7571, 0.7675, 0.771, 0.7709]}\n",
      "{'loss': [2.69191502142334, 2.179163930633545, 2.0333979901123045, 1.8988523708648681, 1.7734498259124756, 1.662649235534668, 1.5600151663513184, 1.4744201512756347, 1.400197301902771, 1.3430615550079346, 1.2887427798309325, 1.24900374067688, 1.2127538738098145, 1.1807094228515624, 1.1535370369110107, 1.1310182336730956, 1.1153557480010987, 1.0970786272964477, 1.0767136629714966, 1.060516477241516, 1.0452816560287475, 1.0352090322113037, 1.02367289453125, 1.0115346136703491, 1.0052796696014403, 0.9966427111663818, 0.9849276587219238, 0.9734979147567749, 0.9674900888671875, 0.9623051504440308, 0.9564474814491272, 0.9471865945053101, 0.9395455063858033, 0.9346548142623902, 0.9285042242584228, 0.9229984506607055, 0.9170898330764771, 0.9104970573310852, 0.9096507073249817, 0.903059717754364, 0.8995805225219726, 0.8932828436965943, 0.8907221786499023, 0.8842185348663331, 0.8784886811447143, 0.8753386386871338, 0.8726243596496582, 0.8679894665527343, 0.8686305028762817, 0.8646419082717895], 'accuracy': [0.619768, 0.720864, 0.756658, 0.780424, 0.797608, 0.80997, 0.82181, 0.831422, 0.838216, 0.844386, 0.85052, 0.855352, 0.860548, 0.865564, 0.869402, 0.8727, 0.875886, 0.87843, 0.882332, 0.885274, 0.88796, 0.89014, 0.892622, 0.894766, 0.897058, 0.898014, 0.90078, 0.90236, 0.904386, 0.905696, 0.90677, 0.909154, 0.910644, 0.911692, 0.913, 0.914392, 0.915482, 0.916938, 0.917628, 0.918644, 0.91999, 0.920866, 0.921746, 0.923064, 0.923832, 0.925022, 0.925868, 0.92621, 0.926692, 0.927916], 'val_loss': [2.0628730712890624, 2.060752870941162, 1.954623525238037, 1.7860725471496581, 1.789222201347351, 1.6954144718170165, 1.5881501014709474, 1.5560125518798829, 1.5697068004608155, 1.470109846496582, 1.4234091133117677, 1.4333874923706054, 1.3933210500717164, 1.3260242355346679, 1.3441803853988648, 1.3359965356826782, 1.333072993850708, 1.3666889663696289, 1.3845041080474854, 1.2796363117218017, 1.2600684553146362, 1.2959488983154297, 1.2753588794708253, 1.3279467357635497, 1.3639327758789062, 1.2405724813461303, 1.2055261138916016, 1.228471804046631, 1.3171282112121583, 1.4258446353912353, 1.2834335559844972, 1.3348070568084718, 1.2982121212005615, 1.3230545721054077, 1.2623742618560791, 1.2962261791229248, 1.3244856747627258, 1.3216767490386963, 1.2971129570007325, 1.3470759033203126, 1.3283390201568603, 1.3641803007125854, 1.3817670742034913, 1.3364386047363281, 1.364761950302124, 1.3396434524536134, 1.3139343936920167, 1.323131849861145, 1.3707831413269043, 1.3408951782226564], 'val_accuracy': [0.7419, 0.7549, 0.7736, 0.7974, 0.7821, 0.7954, 0.796, 0.7897, 0.7849, 0.8013, 0.8046, 0.7912, 0.8024, 0.8163, 0.812, 0.8222, 0.8126, 0.8008, 0.7982, 0.8275, 0.8297, 0.8238, 0.8233, 0.8201, 0.8121, 0.8306, 0.8268, 0.8289, 0.8139, 0.7965, 0.8199, 0.8217, 0.821, 0.8124, 0.8252, 0.8208, 0.8122, 0.82, 0.8215, 0.8101, 0.8201, 0.8089, 0.8119, 0.8199, 0.808, 0.8213, 0.821, 0.8183, 0.811, 0.814]}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(cls_logs)):\n",
    "    print(cls_logs[i].history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_27\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_27 (Flatten)            (None, 16384)        0           conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 16384)        65536       flatten_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_81 (Dropout)            (None, 16384)        0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_81 (Dense)                (None, 200)          3277000     dropout_81[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 200)          800         dense_81[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 200)          0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_82 (Dropout)            (None, 200)          0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_82 (Dense)                (None, 200)          40200       dropout_82[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 200)          800         dense_82[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 200)          0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_83 (Dropout)            (None, 200)          0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_83 (Dense)                (None, 4)            804         dropout_83[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,614,900\n",
      "Trainable params: 3,578,388\n",
      "Non-trainable params: 36,512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/80\n",
      "200000/200000 [==============================] - 30s 149us/sample - loss: 2.4366 - accuracy: 0.5804 - val_loss: 1.6067 - val_accuracy: 0.6969\n",
      "Epoch 2/80\n",
      "200000/200000 [==============================] - 28s 139us/sample - loss: 1.7046 - accuracy: 0.6612 - val_loss: 1.9710 - val_accuracy: 0.5022\n",
      "Epoch 3/80\n",
      "200000/200000 [==============================] - 28s 139us/sample - loss: 1.6477 - accuracy: 0.6997 - val_loss: 1.2934 - val_accuracy: 0.8277\n",
      "Epoch 4/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 1.5777 - accuracy: 0.7292 - val_loss: 1.7631 - val_accuracy: 0.6290\n",
      "Epoch 5/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 1.5175 - accuracy: 0.7505 - val_loss: 1.7653 - val_accuracy: 0.6272\n",
      "Epoch 6/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 1.4558 - accuracy: 0.7679 - val_loss: 1.1793 - val_accuracy: 0.8742\n",
      "Epoch 7/80\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 1.4038 - accuracy: 0.7838 - val_loss: 1.8559 - val_accuracy: 0.6320\n",
      "Epoch 8/80\n",
      "200000/200000 [==============================] - 29s 143us/sample - loss: 1.3584 - accuracy: 0.7943 - val_loss: 1.4071 - val_accuracy: 0.7328\n",
      "Epoch 9/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 1.3122 - accuracy: 0.8046 - val_loss: 1.5275 - val_accuracy: 0.6903\n",
      "Epoch 10/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 1.2608 - accuracy: 0.8140 - val_loss: 1.2566 - val_accuracy: 0.8031\n",
      "Epoch 11/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 1.2240 - accuracy: 0.8234 - val_loss: 1.3062 - val_accuracy: 0.7616\n",
      "Epoch 12/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 1.1826 - accuracy: 0.8313 - val_loss: 1.3375 - val_accuracy: 0.7550\n",
      "Epoch 13/80\n",
      "200000/200000 [==============================] - 28s 138us/sample - loss: 1.1499 - accuracy: 0.8382 - val_loss: 1.3803 - val_accuracy: 0.7398\n",
      "Epoch 14/80\n",
      "200000/200000 [==============================] - 28s 138us/sample - loss: 1.1158 - accuracy: 0.8444 - val_loss: 1.1157 - val_accuracy: 0.8398\n",
      "Epoch 15/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 1.0772 - accuracy: 0.8509 - val_loss: 1.4810 - val_accuracy: 0.6827\n",
      "Epoch 16/80\n",
      "200000/200000 [==============================] - 28s 139us/sample - loss: 1.0496 - accuracy: 0.8557 - val_loss: 1.0900 - val_accuracy: 0.8173\n",
      "Epoch 17/80\n",
      "200000/200000 [==============================] - 28s 139us/sample - loss: 1.0224 - accuracy: 0.8605 - val_loss: 1.0786 - val_accuracy: 0.8426\n",
      "Epoch 18/80\n",
      "200000/200000 [==============================] - 28s 138us/sample - loss: 0.9899 - accuracy: 0.8659 - val_loss: 1.2013 - val_accuracy: 0.7900\n",
      "Epoch 19/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.9695 - accuracy: 0.8708 - val_loss: 1.1434 - val_accuracy: 0.7863\n",
      "Epoch 20/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.9440 - accuracy: 0.8745 - val_loss: 1.2936 - val_accuracy: 0.7448\n",
      "Epoch 21/80\n",
      "200000/200000 [==============================] - 28s 139us/sample - loss: 0.9198 - accuracy: 0.8786 - val_loss: 1.0352 - val_accuracy: 0.8202\n",
      "Epoch 22/80\n",
      "200000/200000 [==============================] - 28s 138us/sample - loss: 0.9005 - accuracy: 0.8829 - val_loss: 1.4086 - val_accuracy: 0.7115\n",
      "Epoch 23/80\n",
      "200000/200000 [==============================] - 28s 138us/sample - loss: 0.8778 - accuracy: 0.8863 - val_loss: 1.5703 - val_accuracy: 0.6730\n",
      "Epoch 24/80\n",
      "200000/200000 [==============================] - 28s 138us/sample - loss: 0.8613 - accuracy: 0.8904 - val_loss: 1.4307 - val_accuracy: 0.7054\n",
      "Epoch 25/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.8420 - accuracy: 0.8928 - val_loss: 1.1574 - val_accuracy: 0.7738\n",
      "Epoch 26/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.8274 - accuracy: 0.8954 - val_loss: 1.3501 - val_accuracy: 0.7219\n",
      "Epoch 27/80\n",
      "200000/200000 [==============================] - 28s 139us/sample - loss: 0.8097 - accuracy: 0.8986 - val_loss: 1.3983 - val_accuracy: 0.7195\n",
      "Epoch 28/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.7943 - accuracy: 0.9018 - val_loss: 1.0450 - val_accuracy: 0.8182\n",
      "Epoch 29/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.7805 - accuracy: 0.9044 - val_loss: 1.0174 - val_accuracy: 0.8167\n",
      "Epoch 30/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.7692 - accuracy: 0.9066 - val_loss: 1.3598 - val_accuracy: 0.7312\n",
      "Epoch 31/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.7580 - accuracy: 0.9091 - val_loss: 1.1787 - val_accuracy: 0.7575\n",
      "Epoch 32/80\n",
      "200000/200000 [==============================] - 28s 139us/sample - loss: 0.7450 - accuracy: 0.9109 - val_loss: 1.3445 - val_accuracy: 0.7204\n",
      "Epoch 33/80\n",
      "200000/200000 [==============================] - 28s 139us/sample - loss: 0.7402 - accuracy: 0.9124 - val_loss: 0.9018 - val_accuracy: 0.8603\n",
      "Epoch 34/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.7267 - accuracy: 0.9140 - val_loss: 1.3857 - val_accuracy: 0.7094\n",
      "Epoch 35/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.7139 - accuracy: 0.9174 - val_loss: 1.2820 - val_accuracy: 0.7480\n",
      "Epoch 36/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.7076 - accuracy: 0.9185 - val_loss: 1.0930 - val_accuracy: 0.7994\n",
      "Epoch 37/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.6944 - accuracy: 0.9213 - val_loss: 1.1870 - val_accuracy: 0.7740\n",
      "Epoch 38/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.6867 - accuracy: 0.9222 - val_loss: 0.8366 - val_accuracy: 0.8789\n",
      "Epoch 39/80\n",
      "200000/200000 [==============================] - 28s 139us/sample - loss: 0.6766 - accuracy: 0.9249 - val_loss: 0.9251 - val_accuracy: 0.8523\n",
      "Epoch 40/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.6723 - accuracy: 0.9247 - val_loss: 0.9857 - val_accuracy: 0.8458\n",
      "Epoch 41/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.6649 - accuracy: 0.9272 - val_loss: 1.0233 - val_accuracy: 0.8380\n",
      "Epoch 42/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.6562 - accuracy: 0.9286 - val_loss: 1.1388 - val_accuracy: 0.7871\n",
      "Epoch 43/80\n",
      "200000/200000 [==============================] - 28s 139us/sample - loss: 0.6531 - accuracy: 0.9302 - val_loss: 1.1348 - val_accuracy: 0.7791\n",
      "Epoch 44/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.6468 - accuracy: 0.9316 - val_loss: 1.4295 - val_accuracy: 0.7391\n",
      "Epoch 45/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.6408 - accuracy: 0.9334 - val_loss: 0.8609 - val_accuracy: 0.8621\n",
      "Epoch 46/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.6351 - accuracy: 0.9336 - val_loss: 1.4885 - val_accuracy: 0.7056\n",
      "Epoch 47/80\n",
      "200000/200000 [==============================] - 28s 138us/sample - loss: 0.6296 - accuracy: 0.9353 - val_loss: 1.5370 - val_accuracy: 0.6971\n",
      "Epoch 48/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.6238 - accuracy: 0.9366 - val_loss: 0.9087 - val_accuracy: 0.8613\n",
      "Epoch 49/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.6238 - accuracy: 0.9371 - val_loss: 1.0547 - val_accuracy: 0.8152\n",
      "Epoch 50/80\n",
      "200000/200000 [==============================] - 28s 139us/sample - loss: 0.6191 - accuracy: 0.9383 - val_loss: 1.1959 - val_accuracy: 0.7878\n",
      "Epoch 51/80\n",
      "200000/200000 [==============================] - 28s 139us/sample - loss: 0.6121 - accuracy: 0.9395 - val_loss: 1.4042 - val_accuracy: 0.7364\n",
      "Epoch 52/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.6115 - accuracy: 0.9408 - val_loss: 1.1291 - val_accuracy: 0.7924\n",
      "Epoch 53/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.6036 - accuracy: 0.9420 - val_loss: 1.4047 - val_accuracy: 0.7415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/80\n",
      "200000/200000 [==============================] - 28s 139us/sample - loss: 0.5938 - accuracy: 0.9438 - val_loss: 0.9939 - val_accuracy: 0.8303\n",
      "Epoch 55/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.5977 - accuracy: 0.9433 - val_loss: 0.9727 - val_accuracy: 0.8525\n",
      "Epoch 56/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.5926 - accuracy: 0.9434 - val_loss: 1.3326 - val_accuracy: 0.7486\n",
      "Epoch 57/80\n",
      "200000/200000 [==============================] - 28s 139us/sample - loss: 0.5866 - accuracy: 0.9445 - val_loss: 1.0891 - val_accuracy: 0.8016\n",
      "Epoch 58/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.5860 - accuracy: 0.9449 - val_loss: 0.8388 - val_accuracy: 0.8587\n",
      "Epoch 59/80\n",
      "200000/200000 [==============================] - 28s 138us/sample - loss: 0.5813 - accuracy: 0.9462 - val_loss: 1.3726 - val_accuracy: 0.7583\n",
      "Epoch 60/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.5733 - accuracy: 0.9484 - val_loss: 1.3649 - val_accuracy: 0.7660\n",
      "Epoch 61/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.5741 - accuracy: 0.9485 - val_loss: 1.3786 - val_accuracy: 0.7594\n",
      "Epoch 62/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.5670 - accuracy: 0.9491 - val_loss: 1.1183 - val_accuracy: 0.8082\n",
      "Epoch 63/80\n",
      "200000/200000 [==============================] - 28s 139us/sample - loss: 0.5578 - accuracy: 0.9506 - val_loss: 1.0905 - val_accuracy: 0.8029\n",
      "Epoch 64/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.5633 - accuracy: 0.9495 - val_loss: 1.4921 - val_accuracy: 0.7287\n",
      "Epoch 65/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.5602 - accuracy: 0.9506 - val_loss: 1.4125 - val_accuracy: 0.7386\n",
      "Epoch 66/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.5542 - accuracy: 0.9516 - val_loss: 1.3225 - val_accuracy: 0.7634\n",
      "Epoch 67/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.5526 - accuracy: 0.9515 - val_loss: 1.1441 - val_accuracy: 0.8212\n",
      "Epoch 68/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.5456 - accuracy: 0.9532 - val_loss: 1.5427 - val_accuracy: 0.7368\n",
      "Epoch 69/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.5460 - accuracy: 0.9533 - val_loss: 1.0939 - val_accuracy: 0.8218\n",
      "Epoch 70/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.5451 - accuracy: 0.9532 - val_loss: 1.1608 - val_accuracy: 0.8104\n",
      "Epoch 71/80\n",
      "200000/200000 [==============================] - 28s 139us/sample - loss: 0.5404 - accuracy: 0.9536 - val_loss: 1.1910 - val_accuracy: 0.7942\n",
      "Epoch 72/80\n",
      "200000/200000 [==============================] - 28s 139us/sample - loss: 0.5372 - accuracy: 0.9553 - val_loss: 1.1880 - val_accuracy: 0.8038\n",
      "Epoch 73/80\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 0.5341 - accuracy: 0.9558 - val_loss: 1.2881 - val_accuracy: 0.7861\n",
      "Epoch 74/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.5368 - accuracy: 0.9552 - val_loss: 1.3433 - val_accuracy: 0.7593\n",
      "Epoch 75/80\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 0.5326 - accuracy: 0.9561 - val_loss: 1.7559 - val_accuracy: 0.6962\n",
      "Epoch 76/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.5290 - accuracy: 0.9567 - val_loss: 1.6190 - val_accuracy: 0.7020\n",
      "Epoch 77/80\n",
      "200000/200000 [==============================] - 28s 138us/sample - loss: 0.5289 - accuracy: 0.9576 - val_loss: 1.1154 - val_accuracy: 0.8185\n",
      "Epoch 78/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.5274 - accuracy: 0.9567 - val_loss: 1.6937 - val_accuracy: 0.7176\n",
      "Epoch 79/80\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.5231 - accuracy: 0.9583 - val_loss: 1.2673 - val_accuracy: 0.7897\n",
      "Epoch 80/80\n",
      "200000/200000 [==============================] - 28s 140us/sample - loss: 0.5226 - accuracy: 0.9589 - val_loss: 1.6244 - val_accuracy: 0.7241\n",
      "INFO:tensorflow:Assets written to: selfsupervised_0/assets\n"
     ]
    }
   ],
   "source": [
    "feat_log = self_supervised_train([i for i in range(50000 * rotations_num)], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: selfsupervised_50_backup2/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-4b5609b8b4ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'selfsupervised_50_backup2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv_feat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sayali/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sayali/lib/python3.6/site-packages/tensorflow_core/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m     81\u001b[0m                   (export_dir,\n\u001b[1;32m     82\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: selfsupervised_50_backup2/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('selfsupervised_50_backup2')\n",
    "model.summary()\n",
    "l = model.get_layer('conv_feat').output\n",
    "feat = tf.keras.Model(inputs = model.input, outputs = l)\n",
    "y_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = feat.predict(np.asarray([combined_data[(50000 + i) * rotations_num] for i in range(10000)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for p in labelsPercent:\n",
    "    labeled, unlabeled = generate_labeled_unlabeled(examplesForClass, p, False)\n",
    "    features = feat.predict(np.asarray([combined_data[i] for i in labeled]))\n",
    "    clf = svm.SVC(C = 100., kernel = 'rbf')\n",
    "    clf.fit(features, np.asarray([np.argmax(true_labels[i]) for i in labeled]))\n",
    "    y_pred.append(clf.predict(features_test))\n",
    "    print(metrics.accuracy_score(np.asarray([np.argmax(true_labels[(50000 + i) * rotations_num]) for i in range(10000)]), y_pred[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqgUVhQ_4CVR"
   },
   "outputs": [],
   "source": [
    "plot_dict = {}\n",
    "\n",
    "for i, p in enumerate(labelsPercent):\n",
    "    plot_dict[\"labels_\" + str(p)] = cls_logs[i].history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOEtN07P4CVT",
    "outputId": "aac9cc73-b8ed-4c62-d0db-688a5a5582fa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_training(name = 'Validation accuracy',\n",
    "              filename = 'self_part_labels',\n",
    "              **plot_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfsupervised_acc = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, p in enumerate(labelsPercent):\n",
    "    selfsupervised_acc[p] = cls_logs[i].history['val_acc'][-1]\n",
    "print(selfsupervised_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYBivMPo4CVX"
   },
   "outputs": [],
   "source": [
    "emptyResNet = ResNet50(include_top=False, weights=None, input_shape=(32, 32, 3))\n",
    "emptyResNet.summary()\n",
    "emptyResNet.compile(optimizer = optimizers.Adam(), loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "emptyResNet.save('emptyResNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycnn = get_feat_model()\n",
    "mycnn.summary()\n",
    "mycnn.compile(optimizer = optimizers.Adam(), loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "mycnn.save(cnn_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_supervised = []\n",
    "cls_logs_supervised = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in labelsPercent:\n",
    "    labeled, unlabeled = generate_labeled_unlabeled(examplesForClass, p, False)\n",
    "    cls_logs_supervised.append(fine_tune(labeled, p, True, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supervised with data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=4,\n",
    "    height_shift_range=4, \n",
    "    brightness_range=[0.5, 1.0],\n",
    "    preprocessing_function=my_preprocess)\n",
    "datagen.fit(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for p in labelsPercent:\n",
    "    labeled, unlabeled = generate_augmented_labeled_unlabeled(augmentedExamplesForClass, p, False)\n",
    "    print(len(labeled))\n",
    "    print(len(unlabeled))\n",
    "    cls_logs_supervised.append(fine_tune(labeled, p, True, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_acc = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, p in enumerate(labelsPercent):\n",
    "    if i < len(cls_logs_supervised):\n",
    "        supervised_acc[p] = cls_logs_supervised[i].history['val_accuracy'][-1]\n",
    "        \n",
    "print(supervised_acc)\n",
    "#{100: 0.8076, 50: 0.7702, 25: 0.7134, 10: 0.6265, 5: 0.5556, 1: 0.3892, 3: 0.4811, 2: 0.4377}\n",
    "#{1: 0.4231, 2: 0.5026, 3: 0.5475, 5: 0.6002, 10: 0.6714, 25: 0.7411, 50: 0.7959, 100: 0.8420}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "super_v = []\n",
    "self_v = []\n",
    "\n",
    "for p in labelsPercent:\n",
    "    super_v.append(supervised_acc[p])\n",
    "    self_v.append(selfsupervised_acc[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesPerClass = []\n",
    "for p in labelsPercent:\n",
    "    samplesPerClass.append(5000 * p // 100)\n",
    "print(samplesPerClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.plot(samplesPerClass, super_v, 'o-', label='  ')\n",
    "ax.plot(samplesPerClass, self_v, 'o-', label='')\n",
    "ax.grid(True)\n",
    "plt.title(' ', size = 24)\n",
    "plt.xlabel('    ( 5000)', fontsize = 18)\n",
    "plt.ylabel('', fontsize = 18)\n",
    "plt.xscale(\"log\")\n",
    "ax.axis([min(samplesPerClass), max(samplesPerClass), 0., 1.])\n",
    "ax.set_xticks(samplesPerClass)\n",
    "ax.tick_params(labelsize= 18)\n",
    "ax.xaxis.set_major_formatter(ScalarFormatter())\n",
    "ax.legend(loc='lower right', fontsize = 18)\n",
    "plt.savefig('acc_compare_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5MUHwA-4CVZ",
    "outputId": "8d13c1d8-486f-430e-d66f-b24a516f826a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, p in enumerate(labelsPercent):\n",
    "    plot_dict = {}\n",
    "    \n",
    "    plot_dict[\"labels_\" + str(p) + \"_selfsupervised\"] = cls_logs[i].history['val_acc']\n",
    "    plot_dict[\"labels_\" + str(p) + \"_supervised\"] = cls_logs_supervised[i].history['val_acc']\n",
    "    \n",
    "    plot_training(name = 'Validation accuracy for ' + str(p) + \"% of labels\",\n",
    "                  filename = 'self_and_default_' + str(p) + \" labels\",\n",
    "                  **plot_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1562,
     "status": "ok",
     "timestamp": 1605689972038,
     "user": {
      "displayName": "Alexander Dimitrienko",
      "photoUrl": "",
      "userId": "07582037467622387890"
     },
     "user_tz": -180
    },
    "id": "izmuo9qeQiRy"
   },
   "outputs": [],
   "source": [
    "unsupervised_threshold = 0.95\n",
    "unsupervised_loss_coef = 1.0\n",
    "unlabeled_epoch_start = 5\n",
    "\n",
    "semisupervised_epochs = 50\n",
    "semisupervised_batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1551,
     "status": "ok",
     "timestamp": 1605689972038,
     "user": {
      "displayName": "Alexander Dimitrienko",
      "photoUrl": "",
      "userId": "07582037467622387890"
     },
     "user_tz": -180
    },
    "id": "EEWfx01uejSP"
   },
   "outputs": [],
   "source": [
    "labelsPercent = [50, 25, 10, 5, 3, 2, 1] #100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1542,
     "status": "ok",
     "timestamp": 1605689972038,
     "user": {
      "displayName": "Alexander Dimitrienko",
      "photoUrl": "",
      "userId": "07582037467622387890"
     },
     "user_tz": -180
    },
    "id": "Lr4woi469A1z"
   },
   "outputs": [],
   "source": [
    "class Augmentator():\n",
    "    def shearX(self, img):\n",
    "        return img.transform(img.size, PIL.Image.AFFINE, (1, (random.choice([-1, 1])) * self.M * 0.3, 0, 0, 1, 0))\n",
    "\n",
    "    def shearY(self, img):\n",
    "        return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, (random.choice([-1, 1])) * self.M * 0.3, 1, 0))\n",
    "\n",
    "    def translateX(self, img):\n",
    "        return img.transform(img.size, PIL.Image.AFFINE, (1, 0, (random.choice([-1, 1])) * self.M * 0.3 * img.size[0], 0, 1, 0))\n",
    "\n",
    "    def translateY(self, img):\n",
    "        return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, (random.choice([-1, 1])) * self.M * 0.3 * img.size[0]))\n",
    "\n",
    "    def rotate(self, img):\n",
    "        return img.rotate((random.choice([-1, 1])) * self.M * 30.0)\n",
    "\n",
    "    def autoContrast(self, img):\n",
    "        return PIL.ImageOps.autocontrast(img)\n",
    "\n",
    "    def invert(self, img):\n",
    "        return PIL.ImageOps.invert(img)\n",
    "\n",
    "    def equalize(self, img):\n",
    "        return PIL.ImageOps.equalize(img)\n",
    "\n",
    "    def solarize(self, img):\n",
    "        return PIL.ImageOps.solarize(img, self.M * 256.0)\n",
    "\n",
    "    def posterize(self, img):\n",
    "        return PIL.ImageOps.posterize(img, int(self.M * 4.0))\n",
    "\n",
    "    def contrast(self, img):\n",
    "        return PIL.ImageEnhance.Contrast(img).enhance(self.M)\n",
    "\n",
    "    def color(self, img):\n",
    "        return PIL.ImageEnhance.Color(img).enhance(self.M)\n",
    "\n",
    "    def brightness(self, img):\n",
    "        return PIL.ImageEnhance.Brightness(img).enhance(self.M)\n",
    "\n",
    "    def sharpness(self, img):\n",
    "        return PIL.ImageEnhance.Sharpness(img).enhance(self.M)\n",
    "    \n",
    "    def __init__(self, N, M):\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "        self.transformations = [self.autoContrast, self.equalize, self.rotate, self.solarize, self.color, self.posterize, self.contrast, self.brightness, self.sharpness, self.shearX, self.shearY, self.translateX, self.translateY]\n",
    "\n",
    "    def __call__(self, img):\n",
    "        chosen_transformations = random.choices(self.transformations, k = self.N)\n",
    "        for f in chosen_transformations:\n",
    "            img = f(img)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3225,
     "status": "ok",
     "timestamp": 1605689973731,
     "user": {
      "displayName": "Alexander Dimitrienko",
      "photoUrl": "",
      "userId": "07582037467622387890"
     },
     "user_tz": -180
    },
    "id": "k4qWlhmFgJYz"
   },
   "outputs": [],
   "source": [
    "def is_called(func):\n",
    "    def newfunc(*args, **kwargs):\n",
    "        print('Called func!')\n",
    "        func(args, kwargs)\n",
    "    return newfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3217,
     "status": "ok",
     "timestamp": 1605689973732,
     "user": {
      "displayName": "Alexander Dimitrienko",
      "photoUrl": "",
      "userId": "07582037467622387890"
     },
     "user_tz": -180
    },
    "id": "fuZ9bGzlkZwe"
   },
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    with tf.GradientTape() as tape:\n",
    "        return tf.math.reduce_sum(tf.multiply(y_true, tf.math.log(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1156,
     "status": "ok",
     "timestamp": 1605690176788,
     "user": {
      "displayName": "Alexander Dimitrienko",
      "photoUrl": "",
      "userId": "07582037467622387890"
     },
     "user_tz": -180
    },
    "id": "g3WLz90N4CVb"
   },
   "outputs": [],
   "source": [
    "#loss_tracker = metrics.CategoricalCrossentropy(name=\"loss\")\n",
    "acc_metric = tf.keras.metrics.Accuracy(name=\"acc\")\n",
    "\n",
    "#loss_tracker_supervised = metrics.CategoricalCrossentropy(name=\"loss_sup\")\n",
    "acc_metric_supervised = tf.keras.metrics.Accuracy(name=\"acc_sup\")\n",
    "\n",
    "#loss_tracker_unsupervised = metrics.CategoricalCrossentropy(name=\"loss_unsup\")\n",
    "acc_metric_unsupervised = tf.keras.metrics.Accuracy(name=\"acc_unsup\")\n",
    "\n",
    "\n",
    "class CustomModel(Model):\n",
    "    @is_called\n",
    "    def train_step(self, data):\n",
    "        X, Y = data\n",
    "        print(X)\n",
    "        X = tf.unstack(X)\n",
    "\n",
    "        print('Going to tape')\n",
    "\n",
    "        x_supervised = np.asarray([combined_data[i] for i in X if i in labeled])\n",
    "        x_unsupervised = np.asarray([combined_data[i] for i in X if i in unlabeled])\n",
    "        x_augmented = x_unsupervised\n",
    "        y_true_supervised = np.asarray([true_labels[i] for i in X if i in labeled])\n",
    "\n",
    "        #Supervised part\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x_supervised)\n",
    "\n",
    "            y_pred_supervised = self(x_supervised, training=True)  \n",
    "            loss_supervised = custom_loss(y_true_supervised, y_pred_supervised)\n",
    "\n",
    "            #Unsupervised part\n",
    "            tape.watch(x_augmented)\n",
    "            y_pred = self(x_unsupervised, training=False)\n",
    "\n",
    "            #y_pred_np = tf.make_ndarray(y_pred.op.get_attr('value'))\n",
    "            y_pred_unsupervised = self(x_augmented, training=True) #if max(y_pred_np[i]) > unsupervised_threshold)\n",
    "            \n",
    "            loss_unsupervised = custom_loss(y_pred, y_pred_unsupervised)\n",
    "\n",
    "            #Loss\n",
    "            #loss, grads = custom_loss(self, y_true_supervised, y_pred, y_pred_supervised, y_pred_unsupervised)\n",
    "            loss = loss_supervised + loss_unsupervised * unsupervised_loss_coef\n",
    "            grads = tape.gradient(loss, self.trainable_variables)\n",
    "\n",
    "            #Weights\n",
    "            self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        #Metrics\n",
    "        #loss_tracker_supervised.update_state(loss_supervised)\n",
    "        acc_metric_supervised.update_state(y_true_supervised, y_pred_supervised)\n",
    "\n",
    "        #loss_tracker_unsupervised.update_state(loss_unsupervised)\n",
    "        acc_metric_unsupervised.update_state(y_pred, y_pred_unsupervised)\n",
    "\n",
    "        #loss_tracker.update_state(loss)\n",
    "        #acc_metric.update_state(np.concatenate(y_pred, y_true_supervised), np.concatenate(y_pred_unsupervised, y_pred_supervised))\n",
    "\n",
    "        return {#\"loss_supervised\": loss_tracker_supervised.result(), \n",
    "                \"acc_supervised\": acc_metric_supervised.result(), \n",
    "                #\"loss_unsupervised\": loss_tracker_unsupervised.result(), \n",
    "                \"acc_unsupervised\": acc_metric_unsupervised.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [#loss_tracker, \n",
    "            acc_metric]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3695,
     "status": "ok",
     "timestamp": 1605689974228,
     "user": {
      "displayName": "Alexander Dimitrienko",
      "photoUrl": "",
      "userId": "07582037467622387890"
     },
     "user_tz": -180
    },
    "id": "MPlDyACEW6Q4"
   },
   "outputs": [],
   "source": [
    "augmentator = Augmentator(2, random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_data = np.zeros(augmented_data.shape, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(files)):\n",
    "    data_len = files[i][b\"data\"].shape[0]\n",
    "    \n",
    "    for j in range(data_len):\n",
    "        row = files[i][b\"data\"][j]\n",
    "        true_labels[data_len * i + j][files[i][b\"labels\"][j]] = 1.\n",
    "        \n",
    "        for k in range(files[i][b\"data\"].shape[1]):\n",
    "            rotated_data[data_len * i + j][(k & 1023) >> 5][k & 31][k >> 10] = row[k]\n",
    "            \n",
    "        rotated_data[data_len * i + j] = np.rot90(rotated_data[data_len * i + j], random.choice([0, 1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(pil_data.shape[0]):\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "    pil_data[i] = np.array(augmentator(Image.fromarray(np.uint8(augmented_data[i // augment_num * augment_num]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r = random.randint(0, pil_data.shape[0] // augment_num)\n",
    "for i in range(augment_num):\n",
    "    plt.imshow(augmented_data[r * augment_num + i].astype(int))\n",
    "    plt.show()\n",
    "    plt.imshow(pil_data[r * augment_num + i].astype(int))\n",
    "    plt.show()\n",
    "    print(augmented_labels[r + i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rotated_data = preprocess_input(rotated_data)\n",
    "#pil_data = preprocess_input(pil_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3226,
     "status": "ok",
     "timestamp": 1605689973730,
     "user": {
      "displayName": "Alexander Dimitrienko",
      "photoUrl": "",
      "userId": "07582037467622387890"
     },
     "user_tz": -180
    },
    "id": "WsI_XRYKb7MI"
   },
   "outputs": [],
   "source": [
    "augmentedExamplesForClass = []\n",
    "for i in range(10):\n",
    "    augmentedExamplesForClass.append([j for j in range(50000 * augment_num) if np.where(augmented_labels[j] == 1)[0][0] == i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_conv_model():\n",
    "    inputs = tf.keras.Input((32, 32, 3))\n",
    "    x = layers.Conv2D(32, (3, 3))(inputs)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3))(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), name = 'conv_feat')(x)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(256)(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dense(10, activation = 'softmax')(x)\n",
    "    \n",
    "    return CustomModel(inputs = inputs, outputs = x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3680,
     "status": "ok",
     "timestamp": 1605689974229,
     "user": {
      "displayName": "Alexander Dimitrienko",
      "photoUrl": "",
      "userId": "07582037467622387890"
     },
     "user_tz": -180
    },
    "id": "lzS0LxF9biZ6"
   },
   "outputs": [],
   "source": [
    "def get_resnet_model():\n",
    "    base = tf.keras.models.load_model('emptyResNet')\n",
    "    l = base.get_layer(feature_layer).output\n",
    "    l = layers.Flatten()(l)\n",
    "    l = layers.Dropout(0.5)(l)\n",
    "    l = layers.Dense(200, kernel_regularizer=regularizers.l1_l2(l1 = 0.0001, l2 = 0.0001))(l)\n",
    "    l = layers.BatchNormalization()(l)\n",
    "    l = layers.Activation('tanh')(l)\n",
    "    l = layers.Dropout(0.5)(l)\n",
    "    l = layers.Dense(200, kernel_regularizer=regularizers.l1_l2(l1 = 0.0001, l2 = 0.0001))(l)\n",
    "    l = layers.BatchNormalization()(l)\n",
    "    l = layers.Activation('tanh')(l)\n",
    "    l = layers.Dropout(0.3)(l)\n",
    "    l = layers.Dense(10, kernel_regularizer=regularizers.l1_l2(l1 = 0.0001, l2 = 0.0001), activation = 'softmax')(l)\n",
    "\n",
    "    return tf.keras.Model(inputs = base.input, outputs = l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labeled_unlabeled_semi(examples, percent, shuffle = True):\n",
    "    #           \n",
    "    if shuffle:\n",
    "        shuffle_examples()\n",
    "    labeled = []\n",
    "    tmp = []\n",
    "    unlabeled = []\n",
    "    \n",
    "    labeled_nums = [i for i in range(5000 - percent * 50, 5000)]\n",
    "    unlabeled_nums = [i for i in range(5000 - percent * 50)]\n",
    "\n",
    "    for i in range(len(examples)):\n",
    "        for j in labeled_nums:\n",
    "            labeled += [examples[i][j]] #50 == 5000 / 100\n",
    "        for j in unlabeled_nums:\n",
    "            unlabeled += [examples[i][j]]\n",
    "        \n",
    "    return labeled, unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vn-Pj6Zse0IK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#SEMI-SUPERVISED LEARNING\n",
    "\n",
    "val_indices = [(50000 + i) * rotations_num for i in range(10000)]\n",
    "x_val = tf.convert_to_tensor(np.take(combined_data, val_indices, axis = 0) / 256.)\n",
    "y_val = tf.convert_to_tensor(np.take(true_labels, val_indices, axis = 0))\n",
    "\n",
    "for p in labelsPercent:\n",
    "    @tf.function\n",
    "    def train_step(x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x, training=True)\n",
    "            #loss_value = custom_loss(y, logits)\n",
    "            loss_value = loss(y, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    train_acc_metric.update_state(y, logits)\n",
    "    return loss_value\n",
    "    \n",
    "    @tf.function\n",
    "    def val_step(x, y):\n",
    "        val_logits = model(x, training=False)\n",
    "        val_acc_metric.update_state(y, val_logits)\n",
    "    \n",
    "    print('%d%% of labeled data' % p)\n",
    "    model = get_cls_model(saved_name, False)\n",
    "    model.summary()\n",
    "    \n",
    "    labeled, unlabeled = generate_augmented_labeled_unlabeled(augmentedExamplesForClass, p, False)\n",
    "    \n",
    "    train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "    val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "    \n",
    "    #Custom training loop\n",
    "    for epoch in range(semisupervised_epochs):\n",
    "        above_threshold_num = 0\n",
    "        start_time = time.time()\n",
    "        random.shuffle(labeled)\n",
    "        random.shuffle(unlabeled)\n",
    "        print(\"\\nEpoch \" + str(epoch) + \"/\" + str(semisupervised_epochs))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for step_num in range(augmented_data.shape[0] // semisupervised_batch_size):\n",
    "            print('Processing step %d/%d, train_acc = %.4f\\r' % (step_num, augmented_data.shape[0] // semisupervised_batch_size, float(train_acc_metric.result())), end=\"\")\n",
    "            labeled_batch_indices = labeled[step_num * semisupervised_batch_size * p // 100 : (step_num + 1) * semisupervised_batch_size * p // 100 ]\n",
    "            unlabeled_batch_indices = unlabeled[step_num * semisupervised_batch_size * (100 - p) // 100 : (step_num + 1) * semisupervised_batch_size * (100 - p) // 100 ]\n",
    "            loss_value_labeled = train_step(tf.convert_to_tensor(np.take(augmented_data, labeled_batch_indices, axis = 0) / 256.), \n",
    "                                            tf.convert_to_tensor(np.take(augmented_labels, labeled_batch_indices, axis = 0)))\n",
    "         \n",
    "            if epoch >= unlabeled_epoch_start:\n",
    "                x_batch_unlabeled = tf.convert_to_tensor(np.take(augmented_data, unlabeled_batch_indices, axis = 0) / 256.)\n",
    "                y_true_unlabeled = model(x_batch_unlabeled, training=False)\n",
    "                \n",
    "                #check threshold\n",
    "                indices_above_threshold = np.take(unlabeled_batch_indices, tf.where(tf.reduce_max(y_true_unlabeled, axis = 1) > unsupervised_threshold)).flatten()\n",
    "                loss_value_unlabeled = 0.0\n",
    "                if indices_above_threshold.shape[0] > 0:\n",
    "                    above_threshold_num += indices_above_threshold.shape[0]\n",
    "                    loss_value_unlabeled = train_step(tf.convert_to_tensor(np.take(pil_data, indices_above_threshold, axis = 0) / 256.), \n",
    "                                                      tf.cast(tf.convert_to_tensor(np.take(y_true_unlabeled.numpy(), tf.where(tf.reduce_max(y_true_unlabeled, axis = 1) > unsupervised_threshold).numpy().flatten(), axis = 0)) + 0.5, tf.int32))\n",
    "\n",
    "            \n",
    "            #print('Step ' + str(step) + ' Loss: ' + str(loss_value_labeled) + ' | ' + str(loss_value_unlabeled))\n",
    "            \n",
    "            #optimizer.lr.assign(lr_schedule_conv(optimizer.lr, epoch))\n",
    "              \n",
    "        print(\"\\nTraining acc over epoch: %.4f\" % (float(train_acc_metric.result()),))\n",
    "        \n",
    "        val_step(x_val, y_val)\n",
    "        print(\"Val acc over epoch: %.4f\" % (float(val_acc_metric.result()),))\n",
    "\n",
    "        train_acc_metric.reset_states()\n",
    "        val_acc_metric.reset_states()\n",
    "        \n",
    "        print(\"%d/%d unlabeled samples are used\" % (above_threshold_num, len(unlabeled)))\n",
    "        print(\"%s seconds for epoch\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#FINE-TUNING AFTER SELF-SUPERVISED\n",
    "\n",
    "val_indices = [(50000 + i) * rotations_num for i in range(10000)]\n",
    "x_val = tf.convert_to_tensor(np.take(combined_data, val_indices, axis = 0) / 256.)\n",
    "y_val = tf.convert_to_tensor(np.take(true_labels, val_indices, axis = 0))\n",
    "\n",
    "for p in [3, 25, 50]:\n",
    "    @tf.function\n",
    "    def train_step(x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x, training=True)\n",
    "            #loss_value = custom_loss(y, logits)\n",
    "            loss_value = loss(y, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        train_acc_metric.update_state(y, logits)\n",
    "        return loss_value\n",
    "    \n",
    "    @tf.function\n",
    "    def val_step(x, y):\n",
    "        val_logits = model(x, training=False)\n",
    "        val_acc_metric.update_state(y, val_logits)\n",
    "\n",
    "    print('%d%% of labeled data' % p)\n",
    "    model = get_cls_model(saved_name, True)\n",
    "    model.summary()\n",
    "    \n",
    "    labeled, unlabeled = generate_augmented_labeled_unlabeled(augmentedExamplesForClass, p, False)\n",
    "    \n",
    "    train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "    val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    \n",
    "    #Custom training loop\n",
    "    for epoch in range(semisupervised_epochs):\n",
    "        above_threshold_num = 0\n",
    "        start_time = time.time()\n",
    "        random.shuffle(labeled)\n",
    "        print(\"\\nEpoch \" + str(epoch) + \"/\" + str(semisupervised_epochs))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for step_num in range(len(labeled) // semisupervised_batch_size):\n",
    "            print('Processing step %d/%d, train_acc = %.4f\\r' % (step_num, augmented_data.shape[0] // semisupervised_batch_size, float(train_acc_metric.result())), end=\"\")\n",
    "            labeled_batch_indices = labeled[step_num * semisupervised_batch_size: (step_num + 1) * semisupervised_batch_size]\n",
    "            loss_value_labeled = train_step(tf.convert_to_tensor(np.take(augmented_data, labeled_batch_indices, axis = 0) / 256.), \n",
    "                                            tf.convert_to_tensor(np.take(augmented_labels, labeled_batch_indices, axis = 0)))\n",
    "         \n",
    "           \n",
    "            #print('Step ' + str(step) + ' Loss: ' + str(loss_value_labeled) + ' | ' + str(loss_value_unlabeled))\n",
    "            \n",
    "        #optimizer.lr.assign(lr_schedule_cls(optimizer.lr, epoch))\n",
    "              \n",
    "        print(\"\\nTraining acc over epoch: %.4f\" % (float(train_acc_metric.result()),))\n",
    "        \n",
    "        val_step(x_val, y_val)\n",
    "        print(\"Val acc over epoch: %.4f\" % (float(val_acc_metric.result()),))\n",
    "\n",
    "        train_acc_metric.reset_states()\n",
    "        val_acc_metric.reset_states()\n",
    "        \n",
    "        print(\"%d/%d unlabeled samples are used\" % (above_threshold_num, len(unlabeled)))\n",
    "        print(\"%s seconds for epoch\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SelfSupervision.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
